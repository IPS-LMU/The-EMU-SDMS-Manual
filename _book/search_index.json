[
["index.html", "The EMU-SDMS Manual Welcome", " The EMU-SDMS Manual Raphael Winkelmann Welcome The EMU Speech Database Management System (EMU-SDMS) is a collection of software tools which aims to be as close to an all-in-one solution for generating, manipulating, querying, analyzing and managing speech databases as possible. This manual introduces and describes the various components of this system. "],
["installing-the-emu-sdms.html", "1 Installing the EMU-SDMS 1.1 Version disclaimer 1.2 For developers and people interested in the source code", " 1 Installing the EMU-SDMS R Download the R programming language from https://cran.r-project.org/ Install the R programming language by executing the downloaded file and following the on-screen instructions. emuR Start up R. Enter install.packages(\"emuR\") after the &gt; prompt to install the package. (You will only need to repeat this if package updates become available.) As the wrassp package is a dependency of the emuR package, it does not have to be installed separately. EMU-webApp (prerequisite) The only thing needed to use the EMU-webApp is a current HTML5 compatible browser (Chrome/Firefox/Safari/Opera/…). However, as most of the development and testing is done using Chrome we recommend using it, as it is by far the best tested browser. 1.1 Version disclaimer This document describes the following versions of the software components: wrassp Package version: 0.1.8 Git tag name: v0.1.6 (on master branch) emuR Package version: 1.1.2 Git tag name: v1.1.2 (on master branch) EMU-webApp Version: 0.1.15 Git SHA1: aaf47d35ffa6fd3cdebe0692e14c9ad6eef1040c As the development of the EMU Speech Database Management System is still ongoing, be sure you have the correct documentation to go with the version you are using. 1.2 For developers and people interested in the source code The information on how to install and/or access the source code of the developer version including the possibility of accessing the versions described in this document (via the Git tag names mentioned above) is given below. wrassp Source code is available here: https://github.com/IPS-LMU/wrassp/ Install developer version in R: install.packages(\"devtools\"); library(\"devtools\"); install_github(\"IPS-LMU/wrassp\") Bug reports: https://github.com/IPS-LMU/wrassp/issues emuR Source code is available here: https://github.com/IPS-LMU/emuR/ Install developer version in R: install.packages(\"devtools\"); library(\"devtools\"); install_github(\"IPS-LMU/emuR\") Bug reports: https://github.com/IPS-LMU/emuR/issues EMU-webApp Source code is available here: https://github.com/IPS-LMU/EMU-webApp/ Bug reports: https://github.com/IPS-LMU/EMU-webApp/issues "],
["chap-overview.html", "2 An overview of the EMU-SDMS1 2.1 The evolution of the EMU-SDMS 2.2 EMU-SDMS: System architecture and default workflow 2.3 EMU-SDMS: Is it something for you?", " 2 An overview of the EMU-SDMS1 The EMU Speech Database Management System (EMU-SDMS) is a collection of software tools which aims to be as close to an all-in-one solution for generating, manipulating, querying, analyzing and managing speech databases as possible. It was developed to fill the void in the landscape of software tools for the speech sciences by providing an integrated system that is centered around the R language and environment for statistical computing and graphics (R Core Team (2016)). This manual contains the documentation for the three software components wrassp, emuR and the EMU-webApp. In addition, it provides an in-depth description of the emuDB database format which is also considered an integral part of the new system. These four components comprise the EMU-SDMS and benefit the speech sciences and spoken language research by providing an integrated system to answer research questions such as: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a strong or weak syllable? This manual is targeted at new EMU-SDMS users as well as users familiar with the legacy EMU system. In addition, it is aimed at people who are interested in the technical details such as data structures/formats and implementation strategies, be it for reimplementation purposes or simply for a better understanding of the inner workings of the new system. To accommodate these different target groups, after initially giving an overview of the system, this manual presents a usage tutorial that walks the user through the entire process of answering a research question. This tutorial will start with a set of .wav audio and Praat .TextGrid (Boersma and Weenink (2016)) annotation files and end with a statistical analysis to address the hypothesis posed by the research question. The following Part I of this documentation is separated into six chapters that give an in-depth explanation of the various components that comprise the EMU-SDMS and integral concepts of the new system. These chapters provide a tutorial-like overview by providing multiple examples. To give the reader a synopsis of the main functions and central objects that are provided by EMU-SDMS’s main R package emuR, an overview of these functions is presented in Part II. Part III focuses on the actual implementation of the components and is geared towards people interested in the technical details. Further examples and file format descriptions are available in various appendices. This structure enables the novice EMU-SDMS user to simply skip the technical details and still get an in-depth overview of how to work with the new system and discover what it is capable of. A prerequisite that is presumed throughout this document is the reader’s familiarity with basic terminology in the speech sciences (e.g., familiarity with the international phonetic alphabet (IPA) and how speech is annotated at a coarse and fine grained level). Further, we assume the reader has a grasp of the basic concepts of the R language and environment for statistical computing and graphics. For readers new to R, there are multiple, freely available R tutorials online (e.g., https://en.wikibooks.org/wiki/Statistical_Analysis:_an_Introduction_using_R/R_basics). R also has a set of very detailed manuals and tutorials that come preinstalled with R. To be able to access R’s own “An Introduction to R” introduction, simply type help.start() into the R console and click on the link to the tutorial. 2.1 The evolution of the EMU-SDMS The EMU-SDMS has a number of predecessors that have been continuously developed over a number of years (e.g., Harrington et al. (1993), Cassidy and Harrington (1996), Cassidy and Harrington (2001), Bombien et al. (2006), Harrington (2010), John (2012)). The components presented here are the completely rewritten and newly designed, next incarnation of the EMU system, which we will refer to as the EMU Speech Database Management System (EMU-SDMS). The EMU-SDMS keeps most of the core concepts of the previous system, which we will refer to as the legacy system, in place while improving on things like usability, maintainability, scalability, stability, speed and more. We feel the redesign and reimplementation elevates the system into a modern set of speech and language tools that enables a workflow adapted to the challenges confronting speech scientists and the ever growing size of speech databases. The redesign has enabled us to implement several components of the new EMU-SDMS so that they can be used independently of the EMU-SDMS for tasks such as web-based collaborative annotation efforts and performing speech signal processing in a statistical programming environment. Nevertheless, the main goal of the redesign and reimplementation was to provide a modern set of tools that reduces the complexity of the tool chain needed to answer spoken language research questions down to a few interoperable tools. The tools the EMU-SDMS provides are designed to streamline the process of obtaining usable data, all from within an environment that can also be used to analyze, visualize and statistically evaluate the data. Upon developing the new system, rather than starting completely from scratch it seemed more appropriate to partially reuse the concepts of the legacy system in order to achieve our goals. A major observation at the time was that the R language and environment for statistical computing and graphics (R Core Team (2016)) was gaining more and more traction for statistical and data visualization purposes in the speech and spoken language research community. However, R was mostly only used towards the end of the data analysis chain where data usually was pre-converted into a comma-separated values or equivalent file format by the user using other tools to calculate, extract and pre-process the data. While designing the new EMU-SDMS, we brought R to the front of the tool chain to the point just beyond data acquisition. This allows the entire data annotation, data extraction and analysis process to be completed in R, while keeping the key user requirements in mind. Due to personal experiences gained by using the legacy system for research puposes and in various undergraduate courses (course material usually based on Harrington (2010)), we learned that the key user requirements were data and database portability, a simple installation process, a simplified/streamlined user experience and cross-platform availability. Supplying all of EMU-SDMS’s core functionality in the form of R packages that do not rely on external software at runtime seemed to meet all of these requirements. As the early incarnations of the legacy EMU system and its predecessors were conceived either at a time that predated the R system or during the infancy of R’s package ecosystem, the legacy system was implemented as a modular yet composite standalone program with a communication and data exchange interface to the R/Splus systems (see Cassidy and Harrington (2001) Section 3 for details). Recent developments in the package ecosystem of R such as the availability of the DBI package (R Special Interest Group on Databases (R-SIG-DB), Wickham, and Müller (2016)) and the related packages RSQLite and RPostgreSQL (Wickham, James, and Falcon (2014), Conway et al. (2016)), as well as the jsonlite package (Ooms (2014)) and the httpuv package (RStudio and Inc. (2015)), have made R an attractive sole target platform for the EMU-SDMS. These and other packages provide additional functional power that enabled the EMU-SDMS’s core functionality to be implemented in the form of R packages. The availability of certain R packages had a large impact on the architectural design decisions that we made for the new system. The R code snippet below shows the simple installation process which we were able to achieve due to the R package infrastructure. Compared to the legacy EMU and other systems, the installation process of the entire system has been reduced to a single R command. Throughout this documentation we will try to highlight how the EMU-SDMS is also able to meet the rest of the above key user requirements. # install the entire EMU-SDMS # by installing the emuR package install.packages(&quot;emuR&quot;) It is worth noting that throughout this manual R code snippets will be given in the form of the above snippet. These examples represent working R code that allow the reader to follow along in a hands-on manor and give a feel for what it is like working with the new EMU-SDMS. 2.2 EMU-SDMS: System architecture and default workflow As was previously mentioned, the new EMU-SDMS is made up of four main components. The components are the emuDB format; the R packages wrassp and emuR; and the web application, the EMU-webApp, which is EMU-SDMS’s new GUI component. An overview of the EMU-SDMS’s architecture and the components’ relationships within the system is shown in Figure 2.1. In Figure 2.1, the emuR package plays a central role as it is the only component that interacts with all of the other components of the EMU-SDMS. It performs file and DB handling for the files that comprise an emuDB (see Chapter @ref(chap:annot_struct_mod)); it uses the wrassp package for signal processing purposes (see Chapter 8; and it can serve emuDBs to the EMU-webApp (see Chapter 9). Figure 2.1: Schematic architecture of the EMU-SDMS Although the system is made of four main components, the user largely only interacts directly with the EMU-webApp and the emuR package. A summary of the default workflow illustrating theses interactions can be seen below: Load database into current R session (load_emuDB()). Database annotation / visual inspection (serve()). This opens up the EMU-webApp in the system’s default browser. Query database (query()). This is optionally followed by requery_hier() or requery_seq() as necessary (see Chapter 6 for details). Get trackdata (e.g. formant values) for the result of a query (get_trackdata()). Prepare data. Visually inspect data. Carry out further analysis and statistical processing. Initially the user creates a reference to an emuDB by loading it into their current R session using the load_emuDB() function (see step 1). This database reference can then be used to either serve (serve()) the database to the EMU-webApp or query (query()) the annotations of the emuDB (see steps 2 and 3). The result of a query can then be used to either perform one or more so-called requeries or extract signal values that correspond to the result of a query() or requery() (see step 4). Finally, the signal data can undergo further preparation (e.g., correction of outliers) and visual inspection before further analysis and statistical processing is carried out (see steps 5, 6 and 7). Although the R packages provided by the EMU-SDMS do provide functions for steps 4, 5 and 6, it is worth noting that the plethora of R packages that the R package ecosystem provides can and should be used to perform these duties. The resulting objects of most of the above functions are derived matrix or data.frame objects which can be used as inputs for hundreds if not thousands of other R functions. 2.3 EMU-SDMS: Is it something for you? Besides providing a fully integrated system, the EMU-SDMS has several unique features that set it apart from other current, widely used systems (e.g., Boersma and Weenink (2016), Wittenburg et al. (2006), Fromont and Hay (2012), Rose et al. (2006), McAuliffe and Sonderegger (2016)). To our knowledge, the EMU-SDMS is the only system that allows the user to model their annotation structures based on a hybrid model of time-based annotations (such as those offered by Praat’s tier-based annotation mechanics) and hierarchical timeless annotations. An example of such a hybrid annotation structure is displayed in Figure 2.2. These hybrid annotations benefit the user in multiple ways, as they reduce data redundancy and explicitly allow relationships to be expressed across annotation levels (see Chapter for further information on hierarchical annotations and Chapter on how to query these annotation structures). Figure 2.2: Example of a hybrid annotation combining time-based (Phonetic level) and hierarchical (Phoneme, Syllable, Text levels including the inter-level links) annotations. Further, to our knowledge, the EMU-SDMS is the first system that makes use of a web application as its primary GUI for annotating speech. This unique approach enables the GUI component to be used in multiple ways. It can be used as a stand-alone annotation tool, connected to a loaded emuDB via emuR’s serve() function and used to communicate to other servers. This enables it to be used as a collaborative annotation tool. An in-depth explanation of how this component can be used in these three scenarios is given in Chapter 9. As demonstrated in the default workflow of Section 2.2, an additional unique feature provided by EMU-SDMS is the ability to use the result of a query to extract derived (e.g., formants and RMS values) and complementary signals (e.g., electromagnetic articulography (EMA) data) that match the segments of a query. This, for example, aids the user in answering questions related to derived speech signals such as: Is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a strong or weak syllable?. Chapter gives a complete walk-through of how to go about answering this question using the tools provided by the EMU-SDMS. The features provided by the EMU-SDMS make it an all-in-one speech database management solution that is centered around R. It enriches the R platform by providing specialized speech signal processing, speech database management, data extraction and speech annotation capabilities. By achieving this without relying on any external software sources except the web browser, the EMU-SDMS significantly reduces the number of tools the speech and spoken language researcher has to deal with and helps to simplify answering research questions. As the only prerequisite for using the EMU-SDMS is a basic familiarity with the R platform, if the above features would improve your workflow, the EMU-SDMS is indeed for you. References "],
["chap-tutorial.html", "3 A tutorial on how to use the EMU-SDMS2 3.1 Converting the TextGrid collection 3.2 Loading and inspecting the database 3.3 Querying and autobuilding the annotation structure 3.4 Autobuilding 3.5 Signal extraction and exploration 3.6 Vowel height as a function of word types (content vs. function): evaluation and statistical analysis 3.7 Conclusion", " 3 A tutorial on how to use the EMU-SDMS2 Using the tools provided by the EMU-SDMS, this tutorial chapter gives a practical step-by-step guide to answering the question: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word? The tutorial only skims over many of the concepts and functions provided by the EMU-SDMS. In-depth explanations of the various functionalities are given in later chapters of this documentation. As the EMU-SDMS is not concerned with the raw data acquisition, other tools such as SpeechRecorder by Draxler and Jänsch (2004) are first used to record speech. However, once audio speech recordings are available, the system provides multiple conversion routines for converting existing collections of files to the new emuDB format described in Chapter 5 and importing them into the new EMU system. The current import routines provided by the emuR package are: convert_TextGridCollection() - Convert TextGrid collections (.wav and .TextGrid files) to the emuDB format, convert_BPFCollection() - Convert Bas Partitur Format (BPF) collections (.wav and .par files) to the emuDB format, convert_txtCollection() - Convert plain text file collections format (.wav and .txt files) to the emuDB format, convert_legacyEmuDB() - Convert the legacy EMU database format to the emuDB format and create_emuDB() followed by add_link/levelDefinition and import_mediaFiles() - Creating emuDBs from scratch with only audio files present. The emuR package comes with a set of example files and small databases that are used throughout the emuR documentation, including the functions help pages. These can be accessed by typing help(function_name) or the short form ?function_name. R code snippet below illustrates how to create this demo data in a user-specified directory. Throughout the examples of this documentation the directory that is provided by the base R function tempdir() will be used, as this is available on every platform supported by R (see ?tempdir for further details). As can be inferred from the list.dirs() output in the below code, the emuR_demoData directory contains a separate directory containing example data for each of the import routines. Additionally, it contains a directory containing an emuDB called ae (the directories name is ae_emuDB, where _emuDB is the default suffix given to directories containing a emuDB; see Chapter 5). # load the package library(emuR) # create demo data in directory provided by the tempdir() function # (of course other directory paths may be chosen) create_emuRdemoData(dir = tempdir()) # create path to demo data directory, which is # called &quot;emuR_demoData&quot; demo_data_dir = file.path(tempdir(), &quot;emuR_demoData&quot;) # show demo data directories list.dirs(demo_data_dir, recursive = F, full.names = F) ## [1] &quot;ae_emuDB&quot; &quot;BPF_collection&quot; &quot;legacy_ae&quot; ## [4] &quot;TextGrid_collection&quot; &quot;txt_collection&quot; This tutorial will start by converting a TextGrid collection containing seven annotated single-sentence utterances of a single male speaker to the emuDB format3. In the EMU-SDMS, a file collection such as a TextGrid collection refers to a set of file pairs where two types of files with different file extentions are present (e.g., .ext1 and .ext2). It is vital that file pairs have the same basenames (e.g., A.ext1 and A.ext2 where A represents the basename) in order for the conversion functions to be able to pair up files that belong together. As other speech software tools also encourage such file pairs (e.g., Kisler et al. (2015)) this is a common collection format in the speech sciences. The R code snippet below shows such a file collection that is part of emuR’s demo data. Figure 3.1 shows the content of an annotation as displayed by Praat’s \"Draw visible sound and Textgrid...\" procedure. # create path to TextGrid collection tg_col_dir = file.path(demo_data_dir, &quot;TextGrid_collection&quot;) # show content of TextGrid_collection directory list.files(tg_col_dir) ## [1] &quot;msajc003.TextGrid&quot; &quot;msajc003.wav&quot; &quot;msajc010.TextGrid&quot; ## [4] &quot;msajc010.wav&quot; &quot;msajc012.TextGrid&quot; &quot;msajc012.wav&quot; ## [7] &quot;msajc015.TextGrid&quot; &quot;msajc015.wav&quot; &quot;msajc022.TextGrid&quot; ## [10] &quot;msajc022.wav&quot; &quot;msajc023.TextGrid&quot; &quot;msajc023.wav&quot; ## [13] &quot;msajc057.TextGrid&quot; &quot;msajc057.wav&quot; Figure 3.1: TextGrid annotation of the emuR_demoData/TextGrid_collection/msajc003.wav / .TextGrid file pair containing the tiers (from top to bottom): Utterance, Intonational, Intermediate, Word, Accent, Text, Syllable, Phoneme, Phonetic, Tone, Foot. 3.1 Converting the TextGrid collection The convert_TextGridCollection() function converts a TextGrid collection to the emuDB format. A precondition that all .TextGrid files have to fulfill is that they must all contain the same tiers. If this is not the case, yet there is an equal tier subset that is contained in all the TextGrid files, this equal subset may be chosen. For example, if all .TextGrid files contain only the tier Phonetic: IntervalTier the conversion will work. However, if a single .TextGrid of the collection has the additional tier Tone: TextTier the conversion will fail. In this case the conversion could be made to work by specifying the equal subset (e.g., equalSubset = c(\"Phonetic\")) and passing it on to the tierNames function argument convert_TextGridCollection(..., tierNames = equalSubset, ...). As can be seen in Figure 3.1, the TextGrid files provided by the demo data contain eleven tiers. To reduce the complexity of the annotations for this tutorial we will only convert the tiers Word (content: C vs. function: F word annotations), Syllable (strong: S vs. weak: W syllable annotations), Phoneme (phoneme level annotations) and Phonetic (phonetic annotations using Speech Assessment Methods Phonetic Alphabet (SAMPA) symbols - Wells and others (1997)) using the tierNames parameter. This conversion can be seen in the R code snippet below. # convert TextGrid collection to the emuDB format convert_TextGridCollection(dir = tg_col_dir, dbName = &quot;my-first&quot;, targetDir = tempdir(), tierNames = c(&quot;Word&quot;, &quot;Syllable&quot;, &quot;Phoneme&quot;, &quot;Phonetic&quot;)) The above call to convert_TextGridCollection() creates a new emuDB directory in the tempdir() directory called my-first_emuDB. This emuDB contains annotation files that contain the same Word, Syllable, Phoneme and Phonetic segment tiers as the original .TextGrid files as well as copies of the original (.wav) audio files. For further details about the structure of an emuDB, see Chapter 5 of this document. 3.2 Loading and inspecting the database As mentioned in Section 2.2, the first step when working with an emuDB is to load it into the current R session. The R code snippet below shows how to load the converted TextGrid collection into R using the load_emuDB() function. # get path to emuDB called &quot;my-first&quot; # that was created by convert_TextGridCollection() path2directory = file.path(tempdir(), &quot;my-first_emuDB&quot;) # load emuDB into current R session db_handle = load_emuDB(path2directory, verbose = FALSE) 3.2.1 Overview Now the my-first emuDB is loaded into R, an overview of the current status and configuration of the database can be displayed using the summary() function as shown the below R code snippet. # show summary summary(db_handle) ## Name: my-first ## UUID: 57297507-ddf4-4bb7-a868-d7c1640dde7f ## Directory: /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T/RtmpunrHfU/my-first_emuDB ## Session count: 1 ## Bundle count: 7 ## Annotation item count: 664 ## Label count: 664 ## Link count: 0 ## ## Database configuration: ## ## SSFF track definitions: ## NULL ## ## Level definitions: ## name type nrOfAttrDefs attrDefNames ## 1 Word SEGMENT 1 Word; ## 2 Syllable SEGMENT 1 Syllable; ## 3 Phoneme SEGMENT 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; ## ## Link definitions: ## NULL The extensive output of summary() is split into a top and bottom half, where the top half focuses on general information about the database (name, directory, annotation item count, etc.) and the bottom half displays information about the various SSFF track, level and link definitions of the emuDB. The summary information about the level definitions shows, for instance, that the my-first database has a Word level of type SEGMENT and therefore contains annotation items that have a start time and a segment duration. It is worth noting that information about the SSFF track, level and link definitions corresponds to the output of the list_ssffTrackDefinitions(), list_levelDefinitions() and list_linkDefinitions() functions. 3.2.2 Database annotation and visual inspection The EMU-SDMS has a unique approach to annotating and visually inspecting databases, as it utilizes a web application called the EMU-webApp to act as its GUI. To be able to communicate with the web application the emuR package provides the serve() function which is used in the R code snippet below. # serve my-first emuDB to the EMU-webApp serve(db_handle) Executing this command will block the R console, automatically open up the system’s default browser and display the following message in the R console: ## Navigate your browser to the EMU-webApp URL: ## http://ips-lmu.github.io/EMU-webApp/ (should happen autom... ## Server connection URL: ## ws://localhost:17890 ## To stop the server press the &#39;clear&#39; button in the ## EMU-webApp or close/reload the webApp in your browser. The EMU-webApp, which is now connected to the database via the serve() function, can be used to visually inspect and annotate the emuDB. Figure 3.2 displays a screenshot of what the EMU-webApp looks like after automatically connecting to the server. As the EMU-webApp is a very feature-rich software annotation tool, this documentation has a whole chapter (see Chapter 9) on how to use it, what it is capable of and how to configure it. Further, the web application provides its own documentation which can be accessed by clicking the EMU icon in the top right hand corner of the application’s top menu bar. To close the connection and free up the blocked R console, simply click the clear button in the top menu bar of the EMU-webApp. Figure 3.2: Screenshot of EMU-webApp displaying msajc003 bundle of my-first emuDB. 3.3 Querying and autobuilding the annotation structure An integral step in the default workflow of the EMU-SDMS is querying the annotations of a database. The emuR package implements a query() function to accomplish this task. This function evaluates an EMU Query Language (EQL) expression and extracts the annotation items from the database that match a query expression. As chapter 6 gives a detailed description of the query mechanics provided by emuR, this tutorial will only use a very small, hopefully easy to understand subset of the EQL. The output of the summary() command in the R code snippet below and the screenshot in Figure 3.2 show that the my-first emuDB contains four levels of annotations. The R code snippet below shows four separate queries that query various segments on each of the available levels. The query expressions all use the matching operator == which returns annotation items whose labels match those specified to the right of the operator and that belong to the level specified to the left of the operator (i.e., LEVEL == LABEL; see Chapter 6 for a detailed description). # query all segments containing the label # &quot;C&quot; (== content word) of the &quot;Word&quot; level sl_text = query(emuDBhandle = db_handle, query = &quot;Word == C&quot;) # query all segments containing the label # &quot;S&quot; (== strong syllable) of the &quot;Syllable&quot; level sl_syl = query(emuDBhandle = db_handle, query = &quot;Syllable == S&quot;) # query all segments containing the label # &quot;f&quot; on the &quot;Phoneme&quot; level sl_phoneme = query(db_handle, query = &quot;Phoneme == f&quot;) # query all segments containing the label # &quot;n&quot; of the &quot;Phonetic&quot; level sl_phonetic = query(db_handle, query = &quot;Phonetic == n&quot;) # show class vector of query result class(sl_phonetic) ## [1] &quot;emuRsegs&quot; &quot;emusegs&quot; &quot;data.frame&quot; # show first entry of sl_phonetic head(sl_phonetic, n = 1) ## segment list from database: my-first ## query was: Phonetic == n ## labels start end session bundle level type ## 1 n 1031.925 1195.925 0000 msajc003 Phonetic SEGMENT # show summary of sl_phonetic summary(sl_phonetic) ## segment list from database: my-first ## query was: Phonetic == n ## with 12 segments ## ## Segment distribution: ## ## n ## 12 As demonstrated in the above R code, the result of a query is an emuRsegs object, which is a super-class of the common data.frame. This object is often referred to as a segment list, or “seglist”. A segment list carries information about the extracted annotation items such as the extracted labels, the start and end times of the segments, the sessions and bundles the items are from and the levels they belong to. An in-depth description of the information contained in a segment list is given in Section 6.1. The above R code snippet shows that the summary() function can also be applied to a segment list object to get an overview of what is contained within it. This can be especially useful when dealing with larger segment lists. 3.4 Autobuilding The simple queries illustrated above query segments from a single level that match a certain label. However, the EMU-SDMS offers a mechanism for performing inter-level queries such as: Query all Phonetic items that contain the label “n” and are part of a content word. For such queries to be possible, the EMU-SDMS offers very sophisticated annotation structure modeling capabilities, which are described in Chapter 4. For the sake of this tutorial we will focus on converting the flat segment level annotation structure displayed in Figure 3.2 to a hierarchical form as displayed in Figure 3.3, where only the Phonetic level carries time information and the annotation items on the other levels are explicitly linked to each other to form a hierarchical annotation structure. Figure 3.3: Example of a hierarchical annotation of the content (==C) word violently belonging to the msajc012 bundle of the my-first demo emuDB. As it is a very laborious task to manually link annotation items together using the EMU-webApp and the hierarchical information is already implicitly contained in the time information of the segments and events of each level, we will now use a function provided by the emuR package to build these hierarchical structures using this information called autobuild_linkFromTimes(). The above R code snippet shows the calls to this function which autobuild the hierarchical annotations in the my-first database. As a general rule for autobuilding hierarchical annotation structures, a good strategy is to start the autobuilding process beginning with coarser grained annotation levels (i.e., the Word/Syllable level pair in our example) and work down to finer grained annotations (i.e., the Syllable/Phoneme and Phoneme/Phonetic level pairs in our example). To build hierachical annotation structures we need link definitions, which together with the level definitions define the annotation structure for the entire database (see Chapter 4 for further details). The autobuild_linkFromTimes() calls in the below R code snippet use the newLinkDefType parameter, which if defined automatically adds a link definition to the database. # invoke autobuild function # for &quot;Word&quot; and &quot;Syllable&quot; levels autobuild_linkFromTimes(db_handle, superlevelName = &quot;Word&quot;, sublevelName = &quot;Syllable&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;ONE_TO_MANY&quot;) # invoke autobuild function # for &quot;Syllable&quot; and &quot;Phoneme&quot; levels autobuild_linkFromTimes(db_handle, superlevelName = &quot;Syllable&quot;, sublevelName = &quot;Phoneme&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;ONE_TO_MANY&quot;) # invoke autobuild function # for &quot;Phoneme&quot; and &quot;Phonetic&quot; levels autobuild_linkFromTimes(db_handle, superlevelName = &quot;Phoneme&quot;, sublevelName = &quot;Phonetic&quot;, convertSuperlevel = TRUE, newLinkDefType = &quot;MANY_TO_MANY&quot;) Figure 3.4: Schematic annotation structure of the emuDB after calling the autobuild function in R code snippet above. As the autobuild_linkFromTimes() function automatically creates backup levels to avoid the accidental loss of boundary or event time information, the R code snippet below shows how these backup levels can be removed to clean up the database. However, using the remove_levelDefinition() function with its force parameter set to TRUE is a very invasive action. Usually this would not be recommended, but for this tutorial we are keeping everything as clean as possible. # list level definitions # as this reveals the &quot;-autobuildBackup&quot; levels # added by the autobuild_linkFromTimes() calls list_levelDefinitions(db_handle) ## name type nrOfAttrDefs attrDefNames ## 1 Word ITEM 1 Word; ## 2 Syllable ITEM 1 Syllable; ## 3 Phoneme ITEM 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; ## 5 Word-autobuildBackup SEGMENT 1 Word-autobuildBackup; ## 6 Syllable-autobuildBackup SEGMENT 1 Syllable-autobuildBackup; ## 7 Phoneme-autobuildBackup SEGMENT 1 Phoneme-autobuildBackup; # remove the levels containing the &quot;-autobuildBackup&quot; # suffix remove_levelDefinition(db_handle, name = &quot;Word-autobuildBackup&quot;, force = TRUE, verbose = FALSE) remove_levelDefinition(db_handle, name = &quot;Syllable-autobuildBackup&quot;, force = TRUE, verbose = FALSE) remove_levelDefinition(db_handle, name = &quot;Phoneme-autobuildBackup&quot;, force = TRUE, verbose = FALSE) # list level definitions list_levelDefinitions(db_handle) ## name type nrOfAttrDefs attrDefNames ## 1 Word ITEM 1 Word; ## 2 Syllable ITEM 1 Syllable; ## 3 Phoneme ITEM 1 Phoneme; ## 4 Phonetic SEGMENT 1 Phonetic; # list level definitions # which were added by the autobuild functions list_linkDefinitions(db_handle) ## type superlevelName sublevelName ## 1 ONE_TO_MANY Word Syllable ## 2 ONE_TO_MANY Syllable Phoneme ## 3 MANY_TO_MANY Phoneme Phonetic As can be seen by the output of list_levelDefinitions() and list_linkDefinitions() in the above R code, the annotation structure of the my-first emuDB now matches that displayed in Figure 3.4. Using the serve() function to open the emuDB in the EMU-webApp followed by clicking on the show hierarchy button in the top menu (and rotating the hierarchy by 90 degrees by clicking the rotate by 90 degrees button) will result in a view similar to the screenshot of Figure 3.5. Figure 3.5: Screenshot of EMU-webApp displaying the autobuilt hierarchy of the my-first emuDB. 3.4.1 Querying the hierarchical annotations Having this hierarchical annotation structure now allows us to formulate a query that helps answer the originally stated question: Given an annotated speech database, is the vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word?. The R code snippet below shows how all the @ vowels in the my-first database are queried. # query annotation items containing # the labels @ on the Phonetic level sl_vowels = query(db_handle, &quot;Phonetic == @&quot;) # show first entry of sl_vowels head(sl_vowels, n = 1) ## segment list from database: my-first ## query was: Phonetic == @ ## labels start end session bundle level type ## 1 @ 1506.175 1548.425 0000 msajc003 Phonetic SEGMENT As the type of word (content vs. function) for each @ vowel that was just extracted is also needed, we can use the requery functionality of the EMU-SDMS (see Chapter 6) to retrieve the word type for each @ vowel. A requery essentially moves through a hierarchical annotation (vertically or horizontally) starting from the segments that are passed into the requery function. The R code below illustrates the usage of the hierarchical requery function, requery_hier(), to retrieve the appropriate annotation items from the Word level. # hierarchical requery starting from the items in sl_vowels # and moving up to the &quot;Word&quot; level sl_word_type = requery_hier(db_handle, seglist = sl_vowels, level = &quot;Word&quot;, calcTimes = FALSE) # show first entry of sl_word_type head(sl_word_type, n = 1) ## segment list from database: my-first ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 F NA NA 0000 msajc003 Word ITEM # show that sl_vowel and sl_word_type have the # same number of row entries nrow(sl_vowels) == nrow(sl_word_type) ## [1] TRUE As can be seen by the nrow() comparison in the above R code, the segment list returned by the requery_hier() function has the same number of rows as the original sl_vowels segment list. This is important, as each row of both segment lists line up and allow us to infer which segment belongs to which word type (e.g., vowel sl_vowels[5,] belongs to the word type sl_word_type[5,]). 3.5 Signal extraction and exploration Now that the vowel and word type information including the vowel start and end time information has been extracted from the database, this information can be used to extract signal data that matches these segments. Using the emuR function get_trackdata() we can calculate the formant values in real time using the formant estimation function, forest(), provided by the wrassp package (see Chapter 8 for details). The following R code shows the usage of this function. # get formant values for the vowel segments td_vowels = get_trackdata(db_handle, seglist = sl_vowels, onTheFlyFunctionName = &quot;forest&quot;, resultType = &quot;tibble&quot;, verbose = F) # show class vector class(td_vowels) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # show dimensions dim(td_vowels) ## [1] 287 24 # show nr of segments max(td_vowels$sl_rowIdx) ## [1] 28 # display all values for fifth segment using dplyr library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union td_vowels %&gt;% filter(sl_rowIdx == 5) ## # A tibble: 12 x 24 ## sl_rowIdx labels start end utts db_uuid session bundle start_item_id ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 2 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 3 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 4 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 5 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 6 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 7 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 8 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 9 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 10 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 11 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## 12 5 @ 2447. 2506. 0000… 572975… 0000 msajc… 119 ## # … with 15 more variables: end_item_id &lt;int&gt;, level &lt;chr&gt;, ## # start_item_seq_idx &lt;int&gt;, end_item_seq_idx &lt;int&gt;, type &lt;chr&gt;, ## # sample_start &lt;int&gt;, sample_end &lt;int&gt;, sample_rate &lt;int&gt;, ## # times_orig &lt;dbl&gt;, times_rel &lt;dbl&gt;, times_norm &lt;dbl&gt;, T1 &lt;int&gt;, ## # T2 &lt;int&gt;, T3 &lt;int&gt;, T4 &lt;int&gt; As can be seen by the call to the class() function, the resulting object is of the type tibble (see ?tibble::tibble for more information) and has 28 blocks of data. These blocks correspond to the number of rows contained in the segment lists extracted above (i.e., nrow(sl_vowels)) and can be matched to the according segment using the sl_rowIdx column (i.e., td_vowels %&gt;% filter(sl_rowIdx == 5) are the formant values belonging to sl_vowels[5,]). As the columns T1, T2, T3, T4 of the printed output of td_vowels %&gt;% filter(sl_rowIdx == 5) suggest, the forest function estimates four formant values. We will only be concerned with the first (column T1) and second (column T2). The below R code shows two ggplot() function calls which produce the plots displayed in Figures 3.6 and 3.6. The first ggplot() call plots all 28 first formant trajectories (achieved by setting the group parameter to sl_rowIdx). To clean up the cluttered first plot, the second ggplot() call uses a segment length normalized version of td_vowels (see ?normalize_length for futher details) as well as using geom_smooth() in combination with setting the group parameter to labels to plot only smoothed conditional means of all @ vowels. # load package library(ggplot2) ggplot(td_vowels) + aes(x = times_rel, y = T1, col = labels, group = sl_rowIdx) + geom_line() + labs(x = &quot;Duration (ms)&quot;, y = &quot;F1 (Hz)&quot;) # normalize length of segments td_vowels_norm = normalize_length(td_vowels) ggplot(td_vowels_norm) + aes(x = times_norm, y = T1, col = labels, group = labels) + geom_smooth() + labs(x = &quot;Duration (normalized)&quot;, y = &quot;F1 (Hz)&quot;) Figure 3.6: ggplot() plots of all F1 @ vowel trajectories. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 3.7: ggplot() plots of the F1 smoothed conditional mean trajectories of all @ vowels. Figures 3.6 and 3.7 give an overview of the first formant trajectories of the @ vowels. For the purpose of data exploration and to get an idea of where the individual vowel classes lie on the F2 x F1 plane, which indirectly provides information about vowel height and tongue position, the R code below again makes use of the ggplot() function. This produces Figure 3.8. To be able to use the stat_ellipse() function, the td_vowels_norm object first has to be modified, as it contains entire formant trajectories but two dimensional data is needed to be able to display it on the F2 x F1 plain. This can, for example, be achieved by only extracting temporal mid-point formant values for each vowel using the get_trackdata() function utilizing its cut parameter. The R code snippet below shows an alternative approach using the dplyr’s filter() function to essentially cut the formant trajectories to a specified proportional segment. By only extracting the values that have a normalized time value of 0.5 (times_norm == 0.5) only the formant values that are at the time normalized mid-point (calculated above using the normalize_length() function) are extracted from the trajectories. # cut formant trajectories at temporal mid-point td_vowels_midpoint = td_vowels_norm %&gt;% filter(times_norm == 0.5) # show dimensions of td_vowels_midpoint dim(td_vowels_midpoint) # calculate centroid td_centroids = td_vowels_midpoint %&gt;% group_by(labels) %&gt;% summarise(T1 = mean(T1), T2 = mean(T2)) # generate plot ggplot(td_vowels_midpoint, aes(x = T2, y = T1, colour = labels, label = labels)) + geom_text(data = td_centroids) + stat_ellipse() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position=&quot;none&quot;) ## [1] 28 24 Figure 3.8: 95% ellipse plot including centroid for F2 x F1 data extracted from the temporal midpoint of the vowel segments. Figure 3.8 displays the first two formants extracted at the temporal midpoint of every @ vowel in sl_vowels. The centroid of these formants is plotted on the F2 x F1 plane, and their 95% ellipsis distribution is also shown. Although not necessarily applicable to the question posed at the beginning of this tutorial, the data exploration using the dplyr and ggplot2 packages can be very helpful tools for providing an overview of the data at hand. 3.6 Vowel height as a function of word types (content vs. function): evaluation and statistical analysis The above data exploration only dealt with the actual @ vowels and disregarded the syllable type they occurred in. However, the question in the introduction of this chapter focuses on whether the @ vowel occurs in a content (labeled C) or function (labeled F) word. For data inspection purposes, the R code snippet below initially extracts the central 60% (filter() conditions times_norm &gt;= 0.2 and times_norm &lt;= 0.8) of the formant trajectories from td_vowels_norm using dplyr and displays them using ggplot(). It should be noted that before the call to ggplot() the labels of the td_vowels_mid_sec are replaced with those of sl_word_type. This allows ggplot() to group the trajectories by their word type as opposed to their vowel labels as displayed in Figure 3.9. # extract central 60% from formant trajectories td_vowels_mid_sec = td_vowels_norm %&gt;% filter(times_norm &gt;= 0.2, times_norm &lt;= 0.8) # replace labels with those of sl_word_type td_vowels_mid_sec$labels = sl_word_type$labels[td_vowels_mid_sec$sl_rowIdx] ggplot(td_vowels_mid_sec) + aes(x = times_norm, y = T1, col = labels, group = labels) + geom_smooth() + labs(x = &quot;Duration (normalized)&quot;, y = &quot;F1 (Hz)&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Figure 3.9: Ensemble averages of F1 contours of all tokens of the central 60% of vowels grouped by word type (function (F) vs. content (W)). As can be seen in Figure 3.9, there seems to be a distinction in F1 trajectory height between vowels in content and function words. The following R snippet shows the code to produce a boxplot once again using the ggplot2 package to further visually inspect the data (see Figure 3.10 for the plot produced by the below code). # use group_by + summarise to calculate the means of the 60% # formant trajectories td_vowels_mid_sec_mean = td_vowels_mid_sec %&gt;% group_by(sl_rowIdx) %&gt;% summarise(labels = unique(labels), meanF1 = mean(T1)) # create boxplot using ggplot ggplot(td_vowels_mid_sec_mean, aes(labels, meanF1)) + geom_boxplot() + labs(x = &quot;Word type&quot;, y = &quot;mean F1 (Hz)&quot;) Figure 3.10: Boxplot produced using ggplot2 to visualize the difference in F1 depending on whether the vowel occurs in content (C) or function (F) word. To confirm or reject this, the following R code presents a very simple statistical analysis of the F1 mean values of the 60% mid-section formant trajectories.4 First, a Shapiro-Wilk test for normality of the distributions of the F1 means for both word types is carried out. As only one type is normally distributed, a Wilcoxon rank sum test is performed. The density distributions (commented out plot() function calls in the code below) are displayed in Figure 3.11. # calculate density for vowels in function words distrF = density(td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;F&quot;,]$meanF1) # uncomment to visualize distribution # plot(distrF) # check that vowels in function # words are normally distributed shapiro.test(td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;F&quot;,]$meanF1) ## ## Shapiro-Wilk normality test ## ## data: td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;F&quot;, ]$meanF1 ## W = 0.98618, p-value = 0.9868 # p-value &gt; 0.05 implying that the distribution # of the data ARE NOT significantly different from # normal distribution -&gt; we CAN assume normality # calculate density for vowels in content words distrC = density(td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;C&quot;,]$meanF1) # uncomment to visualize distribution # plot(distrC) # check that vowels in content # words are normally distributed: shapiro.test(td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;C&quot;,]$meanF1) ## ## Shapiro-Wilk normality test ## ## data: td_vowels_mid_sec_mean[td_vowels_mid_sec_mean$labels == &quot;C&quot;, ]$meanF1 ## W = 0.67216, p-value = 1.819e-05 # p-value &lt; 0.05 implying that the distribution # of the data ARE significantly different from # normal distribution -&gt; we CAN NOT assume normality # (this somewhat unexpected result is probably # due to the small sample size used in this toy example) # -&gt; use Wilcoxon rank sum test # perform Wilcoxon rank sum test to establish # whether vowel F1 depends on word type wilcox.test(meanF1 ~ labels, data = td_vowels_mid_sec_mean) ## ## Wilcoxon rank sum test ## ## data: meanF1 by labels ## W = 119, p-value = 0.04875 ## alternative hypothesis: true location shift is not equal to 0 Figure 3.11: Plots of density distributions of vowels in content words (left plot) and vowels in function words (right plot) of the above R code. As shown by the result of wilcox.test() in the above R code, word type (C vs. F) has a significant influence on the vowel’s F1 (W=121, p&lt;0.05). Hence, the answer to the initially proposed question: Given an annotated speech database, is vowel height of the vowel @ (measured by its correlate, the first formant frequency) influenced by whether it appears in a content or function word? is yes! 3.7 Conclusion The tutorial given in this chapter gave an overview of what it is like working with the EMU-SDMS to try to solve a research question. As many of the concepts were only briefly explained, it is worth noting that explicit explanations of the various components and integral concepts are given in following chapters. Further, additional use cases that have been taken from the emuR_intro vignette can be found in Appendix 14. These use cases act as templates for various types of research questions and will hopefully aid the user in finding a solution similar to what she or he wishes to achieve. References "],
["chap-annot-struct-mod.html", "4 Annotation Structure Modeling5 4.1 Per database annotation structure definition 4.2 Parallel labels and multiple attributes 4.3 Metadata strategy using single bundle root nodes 4.4 Conclusion", " 4 Annotation Structure Modeling5 The EMU-SDMS facilitates annotation structure modeling that surpasses that available in many other commonly used systems. This chapter provides an in-depth explanation of the annotation structure modeling capabilities the EMU-SDMS offers. One of the most common approaches for creating time-aligned annotations has been to differentiate between events that occur at a specific point in time but have no duration and segments that start at a point in time and have a duration. These annotation items are then grouped into time-ordered sets that are often referred to as tiers. As certain research questions benefit from different granularities of annotation, the timeline is often used to relate implicitly items from multiple tiers to each other as shown in Figure 4.1A. While sufficient for single or unrelated tier annotations, we feel this type of representation is not suitable for more complex annotation structures, as it results in unnecessary, redundant data and data sets that are often difficult to analyze. This is because there are no explicit relationships between annotation items, and it is often necessary to introduce error tolerance values to analyze slightly misaligned time values to find relationships iteratively over multiple levels. The main reason for the prevalence of this sub-optimal strategy is largely because the available software tools (e.g., Praat by Boersma and Weenink (2016)) do not permit any other forms of annotations. These widely used annotation tools often only permit the creation and manipulation of segment and event tiers which in turn has forced users to model their annotation structures on these building blocks alone. Linguists who deal with speech and language on a purely symbolic level tend to be more familiar with a different type of annotation structure modeling. They often model their structures in the form of a vertically oriented, directed acyclic graph that, but for a few exceptions that are needed for things like elision modeling (e.g., the /ɪ/ elision that may occur between the canonical representation of the word family /fæmɪli/ and its phonetic representation [fæmli]), loosely adheres to the formal definition of a tree in the graph-theoretical sense (Knuth (1968)) as depicted in Figure 4.1B. While this form of modeling explicitly defines relationships between annotation items (represented by dashed lines in Figure 4.1B), it lacks the ability to map these items to the timeline and therefore the matching speech signal. Figure 4.1: A: a purely time-aligned annotation; B: a purely timeless, symbolic annotation; C: a time-aligned hierarchical annotation. To our knowledge, the legacy EMU system (Cassidy and Harrington (2001)) and its predecessors (e.g., Harrington et al. (1993)) were the first to fuse pragmatically purely time-aligned and symbolic tree-like annotations. This was achieved by providing software tools that allowed for these types of annotation structures to be generated, queried and evaluated. In practice, each annotation item had its own unique identifier within the annotation. These unique IDs could then be used to reference each individual item and link them together using dominance relations to form the hierarchical annotation structure. On the one hand, this dominance relation implies the temporal inclusion of the linked sub-level items and was partially predicated on the no-crossing constraint as described in Coleman and Local (1991)). This constraint does not permit the crossing of dominance relationships with respect to their sequential ordering (see also Section 4.2 of Cassidy and Harrington (2001)). Since the dominance relations imply temporal inclusion, events can only be children in a parent-child relationship. To allow for timeless annotation items, a further timeless level type was used to complement the segment and event type levels used for time-aligned annotations. Each level of annotation items was stored as an ordered set to ensure the sequential integrity of both the time-aligned and timeless item levels. The legacy system also reduced data redundancy by allowing parallel annotations to be defined in the form of linearly linked levels for any given level (e.g., a segment level bearing SAMPA annotations as well as IPA UTF-8 annotations). The new EMU-SDMS has adopted some concepts of the legacy system in that levels of type SEGMENT and EVENT contain annotation items with labels and time information, similar to the tiers known from other software tools such as Praat, while levels of type ITEM are timeless and contain annotation items with labels only. SEGMENT and EVENT levels differ in that units at the SEGMENTs level have a start time and a duration, while units at the EVENT level contain a single time point only. Additionally, every annotation item is able to contain multiple labels and has a unique identifier which is used to link items across levels. These building blocks provide the user with a general purpose annotation modeling tool that allows complex annotation structures to be modeled that best represent the data. An example of a time-aligned hierarchical annotation is depicted in Figure 4.1C, which essentially combines the annotation of Figure 4.1B with the most granular time-bearing level (i.e. the “Phonetic” level) of Figure 4.1A. In accordance with other approaches (among others see Bird and Liberman (2001), Zipser and Romary (2010), Ide and Romary (2004)), the EMU-SDMS annotation structure can be viewed as a graph that consists of three types of nodes (EVENTs, SEGMENTs, ITEMs) and two types of relations (dominance and sequence) which are directed, transitive and indicate the dominance and sequential relationships between nodes of the graph. As was shown in a pseudo-code example that converted an Annotation Graph (Bird and Liberman (2001)) into the legacy EMU annotation format in Cassidy and Harrington (2001), these formats can be viewed as conceptually equivalent sub- or super-set representations of each other. This has also been shown by developments of meta models with independent data representation such as Salt (Zipser and Romary (2010)), which enable abstract internal representations to be derived that can be exported to equal-set or super-set formats without the loss of information. We therefore believe that the decision as to which data format serializations are used by a given application should be guided by the choice of technology and the target audience or research field. This is consistent with the views of the committee for the Linguistic Annotation Framework (LAF) who explicitly state in the ISO CD 24612 (LAF) document (ISO (2012)); Although the LAF pivot format may be used in any context, it is assumed that users will represent annotations using their own formats, which can then be transduced to the LAF pivot format for the purposes of exchange, merging and comparison. The transduction of an EMU annotation into a format such as the LAF pivot format is a simple process, as they share many of the same concepts and are well defined formats. 4.1 Per database annotation structure definition Unlike other systems, the EMU-SDMS requires the user to define the annotation structure formally for all annotations within a database. Much as Document Type Definition (DTD) or XML Schema Definition (XSD) describe the syntactically valid elements in an Extensible Markup Language (XML) document, the database configuration file of an emuDB defines the valid annotation levels and therefore the type of items that are allowed to be present in a database. Unlike DTDs or XSDs, the configuration file can also define semantic relationships between annotation levels which fall outside the scope of traditional, syntactically oriented schema definitions and validation. This global definition of an annotation structure has numerous benefits for the data integrity of the database, as the EMU-SDMS can perform consistency checks and prevent malformed as well as semantically void annotation structures.6 Because of these formal definitions, the EMU system generally distinguishes between the actual representations of a structural element which are contained within the database and their formal definitions. An example of an actual representation, that is a subset of the actual annotation, would be a level contained in an annotation file that contains SEGMENTs that annotate a recording. The corresponding formal definition would be a level definition entry in the database’s configuration file, which specifies and validates the level’s existence within the database. As mentioned above, the actual annotation files of an emuDB contain the annotation items as well as their hierarchical linking information. To be able to check the validity of a connection between two items, the user specifies which links are permitted for the entire database just as for the level definitions. The permitted hierarchical relationships in an emuDB are expressed through link definitions between level definitions as part of the database configuration. There are three types of valid hierarchical relationships between levels: ONE_TO_MANY, MANY_TO_MANY and ONE_TO_ONE. These link definitions specify the permitted relationships between instances of annotation items of one level and those of another. The structure of the hierarchy that corresponds to the annotation depicted in Figure 4.1C can be seen in Figure 4.2A. The structure in Figure 4.2A is a typical example of an EMU hierarchy where only the level of type SEGMENT contains time information and the others are timeless as they are of the type ITEM. The top three levels, Text, Syllable and Phoneme, have a ONE_TO_MANY relationship specifying that a single item in the parent level may have a dominance relationship with multiple items in the child level. In this example, the relationship between and Phonetic is MANY_TO_MANY: this type of relationship can be used to represent schwa elision and subsequent sonorant syllabification, as when the final syllable of sudden is d@n at the Phoneme level but dn at the Phonetic level. Figure 4.2B displays an example of a more complex, intersecting hierarchical structure definition where Abercrombian feet (Abercombie (1967)) are incorporated into the tones and break indices (ToBI) (Beckman and Ayers (1997)) prosodic hierarchy by allowing an intonational phrase to be made up of one or more feet (for further details see Harrington (2010) page 98). Figure 4.2: A: a schematic representation of the hierarchical structure of an emuDB that corresponds to the annotation depicted in figure 4.1C; B: example of a more complex, intersecting hierarchical structure. Based on our experience, the explicit definition of the annotation structure for every database which was also integral to the legacy system addresses the excessively expressive nature of annotational modeling systems mentioned in Bird and Liberman (2001). Although, in theory, infinitely large hierarchies can be defined for a database, users of the legacy system typically chose to use only moderately complex annotation structures. The largest hierarchy definitions we have encountered spanned up to fifteen levels while the average amount of levels was between three and five. This self-restriction is largely due to the primary focus of speech and spoken language domain-specific annotations, as the number of annotation levels between chunks of speech above the word level (intonational phrases/sentences/turns/etc.) and the lower levels (phonetic segmentation/EMA gestural landmark annotation/tone annotation/etc.) is a finite set. 4.2 Parallel labels and multiple attributes The legacy EMU system made a distinction between linearly and non-linearly linked inter-level links. Linearly linked levels were used to describe, enrich or supplement another level. For example, a level called Category might have been included as a separate level from Word for marking words’ grammatical category memberships (thus each word might be marked as one of adjective, noun, verb, etc.), or information about whether or not a syllable is stressed might be included on a separate Stress tier (description taken from Harrington (2010) page 77). Using ONE_TO_ONE link definitions to define a relationship between two levels, it is still possible to model linearly linked levels in the new EMU-SDMS. However, an additional, cleaner concept that reduces the extra level overhead has been implemented that allows every annotation item to carry multiple attributes (i.e., labels). Further, using this construct reduces the number of levels, items and links and therefore the hierarchical complexity of an annotation. The generic term “attribute” (vs. “label”) was chosen to have the flexibility of adding attributes that are not of the type STRING (i.e., labels) to the annotation modeling capabilities of the EMU-SDMS in future versions. Figure 4.3 shows the annotation structure modeling difference between linearly linked levels (see Figure 4.3A) and an annotation structure using multiple attributes (see Figure 4.3B). Figure 4.3A shows three separate levels (Word, Accent and Text) that have a ONE_TO_ONE relationship. Each of their annotation items is linked to exactly one annotation item in the child level (e.g., A1-A3). Figure 4.3B shows a single level that has three attribute definitions (Word, Accent and Text) and each annotation item contains three attributes (e.g., A1-A3). Figure 4.3: Schematic representation of annotation structure modeling difference between A: linearly linked levels and B: an annotation structure using multiple attributes. It is worth noting that every level definition must have an attribute definition which matches its level name. This primary attribute definition must also be present in every annotation item belonging to a level. As emuR’s database interaction functions, such as add_levelDefinition(), and the EMU-webApp automatically perform the necessary actions this should only be of interest to (semi-)advanced users wishing to automatically generate the _annot.json format. 4.3 Metadata strategy using single bundle root nodes As the legacy EMU system and the new EMU-SDMS do not have an explicit method for storing metadata associated with bundles7, over the years an annotation structure convention has been developed to combat this issue. The convention is to use a generic top level (often simply called bundle) that contains a single annotation item in every annotation file. Using the multiple attribute annotation structure modeling capability of the EMU-SDMS, this single annotation item can hold any meta data associated with the bundle. Additionally linking the item to all the annotation items of its child level effectively makes it a parent to every item of the hierarchy. This linking information can later be exploited to query only bundles with matching meta data (see Chapter 6 for details). Figure 4.4 displays a hierarchical annotation where the top level (bundle) contains information about the speaker’s gender, the city of birth (COB) and age. Figure 4.4: Hierarchical annotation displaying single bundle root node metadata strategy where the label of the primary attribute definition (bundle) is empty, gender encodes the speaker’s gender, COB encodes the speakers city of birth and age encodes the speaker’s age in the form of a string. 4.4 Conclusion The annotation structure modeling capabilities of the EMU-SDMS surpass those of many other commonly used systems. They do so by not only allowing the use of levels containing time information (levels of type SEGMENT and EVENT) but also timeless levels (levels of type ITEM). Additionally, they allow users to define hierarchical annotation structures by allowing explicit links to be implemented from one level’s items to those of another. Although it is not obligatory to use them in the EMU-SDMS, we feel the usage of hierarchical annotations allow for complex rich data modeling and are often cleaner representations of the annotations at hand. References "],
["chap-emuDB.html", "5 The emuDB Format8 5.1 Database design 5.2 Creating an emuDB 5.3 Conclusion", " 5 The emuDB Format8 This chapter describes the emuDB format, which is the new database format of the EMU-SDMS, and shows how to create and interact with this format. The emuDB format is meant as a simple, general purpose way of storing speech databases that may contain complex, rich, hierarchical annotations as well as derived and complementary speech data. These different components will be described throughout this chapter, and examples will show how to generate and manipulate them. On designing the new EMU system, considerable effort went into designing an appropriate database format. We needed a format that was standardized, well structured, easy to maintain, easy to produce, easy to manipulate and portable. We decided on the JavaScript Object Notation (JSON) file format9 as our primary data source for several reasons. It is simple, standardized, widely-used and text-based as well as machine and human readable. In addition, this portable text format allows expert users to (semi-) automatically process and/or generate annotations. Other tools such as the BAS Webservices (Kisler, Schiel, and Sloetjes 2012) and SpeechRecorder (Draxler and Jänsch 2004) have already taken advantage of being able to produce such annotations. Using database back-end options such as relational or graph databases of either the SQL or NoSQL variety as the primary data source for annotations would not directly permit other tools to produce annotations because intermediary exchange file formats would have to be defined to permit this functionality with these back-ends. Our choice of the JSON format was also guided by the decision to incorporate web technologies as part of the EMU-SDMS for which the JSON format is the de facto standard (see Chapter 9). Further, as the default encoding of the JSON format is UTF-8 the EMU-SDMS fully supports the Unicode character set for any user-defined string within an emuDB (e.g. level names and labels)10. We chose to use the widely adopted Waveform Audio File Format (WAVE, or more commonly known as WAV due to its filename extension) as our primary media/audio format. Although some components of the EMU-SDMS, notably the wrassp package, can handle various other media/audio formats (see ?wrassp::AsspFileFormats for details) this is the only audio file format currently supported by every component of the EMU-SDMS. Nevertheless, the wrassp package can be utilized to convert files from one of it’s other supported file formats to the WAV format.11 Future releases of the EMU-SDMS might include the support of other media/audio formats. In contrast to other systems, including the legacy EMU system, we chose to fully standardize the on-disk structure of speech databases with which the system is capable of working. This provides a standardized and structured way of storing speech databases while providing the necessary amount of freedom and separability to accommodate multiple types of data. Further, this standardization enables fast parsing and simplification of file-based error tracking and simplifies database subset and merging operations as well as database portability. An overview of all database interaction functions is given in Section 10.2. 5.1 Database design An emuDB consists of a set of files and directories that adhere to a certain structure and naming convention (see Figure 5.1). The database root directory must include a single _DBconfig.json file that contains the configuration options of the database such as its level definitions, how these levels are linked in the database hierarchy and how the data is to be displayed by the graphical user interface. A detailed description of the _DBconfig.json file is given in Appendix 15.1.1. The database root directory also contains arbitrarily named session directories (except for the obligatory _ses suffix). These session directories can be used to group the recordings of a database in a logical manner. Sessions can be used, for example, to group all recordings from speaker AAA into a session called AAA_ses. Figure 5.1: Schematic emuDB file and directory structure. Each session directory can contain any number of _bndl directories (e.g., rec1_bndl rec2_bndl … rec9_bndl). All files belonging to a recording (i.e., all files describing the same timeline) are stored in the same bundle directory. This includes the actual recording (.wav) and can contain optional derived or supplementary signal files in the simple signal file format (SSFF) (Cassidy 2013) such as formants (.fms) or the fundamental frequency (.f0), both of which can be calculated using the wrassp package (see Chapter 8). Each bundle directory contains the annotation file (_annot.json) of that bundle (i.e., the annotations and the hierarchical linking information; see Appendix 15.1.2 for a detailed description of the file format). JSON schema files for all the JSON files types used have been developed to ensure the syntactic integrity of the database (see https://github.com/IPS-LMU/EMU-webApp/tree/master/dist/schemaFiles). All files within a bundle that are associated with that bundle must have the same basename as the _bndl directory prefix. For example, the signal file in bundle rec1_bndl must have the name rec1.wav to be recognized as belonging to the bundle. The optional _emuDBcache.sqlite file in the root directory (see Figure 5.1 contains the relational cache representation of the annotations of the emuDB (see Chapter 11 for further details). All files in an _bndl directory that do not follow the above naming conventions will simply be ignored by the database interaction functions of the emuR package. 5.2 Creating an emuDB The two main strategies for creating emuDBs are either to convert existing databases or file collections to the new format or to create new databases from scratch where only .wav audio files are present. Chapter 3 gave an example of how to create an emuDB from an existing TextGrid file collection and other conversion routines are covered in Section 10.1. In this chapter we will focus on creating an emuDB from scratch with nothing more than a set of .wav audio files present. 5.2.1 Creating an emuDB from scratch The R code snippet below shows how an empty emuDB is created in the directory provided by R’s tempdir() function. As can be seen by the output of the list.files() function, create_emuDB() creates a directory containing a _DBconfig.json file only. # load package library(emuR, warn.conflicts = F) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create emuDB called &quot;fromScratch&quot; create_emuDB(name = &quot;fromScratch&quot;, targetDir = tempdir(), verbose = F) # generate path to the empty fromScratch created above dbPath = file.path(tempdir(), &quot;fromScratch_emuDB&quot;) # show content of empty fromScratch emuDB list.files(dbPath) ## [1] &quot;fromScratch_DBconfig.json&quot; 5.2.2 Loading and editing an empty database The initial step in manipulating and generally interacting with a database is to load the database into the current R session. The R code below shows how to load the fromScratch database and shows the empty configuration by displaying the output of the summary() function. # load database dbHandle = load_emuDB(dbPath, verbose = F) # show summary of dbHandle summary(dbHandle) ## Name: fromScratch ## UUID: 7614837d-fe33-4212-8acc-dcea81349c30 ## Directory: /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T/RtmprLq7ig/fromScratch_emuDB ## Session count: 0 ## Bundle count: 0 ## Annotation item count: 0 ## Label count: 0 ## Link count: 0 ## ## Database configuration: ## ## SSFF track definitions: ## NULL ## ## Level definitions: ## NULL ## ## Link definitions: ## NULL # show class vector of dbHandle class(dbHandle) ## [1] &quot;emuDBhandle&quot; As can be seen in the above R code example, the class of a loaded emuDB is emuDBhandle. A emuDBhandle object is used to reference a loaded emuDB in the database interaction functions of the emuR package. In this chapter we will show how to use this emuDBhandle object to perform database manipulation operations. Most of the emuDB manipulation functions follow the following function prefix naming convention: add_XXX add a new instance of XXX / set_XXX set the current instance of XXX, list_XXX list the current instances of XXX / get_XXX get the current instance of XXX, remove_XXX remove existing instances of XXX. 5.2.3 Level definitions Unlike other systems, the EMU-SDMS requires the user to formally define the annotation structure for the entire database. An essential structural element of any emuDB are its levels. A level is a more general term for what is often referred to as a tier. It is more general in the sense that people usually expect tiers to contain time information. Levels can either contain time information if they are of the type EVENT or of the type SEGMENT but are timeless if they are of the type ITEM (see Chapter 4 for further details). It is also worth noting that an emuDB distinguishes between the definition of an annotation structure element and the actual annotations. The definition of an annotation structure element such as a level definition is merely an entry in the _DBconfig.json file which specifies that this level is allowed to be present in the _annot.json files. The levels that are present in an _annot.json file, on the other hand, have to adhere to the definitions in the _DBconfig.json. As the fromScratch database (already loaded) does not contain any annotation structural element definitions, the R code snippet below shows how a new level definition called Phonetic of type SEGMENT is added to the emuDB. # show no level definitions # are present list_levelDefinitions(dbHandle) ## NULL # add level defintion add_levelDefinition(dbHandle, name = &quot;Phonetic&quot;, type = &quot;SEGMENT&quot;) # show newly added level definition list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Phonetic SEGMENT 1 Phonetic; The example below shows how a further level definition is added that will contain the orthographic word transcriptions for the words uttered in our recordings. This level will be of the type ITEM, meaning that elements contained within the level are sequentially ordered but do not contain any time information. # add level definition add_levelDefinition(dbHandle, name = &quot;Word&quot;, type = &quot;ITEM&quot;) # list newly added level definition list_levelDefinitions(dbHandle) ## name type nrOfAttrDefs attrDefNames ## 1 Phonetic SEGMENT 1 Phonetic; ## 2 Word ITEM 1 Word; The function remove_levelDefinition() can also be used to remove unwanted level definitions. However, as we wish to further use the levels Phonetic and Word, we will not make use of this function here. 5.2.3.1 Attribute definitions Each level definition can contain multiple attributes, the most common, and currently only supported attribute being a label (of type STRING). Thus it is possible to have multiple parallel labels (i.e., attribute definitions) in a single level. This means that a single annotation item instance can contain multiple labels while sharing other properties such as the start and duration information. This can be useful when modeling certain types of data. An example of this would be the Phonetic level created above. It is often the case that databases contain both the phonetic transcript using IPA UTF-8 symbols as well as a transcript using Speech Assessment Methods Phonetic Alphabet (SAMPA) symbols. To avoid redundant time information, both of these annotations can be stored on the same Phonetic level using multiple attribute definitions (i.e., parallel labels). The next R code snippet shows the current attribute definitions of the Phonetic level. # list attribute definitions of &#39;Phonetic&#39; level list_attributeDefinitions(dbHandle, levelName = &quot;Phonetic&quot;) ## name level type hasLabelGroups hasLegalLabels ## 1 Phonetic Phonetic STRING FALSE FALSE Even though no attribute definition has been added to the Phonetic level, it already contains an attribute definition that has the same name as its level. This attribute definition represents the obligatory primary attribute of that level. As every level must contain an attribute definition that has the same name as its level, it is automatically added by the add_levelDefinition() function. To follow the above example, the next R code snippet adds a further attribute definition to the Phonetic level that contains the SAMPA versions of our annotations. # add add_attributeDefinition(dbHandle, levelName = &quot;Phonetic&quot;, name = &quot;SAMPA&quot;) ## NULL # list attribute definitions of &#39;Phonetic&#39; level list_attributeDefinitions(dbHandle, levelName = &quot;Phonetic&quot;) ## name level type hasLabelGroups hasLegalLabels ## 1 Phonetic Phonetic STRING FALSE FALSE ## 2 SAMPA Phonetic STRING FALSE FALSE 5.2.3.2 Legal labels As can be inferred from the columns hasLabelGroups and hasLegalLabels of the output of the above list_attributeDefinitions() function, attribute definitions can also contain two further optional fields. The legalLabels field contains an array of strings that specifies the labels that are legal (i.e., allowed or valid) for the given attribute definition. As the EMU-webApp does not allow the annotator to enter any labels that are not specified in this array, this is a simple way of assuring that a level has a consistent label set. The following R code snippet shows how the set_legalLabels and get_legalLabels functions can be used to specify a legal label set for the primary Word attribute definition of the Word level. # define allowed word labels wordLabels = c(&quot;amongst&quot;, &quot;any&quot;, &quot;are&quot;, &quot;always&quot;, &quot;and&quot;, &quot;attracts&quot;) # show empty legal labels # for &quot;Word&quot; attribute definition get_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;) ## [1] NA # set legal labels values # for &quot;Word&quot; attribute definition set_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;, legalLabels = wordLabels) # show recently added legal labels # for &quot;Word&quot; attribute definition get_legalLabels(dbHandle, levelName = &quot;Word&quot;, attributeDefinitionName = &quot;Word&quot;) ## [1] &quot;amongst&quot; &quot;any&quot; &quot;are&quot; &quot;always&quot; &quot;and&quot; &quot;attracts&quot; 5.2.3.3 Label groups A further optional field is the labelGroups field. It contains specifications of groups of labels that can be referenced by a name given to the group while querying the emuDB. The R code below shows how the add_attrDefLabelGroup() function is used to add two label groups to the Phonetic attribute definition. One of the groups is used to reference a subset of longVowels and the other to reference a subset of shortVowels on the Phonetic level. # add long vowels label group add_attrDefLabelGroup(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;, labelGroupName = &quot;longVowels&quot;, labelGroupValues = c(&quot;i:&quot;, &quot;u:&quot;)) # add short vowels label group add_attrDefLabelGroup(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;, labelGroupName = &quot;shortVowels&quot;, labelGroupValues = c(&quot;i&quot;, &quot;u&quot;, &quot;@&quot;)) # list current label groups list_attrDefLabelGroups(dbHandle, levelName = &quot;Phonetic&quot;, attributeDefinitionName = &quot;Phonetic&quot;) ## name values ## 1 longVowels i:; u: ## 2 shortVowels i; u; @ # query all short vowels # Note the result of this query # is empty as no annotations are present # in the &#39;fromScratch&#39; emuDB query(dbHandle, &quot;Phonetic == shortVowels&quot;) ## segment list from database: fromScratch ## query was: Phonetic == shortVowels ## [1] labels start end session bundle level type ## &lt;0 rows&gt; (or 0-length row.names) For users who are familiar with or transitioning from the legacy EMU system, it is worth noting that the label groups correspond to the unfavorably named Legal Labels entries of the GTemplate Editor (i.e., legal entries in the .tpl file) of the legacy system. In the new system the legalLabels entries specify the legal or allowed label values of attribute definitions while the labelGroups specify groups of labels that can be referenced by the names given to the groups while performing queries. A new feature of the EMU-SDMS is the possibility of defining label groups for the entire emuDB as opposed to a single attribute definition (see ?add_labelGroups for further details). This avoids the redundant definition of label groups that should span multiple attribute definitions (e.g., a longVowels subset that is to be queried on a level called Phonetic_1 as well as a level called Phonetic_2). 5.2.4 Link definitions An essential and very powerful conceptual and structural element of any emuDB is its hierarchy. Using hierarchical structures is highly recommended but not a must. Hierarchical annotations allow for complex, rich data modeling and are often cleaner representations of the annotations at hand. As Chapter 4 contains in-depth explanations of the annotation modeling capabilities of the EMU-SDMS and Chapter 6 shows how these structures can be queried using emuR’s query mechanics, this chapter will omit an explanation of hierarchical annotation structures. The following R code shows how a ONE_TO_MANY relationship between the Word and Phonetic in the form of a link definition is added to an emuDB. # show that currently no link definitions # are present list_linkDefinitions(dbHandle) ## NULL # add new &quot;ONE_TO_MANY&quot; link definition # between &quot;Word&quot; and &quot;Phonetic&quot; levels add_linkDefinition(dbHandle, type = &quot;ONE_TO_MANY&quot;, superlevelName = &quot;Word&quot;, sublevelName = &quot;Phonetic&quot;) # show newly added link definition list_linkDefinitions(dbHandle) ## type superlevelName sublevelName ## 1 ONE_TO_MANY Word Phonetic A schematic of the simple hierarchical structure of the fromScratch created by the above R code is displayed in Figure 5.2. Figure 5.2: A schematic representation of the simple hierarchical structure of the fromScratch created by the add_linkDefinition() function call in above R code snippet. 5.2.5 File handling The previous sections of this chapter defined the simple structure of the fromScratch emuDB. An essential element that is still missing from the emuDB is the actual audio speech data12. The following R code example shows how the import_mediaFiles() function can be used to import audio files, referred to as media files in the context of an emuDB, into the fromScratch emuDB. # get the path to directory containing .wav files wavDir = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;txt_collection&quot;) # Import media files into emuDB session called fromWavFiles. # Note that the txt_collection directory also contains .txt files. # These are simply ignored by the import_mediaFiles() function. import_mediaFiles(dbHandle, dir = wavDir, targetSessionName = &quot;fromWavFiles&quot;, verbose = F) # list session list_sessions(dbHandle) ## name ## 1 fromWavFiles # list bundles list_bundles(dbHandle) ## session name ## 1 fromWavFiles msajc003 ## 2 fromWavFiles msajc010 ## 3 fromWavFiles msajc012 ## 4 fromWavFiles msajc015 ## 5 fromWavFiles msajc022 ## 6 fromWavFiles msajc023 ## 7 fromWavFiles msajc057 # show first two files in the emuDB library(tibble) # convert to tibble only to prettify output as_tibble(head(list_files(dbHandle), n = 2)) ## # A tibble: 2 x 4 ## session bundle file absolute_file_path ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fromWavFi… msajc0… msajc003_a… /private/var/folders/yk/8z9tn7kx6hbcg_9n4… ## 2 fromWavFi… msajc0… msajc003.w… /private/var/folders/yk/8z9tn7kx6hbcg_9n4… The import_mediaFiles() call above added a new session called fromWavFiles to the fromScratch emuDB containing a new bundle for each of the imported media files. The annotations of every bundle, despite containing empty levels, adhere to the structure specified above. This means that every _annot.json file created contains an empty Word and Phonetic level array and the links array is also empty. The emuR package also provides a mechanism for adding files to preexisting bundle directories, as this can be quite tedious to perform manually due to the nested directory structure of an emuDB. The following R code shows how preexisting .zcr files that are produced by wrassp’s zcrana() function can be added to the preexisting session and bundle structure. As the directory referenced by wavDir does not contain any .zcr files, the next R code example first creates them and then adds them to the emuDB (see Chapter 8 for further details). # load wrassp package library(wrassp) # list all wav files in wavDir wavFilePaths = list.files(wavDir, pattern = &quot;.*.wav&quot;, full.names = TRUE) # calculate zero-crossing-rate files # using zcrana function of wrassp package zcrana(listOfFiles = wavFilePaths, verbose = FALSE) ## [1] 7 # add zcr files to emuDB add_files(dbHandle, dir = wavDir, fileExtension = &quot;zcr&quot;, targetSessionName = &quot;fromWavFiles&quot;) # show first three files in emuDB (convert to tibble only # to prettify output) as_tibble(head(list_files(dbHandle), n = 3)) ## # A tibble: 3 x 4 ## session bundle file absolute_file_path ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fromWavFi… msajc0… msajc003_a… /private/var/folders/yk/8z9tn7kx6hbcg_9n4… ## 2 fromWavFi… msajc0… msajc003.w… /private/var/folders/yk/8z9tn7kx6hbcg_9n4… ## 3 fromWavFi… msajc0… msajc003.z… /private/var/folders/yk/8z9tn7kx6hbcg_9n4… 5.2.6 SSFF track definitions A further important structural element of any emuDB is use of the so-called SSFF tracks, which are often simply referred to as tracks. These SSFF tracks reference data that is stored in the SSFF (see Appendix 15.1.3 for a detailed description of the file format) within the _bndl directories. The two main types of data are: complementary data that was acquired during the recording such as by EMA or EPG; or derived data, that is data that was calculated from the original audio signal such as formant values and their bandwidths or the short-term Root Mean Square amplitude of the signal. As Section 8.7 covers how the SSFF file output of a wrassp function can be added to an emuDB, an explanation will be omitted here. The following R code snippet shows how the .zcr files added in the R example above can be added as an SSFF track definition (see Chapter 8 for further details). # show that no SSFF track definitions # are present list_ssffTrackDefinitions(dbHandle) ## NULL # add SSFF track definition to emuDB add_ssffTrackDefinition(dbHandle, name = &quot;zeroCrossing&quot;, columnName = &quot;zcr&quot;, fileExtension = &quot;zcr&quot;) # show newly added SSFF track definition list_ssffTrackDefinitions(dbHandle) ## name columnName fileExtension ## 1 zeroCrossing zcr zcr 5.2.7 Configuring the EMU-webApp and annotating the emuDB As previously mentioned, the current fromScratch emuDB contains only empty levels. In order to start annotating the database, the EMU-webApp has to be configured to display the desired information. Although the configuration of the EMU-webApp is stored in the _DBconfig.json file and is therefore a part of the emuDB format, here we will omit an explanation of the extensive possibilities of configuring the web application (see Chapter 9 for an in-depth explanation). The R code snippet below shows how the Phonetic level is added to the level canvases order array of the default perspective. # show empty level canvases order get_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;) ## NULL # set level canvases order to display &quot;Phonetic&quot; level set_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;, order = c(&quot;Phonetic&quot;)) # show newly added level canvases order get_levelCanvasesOrder(dbHandle, perspectiveName = &quot;default&quot;) ## [1] &quot;Phonetic&quot; As a final step before beginning the annotation process, the fromScratch emuDB has to be served to the EMU-webApp for annotation and visualization purposes. The code below shows how this can be achieved using the serve() function. # serve &quot;fromScratch&quot; emuDB to the EMU-webApp serve(dbHandle) 5.3 Conclusion This chapter introduced the elements that comprise the new emuDB format and provided a practical overview of the essential database interaction functions provided by the emuR package. We feel the emuDB format provides a general purpose, flexible approach to storing speech databases with the added benefit of being able to directly manipulate and analyse these databases using the tools provided by the EMU-SDMS. References "],
["chap-querysys.html", "6 The query system 6.1 emuRsegs: The resulting object of a query 6.2 EQL: The EMU Query Language version 2 6.3 Discussion", " 6 The query system This chapter describes the newly implemented query system of the emuR package. When developing the new emuR package it was essential that it had a query mechanism allowing users to query a database’s annotations in a simple manner. The EMU query language (EQL) of the EMU-SDMS arose out of years of developing and improving upon the query language of the legacy system (e.g., Cassidy and Harrington (2001), Harrington (2010), John (2012)). As a result, today we have an expressive, powerful, yet simple to learn and domain-specific query language. The EQL defines a user interface by allowing the user to formulate a formal language expression in the form of a query string. The evaluation of a query string results in a set of annotation items or, alternatively, a sequence of items of a single annotation level in the emuDB from which time information, if applicable (see Section ??), has been deduced from the time-bearing sub-level. An example of this is a simple query that extracts all strong syllables (i.e., syllable annotation items containing the label S on the Syllable level) from a set of hierarchical annotations (see Figure 6.1 for an example of a hierarchical annotation). The respective EQL query string \"Syllable == S\" results in a set of segments containing the annotation label S. Due to the temporal inclusion constraint of the domination relationship, the start and end times of the queried segments are derived from the respective items of the Phonetic level (i.e., the m and H nodes in Figure 6.1, as this is the time-bearing sub-level. The EQL described here allows users to query the complex hierarchical annotation structures in their entirety as they are described in Chapter 4. Figure 6.1: Simple partial hierarchy of an annotation of the word amongst in the msajc003 bundle in the ae demo emuDB. The R code snippet below shows how to create the demo data that is provided by the emuR package followed by loading an example emuDB called ae into the current R session. This database will be used in all the examples throughout this chapter. # load package library(emuR) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) 6.1 emuRsegs: The resulting object of a query In emuR the result of a query or requery (see Section 6.2.7) is a pre-specified object which is a superclass of the common data.frame. R code snippet below shows the result of a slightly expanded version of the above query (\"Syllable == S\"), which additionally uses the dominates operator (i.e., the ^ operator; for further information see Section @ref(subsubsec:query_dominationQueries)) to reduce the queried annotations to the partial hierarchy depicted in Figure @ref(fig:amongstHier} in the ae demo emuDB. In this example, the classes of the resulting object including its printed output are displayed. The class vector of a resulting emuRsegs object also contains the legacy EMU system’s emusegs class, which indicates that this object is fully backwards compatible with the legacy class and the methods available for it (see Harrington (2010) for details). The printed output provides information about which database was queried and what the query was as well as information about the labels, start and end times (in milliseconds), session, bundle, level and type information. The call to colnames() shows that the resulting object has additional columns, which are ignored by the print() function. This somewhat hidden information is used to store information about what the exact items or sequence of items were retrieved from the emuDB. This information is needed to know which items to start from in a requery (see Section @ref(subsec:requery}) and is also the reason why an emuRsegs object should be viewed as a reference of sequences of annotation items that belong to a single level in all annotation files of an emuDB. # query database sl = query(ae, &quot;[Syllable == S ^ Text == amongst]&quot;) # show class vector class(sl) ## [1] &quot;emuRsegs&quot; &quot;emusegs&quot; &quot;data.frame&quot; # show sl object sl ## segment list from database: ae ## query was: [Syllable == S ^ Text == amongst] ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM # show all (incl. hidden) column names colnames(sl) ## [1] &quot;labels&quot; &quot;start&quot; &quot;end&quot; ## [4] &quot;utts&quot; &quot;db_uuid&quot; &quot;session&quot; ## [7] &quot;bundle&quot; &quot;start_item_id&quot; &quot;end_item_id&quot; ## [10] &quot;level&quot; &quot;start_item_seq_idx&quot; &quot;end_item_seq_idx&quot; ## [13] &quot;type&quot; &quot;sample_start&quot; &quot;sample_end&quot; ## [16] &quot;sample_rate&quot; 6.2 EQL: The EMU Query Language version 2 The EQL user interface was retained from the legacy system because it was sufficiently flexible and expressive enough to meet the query needs in most types of speech science research. The EQL parser implemented in emuR is based on the Extended Backus–Naur form (EBNF) (Garshol 2003) formal language definition of John (2012), which defines the symbols and the relationship of those symbols to each other on which this language is built (see adapted version of entire EBNF in Appendix 17). Here we will describe the various terms and components that comprise the slightly adapted version 2 of the EQL. It is worth noting that the new query mechanism uses a relational back-end to handle the various query operations (see Chapter 11 for details). This means that expert users, who are proficient in Structured Query Language (SQL) may also query this relational back-end directly. However, we feel the EQL provides a simple abstraction layer which is sufficient for most speech and spoken language research. 6.2.1 Simple queries The most basic form of an EQL query is a simple equality, inequality, matching or non-matching query, two of which are displayed in R code snippet below. The syntax of a simple query term is [L OPERATOR A], where L specifies a level (or alternatively the name of a parallel attribute definition); OPERATOR is one of == (equality), !$=$ (inequality), =~ (matching) or !~ (non-matching); and A is an expression specifying the labels of the annotation items of L.13 The second query in the R code snippet below queries an event level. The result of querying an event level contains the same information as that of a segment level query except that the derived end times have the value zero. # query all annotation items containing # the label &quot;m&quot; on the &quot;Phonetic&quot; level sl = query(ae, &quot;Phonetic == m&quot;) # query all items NOT containing the # label &quot;H*&quot; on the &quot;Tone&quot; level sl = query(ae, &quot;Tone != H*&quot;) # show first entry of sl head(sl, n = 1) ## event list from database: ae ## query was: Tone != H* ## labels start end session bundle level type ## 1 L- 1107 0 0000 msajc003 Tone EVENT The R code snippet above queries two levels that contain time information: a segment level and an event level. As described in Chapter 4, annotations in the EMU-SDMS may also contain levels that do not contain time information. The R code snippet below shows a query that queries annotation items on a level that does not contain time information (the Syllable level) to show that the result contains deduced time information from the time-bearing sub-level. # query all annotation items containing # the label S on the Syllable level sl = query(ae, &quot;Syllable == S&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: Syllable == S ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM 6.2.1.1 Queries using regular expressions The slightly expanded version 2 of the EQL, which comes with the emuR package, introduces regular expression operators (=~ and !~). These allow users to formulate regular expressions for more expressive and precise pattern matching of annotations. A minimal set of examples displaying the new regular expression operators is shown in Table ??. ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Table 6.1: EQL V2: examples of simple and complex query strings using RegEx operators including their function descriptions. Query Function Phonetic =~ '[AIOUEV]' A disjunction of annotations using a RegEx character class Word =~ a.* All words beginning with a Word !~ .*st All words not ending in st [Phonetic == n ^ #Syllable =~ .*] All syllables that dominate an n segment of the Phonetic level 6.2.2 Combining simple queries The EQL contains three operators that can be used to combine the simple query terms described above as well as position queries which we will describe below. These three operators are the sequence operator, -&gt;; the conjunction operator, &amp;; and the domination operator, ^, which is used to perform hierarchical queries. These three types of queries are described below. To start with, we describe the two types of queries that query more complex annotation structures on the same level (sequence and conjunction queries). This is followed by a description of domination queries that query hierarchically linked annotation structures, sometimes spanning multiple annotation levels. 6.2.2.1 Sequence queries The syntax of a query string using the -&gt; sequence operator is [L == A -&gt; L == B] where annotation item A on level L precedes item B on level L. For a sequence query to work, both arguments must be on the same level. Alternatively parallel attribute definitions of the same level may also be chosen (see Chapter 4 for further details). An example of a query string using the sequence operator is displayed in the R code snippet below. All rows in the resulting segment list have the start time of @, the end time of n and their labels are @-&gt;n, where the -&gt; substring denotes the sequence. # query all sequences of items on the &quot;Phonetic&quot; level # in which an item containing the label &quot;@&quot; is followed by # an item containing the label &quot;n&quot; sl = query(ae, &quot;[Phonetic == @ -&gt; Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phonetic == @ -&gt; Phonetic == n] ## labels start end session bundle level type ## 1 @-&gt;n 1715.425 1791.425 0000 msajc003 Phonetic SEGMENT 6.2.2.2 Result modifier Because users are often interested in just one element of a compound query such as sequence queries (e.g., the @s in a @-&gt;n sequences), the EQL offers a so-called result modifier symbol, #. This symbol may be placed in front of any simple query component of a multi component query as depicted in the R code snippet below. Placing the hashtag in front of either the left or the right simple query term will result in segment lists that contains only the annotation items of the simple query term that have the hashtag in front of it. Only one result modifier may be used per query. # query the &quot;@&quot;s in &quot;@-&gt;n&quot; sequences sl = query(ae, &quot;[#Phonetic == @ -&gt; Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [#Phonetic == @ -&gt; Phonetic == n] ## labels start end session bundle level type ## 1 @ 1715.425 1741.425 0000 msajc003 Phonetic SEGMENT # query the &quot;n&quot;s in a &quot;@-&gt;n&quot; sequences sl = query(ae, &quot;[Phonetic == @ -&gt; #Phonetic == n]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phonetic == @ -&gt; #Phonetic == n] ## labels start end session bundle level type ## 1 n 1741.425 1791.425 0000 msajc003 Phonetic SEGMENT 6.2.2.3 Conjunction queries The syntax of a query string using the conjunction operator can schematically be written as: [L_a1 == A &amp; L_a2 == B &amp; L_a3 == C &amp; L_a4 == D &amp; ... &amp; L_an == N], where annotation items on level L have the label A and also have the parallel labels B, C, D, …, N (see Chapter 4 for more information about parallel labels). By analogy with the sequence operator, all simple query statements must refer to the same level (i.e., only parallel attributes definitions of the same level indicated by the a1 - an may to be chosen). Hence, the conjunction operator is used to combine query conditions on the same level. Using the conjunction operator is useful for two reasons: It combines different attributes of the same level: [Text == always &amp; Accent == S] where Text and Accent are additional attributes of level Word; and It combines a simple query with a function query (see Position Queries Section ??): [Phonetic == l &amp; Start(Word, Phonetic) == 1]. An example of a query string using the conjunction operator is displayed in the R code snippet below. # query all words with the orthographic transcription &quot;always&quot; # that also have a strong word accent (&quot;S&quot;) query(ae, &quot;[Text == always &amp; Accent == S]&quot;) ## segment list from database: ae ## query was: [Text == always &amp; Accent == S] ## labels start end session bundle level type ## 1 always 775.475 1280.175 0000 msajc022 Text ITEM The above R code snippet does not make use of the result modifier symbol. However, only the annotation items of the left simple query term (Text == always) are returned. This behavior is true for all EQL operators that combine simple query terms except for the sequence operator. As it is more explicit to use the result modifier to express the desired result, we recommend using the result modifier where possible. The more explicit variant of the above query which yields the same result is “[#Text == always &amp; Word == C]”. 6.2.2.4 Domination/hierarchical queries Compared to sequence and conjunction queries, a domination query using the operator ^ is not bound to a single level. Instead, it allows users to query annotation items that are directly or indirectly linked over one or more levels. Queries using the domination operator are often referred to as hierarchical queries as they provide the ability to query the hierarchical annotations in a vertical or inter-level manner. Figure 6.2 shows the same partial hierarchy as Figure 6.1 but highlights the annotational items that are dominated by the strong syllable (S) of the Syllable level. Such linked hierarchical sub-structures can be queried using hierarchical/domination queries. Figure 6.2: Partial hierarchy depicting all annotation items that are dominated by the strong syllable (S) of the Syllable level (inside dashed box). Items marked extcolor{three_color_c1}{green} belong to the extcolor{three_color_c1}{Phoneme} level, items marked extcolor{three_color_c2}{orange} belong to the extcolor{three_color_c2}{Phonetic} level and the extcolor{three_color_c3}{purple} dashed box indicates the set of items that are dominated by S. A schematic representation of a simple domination query string that retrieves all annotation items A of level L1 that are dominated by items B in level L2 (i.e., items that are directly or indirectly linked) is [L1 == A ^{ L2 == B]}. Although the domination relationship is directed the domination operator is not. This means that either items in L1 dominate items in L2 or items in L2 dominate items in L1. Note that link definitions that specify the validity of the domination have to be present in the emuDB configuration for this to work (see Chapter 5 for details). An example of a query string using the domination operator is displayed in the R code snippet below. # query all &quot;p&quot; phoneme items that belong # to / are dominated by a strong syllable (&quot;S&quot;) sl = query(ae, &quot;[Phoneme == p ^ Syllable == S]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Phoneme == p ^ Syllable == S] ## labels start end session bundle level type ## 1 p 558.575 639.575 0000 msajc015 Phoneme ITEM As with the conjunction query, if no result modifier is present, a dominates query returns the annotation items of the left simple query term. Hence, the more explicit variant of the above query is \"[#Phoneme == p ^ Syllable == S]\". 6.2.3 Position queries The EQL has three function terms that specify where in a domination relationship a child level annotation item is allowed to occur. The three function terms are Start(), End() and Medial(). A schematic representation of a query string representing a simple usage of the Start(), End() and Medial() function would be: POSFCT(L1, L2) == TRUE. In this representation POSFCT is a placeholder for one of the three functions, at which level L1 must dominate level L2. Where L1 does indeed dominate L2, the corresponding item from level L2 is returned. If the expression is set to FALSE (i.e., POSFCT(L1, L2) == FALSE), all the items that do not match the condition of L2 are returned. An illustration of what is returned by each of the position functions depending on if they are set to TRUE or FALSE is depicted in Figure 6.3, while the R code snippet below shows an example query using a position query term. Figure 6.3: Illustration of what is returned by the Start(), Medial() and End() functions depending if they are set to extbf{A:} extcolor{three_color_c1}{TRUE} (green) or extbf{B:} extcolor{three_color_c2}{FALSE} (orange). # query all phoneme items that occur # at the start of a syllable sl = query(ae, &quot;[Start(Syllable, Phoneme) == TRUE]&quot;) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: [Start(Syllable, Phoneme) == TRUE] ## labels start end session bundle level type ## 1 V 187.425 256.925 0000 msajc003 Phoneme ITEM 6.2.4 Count queries A further query component of the EQL are so-called count queries. They allow the user to specify how many child nodes a parent annotation item is allowed to have. Figure @ref(fig:query_amongstHierCount) displays two syllables, one containing one phoneme and one phonetic annotation item, the other containing five phoneme and six phonetic items. Using EQL’s Num() function it is possible to specify which of the two syllables should be retrieved, depending on the number of phonemic or phonetic elements to which it is directly or indirectly linked. The R code snippet below shows a query that queries all syllables that contain five phonemes. Figure 6.4: Partial hierarchy depicting a Syllable containing one Phoneme and Phonetic item (green) and a Syllable containing five Phoneme and six Phonetic items (orange). A schematic representation of a query string utilizing the count mechanism would be [Num(L1, L2) == N], where L1 contains N annotation items in L2. For this type of query to work L1 has to dominate L2 (i.e., be a parent level to L2). As the query matches a number (N), it is also possible to use the operators &gt; (more than), &lt; (less than) and != (not equal to). The resulting segment list contains items of L1. # retrieve all syllables that contain five phonemes query(ae, &quot;[Num(Syllable, Phoneme) == 5]&quot;) ## segment list from database: ae ## query was: [Num(Syllable, Phoneme) == 5] ## labels start end session bundle level type ## 1 S 256.925 674.175 0000 msajc003 Syllable ITEM ## 2 S 739.925 1289.425 0000 msajc003 Syllable ITEM ## 3 W 2228.475 2753.975 0000 msajc010 Syllable ITEM ## 4 S 1890.275 2469.525 0000 msajc022 Syllable ITEM ## 5 S 1964.425 2554.175 0000 msajc023 Syllable ITEM 6.2.5 More complex queries By using the correct bracketing, all of the above query components can be combined to formulate more complex queries that can be used to answer questions such as: Which occurrences of the word “his” follow three-syllable words which contain a schwa (@) in the first syllable? Such multi-part questions can usually be broken down into several sub-queries. These sub-queries can then be recombined to formulate the complex query. The steps to answering the above multi-part question are: Which occurrences of the word “his” …: [Text == his] … three-syllable words …: [Num(Text, Syllable) == 3] … contain a schwa (@) in the first syllable …: [Phoneme == @ ^ Start(Word, Syllable) == 1] All three can be combined by saying 2 dominates 3 ([2 ^ 3]) and these are followed by 1 ([2 ^ 3] -&gt; 1]) The combine query is depicted in the R code snippet below. This complex query demonstrates the expressive power of the query mechanism that the EMU-SDMS provides. # perform complex query # Note that the use of paste0() is optional, as # it is only used for formatting purposes query(ae, paste0(&quot;[[[Num(Text, Syllable) == 3] &quot;, &quot;^ [Phoneme == @ ^ Start(Word, Syllable) == 1]] &quot;, &quot;-&gt; #Text = his]&quot;)) ## segment list from database: ae ## query was: [[[Num(Text, Syllable) == 3] ^ [Phoneme == @ ^ Start(Word, Syllable) == 1]] -&gt; #Text = his] ## labels start end session bundle level type ## 1 his 2693.675 2780.725 0000 msajc015 Text ITEM As mastering these complex compound queries can require some practice, several simple as well as more complex examples that combine the various EQL components described above are available in Appendix 18. These examples provide practical examples to help users find queries suited to their needs. 6.2.6 Deducing time The default behavior of the legacy EMU system was to automatically deduce time information for queries of levels that do not contain time information. This was achieved by searching for the time-bearing sub-level and calculating the start and end times from the left-most and right-most annotation items which where directly or indirectly linked to the retrieved parent item. This upward purculation of time information is also the default behavior of the new EMU-SDMS. However, a new feature has been added to the query engine which allows the calculation of time to be switched off for a given query using the calcTimes parameter of the query() function. This is beneficial in two ways: for one, levels that do not have a time-bearing sub-level may be queried and secondly, the execution time of queries can be greatly improved. The performance increase becomes evident when performing queries on large data sets on one of the top levels of the hierarchy (e.g., Utterance or Intonational in the ae emuDB). When deducing time information for annotation items that contain large portions of the hierarchy, the query engine has to walk down large partial hierarchies to find the left-most and right-most items on the time-bearing sub-level. This can be a computationally expensive operation and is often unnecessary, especially during data exploration. The R code snippet below shows the usage of this parameter by querying all of the items of the Intonational level and displaying the NA values for start and end times in the resulting segment list. It is worth noting that the missing time information excluded during the original query can be retrieved at a later point in time by performing a hierarchical requery (see Section 6.2.7) on the same level. # query all intonational items sl = query(ae, &quot;Intonational =~ .*&quot;, calcTimes = F) # show first entry of sl head(sl, n = 1) ## segment list from database: ae ## query was: Intonational =~ .* ## labels start end session bundle level type ## 1 L% NA NA 0000 msajc003 Intonational ITEM 6.2.7 Requery A popular feature of the legacy system was the ability to use the result of a query to perform an additional query, called a requery, starting from the resulting items of a query. The requery functionality was used to move either sequentially (horizontally) or hierarchically (vertically) through the hierarchical annotation structure. Although this feature technically does not extend the querying functionality (it is possible to formulate EQL queries that yield the same results as a query followed by \\(1:n\\) requeries), requeries benefit the user by breaking down the task of formulating long query terms into multiple, simpler queries. Compared with the legacy system, this feature is implemented in the emuR package in a more robust way, as unique item IDs are present in the result of a query, eliminating the need for searching the starting segments based on their time information. Examples of queries and their results within a hierarchical annotation based on a hierarchical and sequential requery as well as their EQL equivalents are illustrated in Figure 6.5. Figure 6.5: Three-step ( extcolor{three_color_c1}{query} -&gt; extcolor{three_color_c2}{requery_hier} -&gt; extcolor{three_color_c3}{requery_seq}) requery procedure, its single extcolor{darkgray}{query} counterpart and their color coded movements within the annotation hierarchy. The R code snippet below illustrates how the same results of the sequential query [\\#Phonetic =~ .* -&gt; Phonetic == n] can be achieved using the requery_seq() function. Further, it shows how the requery_hier() function can be used to move vertically through the annotation structure by starting at the Syllable level and retrieving all the Phonetic items for the query result. ######################## # requery_seq() # query all &quot;n&quot; phonetic items sl_n = query(ae, &quot;Phonetic == n&quot;) # sequential requery (left shift result by 1 (== offset of -1)) # and hence retrieve all phonetic items directly preceeding # all &quot;n&quot; phonetic items sl_precn = requery_seq(ae, seglist = sl_n, offset = -1) # show first entry of sl_precn head(sl_precn, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 E 949.925 1031.925 0000 msajc003 Phonetic SEGMENT ######################## # requery_hier() # query all strong syllables (S) sl_s = query(ae, &quot;Syllable == S&quot;) # hierarchical requery sl_phonetic = requery_hier(ae, seglist = sl_s, level = &quot;Phonetic&quot;) # show first entry of sl_phonetic head(sl_phonetic, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 m-&gt;V-&gt;N-&gt;s-&gt;t-&gt;H 256.925 674.175 0000 msajc003 Phonetic SEGMENT 6.3 Discussion This chapter gave an overview of the abilities of the query system of the EMU-SDMS. We feel the EQL is an expressive, powerful, yet simple to learn and domain-specific query language that allows users to adequately query complex annotation structures. Further, the query system provided by the EMU-SDMS surpasses the querying capabilities of most commonly used systems. As the result of a query is a superclass of the common data.frame object, these results can easily be further processed using various R functions (e.g., to remove unwanted segments). Further, the results of queries can be used as input to the get_trackdata() function (see Chapter 7) which makes the query system a vital part in the default workflow described in Chapter 2. Although the query mechanism of the EMU-SDMS covers most linguistic annotation query needs (including co-occurrence and domination relationship child position queries), it has limitations due to its domain-specific nature, its simplicity and its predefined result type. Performing more general queries such as: What is the average age of the male speakers in the database who are taller than 1.8 meters? is not directly possible using the EQL. Even if the gender, height and age parameters are available as part of the database’s annotations (e.g., using the single bundle root node metadata strategy described in Chapter 4) they would be encoded as strings, which do not permit direct calculations or numerical comparisons. However, it is possible to answer these types of questions using a multi-step approach. One could, for example, extract all height items and convert the strings into numbers to filter the items containing a label that is greater than 1.8. These filtered items could then be used to perform two requeries to extract all male speakers and their age labels. These age labels could once again be converted into numbers to calculate their average. Although not as elegant as other languages, we have found that most questions that arise as part of studies working with spoken language database can be answered using such a multi-step process including some data manipulation in R, provided the necessary information is encoded in the database. Additionally, from the viewpoint of a speech scientist, we feel that the intuitiveness of an EQL expression (e.g., a query to extract the sibilant items for the question asked in the introduction: \"Phonetic == s|z|S|Z\") exceeds that of a comparable general purpose query language (e.g. a semantically similar SQL statement: SELECT desired_columns FROM items AS i, labels AS l WHERE i.unique_bundle_item_id = l.uniq_bundle_item_id AND l.label = 's' OR l.label = 'z' OR l.label = 's' OR l.label = 'S' OR l.label = 'Z'). This difference becomes even more apparent with more complex EQL statements, which can have very long, complicated and sometimes multi-expression SQL counterparts. A problem which the EMU-SDMS does not explicitly address is the problem of cross-corpus searches. Different emuDBs may have varying annotation structures with varying semantics regarding the names or labels given to objects or annotation items in the databases. This means that it is very likely that a complex query formulated for a certain emuDB will fail when used to query other databases. If, however, the user either finds a query that works on every emuDB or adapts the query to extract the items she/he is interested in, a cross-corpus comparison is simple. As the result of a query and the corresponding data extraction routines are the same, regardless of database they where extracted from, these results are easily comparable. However, it is worth noting that the EMU-SDMS is completely indifferent to the semantics of labels and level names, which means it is the user’s responsibility to check if a comparison between databases is justifiable (e.g., are all segments containing the label “@” of the level “Phonetic”\" in all emuDBs annotating the same type of phoneme?). References "],
["chap-sigDataExtr.html", "7 Signal data extraction14 7.1 Extracting pre-defined tracks 7.2 Adding new tracks 7.3 Calculating tracks on-the-fly 7.4 The resulting object: trackdata vs. emuRtrackdata 7.5 Conclusion", " 7 Signal data extraction14 As mentioned in the default workflow of Chapter @ref(chap:overview}, after querying the symbolic annotation structure and dereferencing its time information, the result is a set of items with associated time stamps. It was necessary that the emuR package contain a mechanism for extracting signal data corresponding to this set of items. As illustrated in Chapter @ref(chap:wrassp}, wrassp provides the R ecosystem with signal data file handling capabilities as well as numerous signal processing routines. emuR can use this functionality to either obtain pre-stored signal data or calculate derived signal data that correspond to the result of a query. Figure @ref(fig:sigDataExtr}A shows a snippet of speech with overlaid annotations where the resulting SEGMENT of an example query (e.g., \"Phonetic == ai\") is highlighted in yellow. Figure 7.1B displays a time parallel derived signal data contour as would be returned by one of wrassp’s file handling or signal processing routines. The yellow segment in Figure 7.1B marks the corresponding samples that belong to the ai segment of Figure 7.1A. Figure 7.1: Segment of speech with overlaid annotations and time parallel derived signal data contour. The R code snippet below shows how to create the demo data that will be used throughout this chapter. # load the package library(emuR) # create demo data in directory provided by the tempdir() function create_emuRdemoData(dir = tempdir()) # get the path to a emuDB called &quot;ae&quot; that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load emuDB into current R session ae = load_emuDB(path2directory) 7.1 Extracting pre-defined tracks To access data that are stored in files, the user has to define tracks for a database that point to sequences of samples in files that match a user-specified file extension. The user-defined name of such a track can then be used to reference the track in the signal data extraction process. Internally, emuR uses wrassp to read the appropriate files from disk, extract the sample sequences that match the result of a query and return values to the user for further inspection and evaluation. The R code snippet below shows how a signal track that is already defined in the ae demo database can be extracted for all annotation items on the Phonetic level containing the label ai. # list currently available tracks list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms # query all &quot;ai&quot; phonetic segments ai_segs = query(ae, &quot;Phonetic == ai&quot;) # get &quot;fm&quot; track data for these segments # Note that verbose is set to FALSE # only to avoid a progress bar # being printed in this document. ai_td_fm = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;fm&quot;, verbose = FALSE) # show summary of ai_td_fm summary(ai_td_fm) ## Emu track data from 6 segments ## ## Data is 4 dimensional from track fm ## Mean data length is 30.5 samples Being able to access data that is stored in files is important for two main reasons. Firstly, it is possible to generate files using external programs such as VoiceSauce (Shue et al. 2011), which can export its calculated output to the general purpose SSFF file format. This file mechanism is also used to access data produced by EMA, EPG or many other forms of signal data recordings. Secondly, it is possible to track, save and access manipulated data such as formant values that have been manually corrected. It is also worth noting that the get_trackdata() function has a predefined track which is always available without it having to be defined. The name of this track is MEDIAFILE_SAMPLES which references the actual samples of the audio files of the database. The R code snippet below shows how this predefined track can be used to access the audio samples belonging to the segments in ai_segs. # get media file samples ai_td_mfs = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;MEDIAFILE_SAMPLES&quot;, verbose = FALSE) # show summary of ai_td_fm summary(ai_td_mfs) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track MEDIAFILE_SAMPLES ## Mean data length is 3064.333 samples 7.2 Adding new tracks As described in detail in Section 8.7, the signal processing routines provided by the wrassp package can be used to produce SSFF files containing various derived signal data (e.g., formants, fundamental frequency, etc.). The R code snippet below shows how the add_ssffTrackDefinition() can be used to add a new track to the ae emuDB. Using the onTheFlyFunctionName parameter, the add_ssffTrackDefinition() function automatically executes the wrassp signal processing function ksvF0 (onTheFlyFunctionName = \"ksvF0\") and stores the results in SSFF files in the bundle directories. # add new track and calculate # .f0 files on-the-fly using wrassp::ksvF0() add_ssffTrackDefinition(ae, name = &quot;F0&quot;, onTheFlyFunctionName = &quot;ksvF0&quot;, verbose = FALSE) # show newly added track list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms ## 3 F0 F0 f0 # show newly added files library(tibble) # convert to tibble only to prettify output as_tibble(list_files(ae, fileExtension = &quot;f0&quot;)) ## # A tibble: 7 x 4 ## session bundle file absolute_file_path ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0000 msajc003 msajc00… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 2 0000 msajc010 msajc01… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 3 0000 msajc012 msajc01… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 4 0000 msajc015 msajc01… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 5 0000 msajc022 msajc02… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 6 0000 msajc023 msajc02… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… ## 7 0000 msajc057 msajc05… /private/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld… # extract newly added trackdata ai_td = get_trackdata(ae, seglist = ai_segs, ssffTrackName = &quot;F0&quot;, verbose = FALSE) # show summary of ai_td summary(ai_td) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track F0 ## Mean data length is 30.5 samples 7.3 Calculating tracks on-the-fly With the wrassp package, we were able to implement a new form of signal data extraction which was not available in the legacy system. The user is now able to select one of the signal processing routines provided by wrassp and pass it on to the signal data extraction function. The signal data extraction function can then apply this wrassp function to each audio file as part of the signal data extraction process. This means that the user can quickly manipulate function parameters and evaluate the result without having to store to disk the files that would usually be generated by the various parameter experiments. In many cases this new functionality eliminates the need for defining a track definition for the entire database for temporary data analysis purposes. The R code snippet below shows how the onTheFlyFunctionName parameter of the get_trackdata() function is used. ai_td_pit = get_trackdata(ae, seglist = ai_segs, onTheFlyFunctionName = &quot;mhsF0&quot;, verbose = FALSE) # show summary of ai_td summary(ai_td_pit) ## Emu track data from 6 segments ## ## Data is 1 dimensional from track pitch ## Mean data length is 30.5 samples 7.4 The resulting object: trackdata vs. emuRtrackdata The default resulting object of a call to get_trackdata() is of class trackdata (see R code snippet below). The emuR package provides multiple routines such as dcut(), trapply() and dplot() for processing and visually inspecting objects of this type (see harrington:2010a and Section 3.5 for examples of how these can be used). # show class vector of ai_td_pit class(ai_td_pit) ## [1] &quot;trackdata&quot; As the trackdata object is a fairly complex nested matrix object with internal reference matrices, which can be cumbersome to work with, the emuR package introduces a new equivalent object type called emuRtrackdata that essentially is a flat data.frame object. This object type can be retrieved by setting the resultType parameter of the get_trackdata() function to emuRtrackdata. The R code snippet below shows how this can be achieved. ai_emuRtd_pit = get_trackdata(ae, seglist = ai_segs, onTheFlyFunctionName = &quot;mhsF0&quot;, resultType = &quot;emuRtrackdata&quot;, verbose = FALSE) # show first row (convert to tibble only to prettify output) as_tibble(ai_emuRtd_pit[1, ]) ## # A tibble: 1 x 21 ## sl_rowIdx labels start end utts db_uuid session bundle start_item_id ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 ai 863. 1016. 0000… 0fc618… 0000 msajc… 161 ## # … with 12 more variables: end_item_id &lt;int&gt;, level &lt;chr&gt;, ## # start_item_seq_idx &lt;int&gt;, end_item_seq_idx &lt;int&gt;, type &lt;chr&gt;, ## # sample_start &lt;int&gt;, sample_end &lt;int&gt;, sample_rate &lt;int&gt;, ## # times_orig &lt;dbl&gt;, times_rel &lt;dbl&gt;, times_norm &lt;dbl&gt;, T1 &lt;dbl&gt; # show relative time values of the first segment # (relative time values always start at 0 for every segment) ai_emuRtd_pit[ai_emuRtd_pit$sl_rowIdx == 1, ]$times_rel ## [1] 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 ## [18] 85 90 95 100 105 110 115 120 125 130 135 140 145 # show original time values of the first segment # (absolute time values always start at the original # time stamp for that sample within the track) ai_emuRtd_pit[ai_emuRtd_pit$sl_rowIdx == 1, ]$times_orig ## [1] 867.5 872.5 877.5 882.5 887.5 892.5 897.5 902.5 907.5 912.5 ## [11] 917.5 922.5 927.5 932.5 937.5 942.5 947.5 952.5 957.5 962.5 ## [21] 967.5 972.5 977.5 982.5 987.5 992.5 997.5 1002.5 1007.5 1012.5 As can be seen by the first row output of the R code snippet above, the emuRtrackdata object is an amalgamation of both a segment list and a trackdata object. The first sl_rowIdx column of the ai_emuRtd_pit object indicates the row index of the segment list the current row belongs to, the times_rel and times_orig columns represent the relative time and the original time of the samples contained in the current row (see above R code snippet) and T1 (to Tn in n dimensional trackdata) contains the actual signal sample values. It is also worth noting that the emuR package provides a function called create_emuRtrackdata(), which allows users to create emuRtrackdata from a segment list and a trackdata object. This is beneficial as it allows trackdata objects to be processed using functions provided by the emuR package (e.g., dcut() and trapply()) and then converts them into a standardized data.frame object for further processing (e.g., using R packages such as lme4 or ggplot2 which were implemented to use with data.frame objects). The R code snippet below shows how the create_emuRtrackdata() function is used. # create emuRtrackdata object ai_emuRtd_pit = create_emuRtrackdata(sl = ai_segs, td = ai_td_pit) # show first row and # selected columns of ai_emuRtd_pit ai_emuRtd_pit[1, ] ## sl_rowIdx labels start end session bundle level type ## 1 1 ai 862.875 1015.825 0000 msajc010 Phonetic SEGMENT ## times_orig times_rel times_norm T1 ## 1 867.5 0 0 134.7854 The general question remains as to when to use the trackdata and when to use the emuRtrackdata object and what the benefit of each class is. The trackdata object has a number of associated class functions (e.g. trapply(), dcut(), dcut() and eplot()) that ease data manipulation and visualization. Further, it avoids data redundancy and therefore has a smaller memory footprint than the emuRtrackdata object (this is usually negligible on current systems); however, this makes it rather difficult to work with. The emuRtrackdata object is intended as a long term replacement for the trackdata object as it contains all of the information of the corresponding trackdata object as well as its associated segment list. As is often the case with tabular data, the emuRtrackdata object carries certain redundant information (e.g. segment start and end times). However, the benefit of having a data.frame object that contains all the information needed to process the data is the ability to replace package specific functions (e.g. trapply() etc.) with standardized data.frame processing and visualization procedures that can be applied to any data.frame object independent of the package that generated it. Therefore, the knowledge that is necessary to process an emuRtrackdata object can be transferred to/from other packages which is not the case for trackdata object. Future releases of the emuR package as well as this manual will contain various examples of how to replace the functionality of the package-specific functions mentioned above with equivalent data manipulation and visualization using the dplyr as well as the ggplot2 packages. 7.5 Conclusion This chapter introduced the signal data extraction mechanics of the emuR package. The combination of the get_trackdata() function and the file handling and signal processing abilities of the wrassp package (see Chapter 8 for further details) provide the user with a flexible system for extracting derived or complementary signal data belonging to their queried annotation items. References "],
["chap-wrassp.html", "8 The R package wrassp15 8.1 Introduction 8.2 File I/0 and the AsspDataObj 8.3 Signal processing 8.4 The wrasspOutputInfos object 8.5 Formants and their bandwidths 8.6 Logging wrassp’s function calls 8.7 Using wrassp in the EMU-SDMS 8.8 Storing data in the SSFF file format 8.9 Conclusion", " 8 The R package wrassp15 8.1 Introduction This chapter gives an overview and introduction to the wrassp package. The wrassp package is a wrapper for R around Michel Scheffers’ libassp (Advanced Speech Signal Processor). The libassp library and therefore the wrassp package provide functionality for handling speech signal files in most common audio formats and for performing signal analyses common in the phonetic and speech sciences. As such, wrassp fills a gap in the R package landscape as, to our knowledge, no previous packages provided this specialized functionality. The currently available signal processing functions provided by wrassp are: acfana(): Analysis of short-term autocorrelation function afdiff(): Computes the first difference of the signal affilter(): Filters the audio signal (e.g., low-pass and high-pass) cepstrum(): Short-term cepstral analysis cssSpectrum(): Cepstral smoothed version of dftSpectrum() dftSpectrum(): Short-term DFT spectral analysis forest(): Formant estimation ksvF0(): F0 analysis of the signal lpsSpectrum(): Linear predictive smoothed version of dftSpectrum() mhsF0(): Pitch analysis of the speech signal using Michel Scheffers’ Modified Harmonic Sieve algorithm rfcana(): Linear prediction analysis rmsana(): Analysis of short-term Root Mean Square amplitude zcrana(): Analysis of the averages of the short-term positive and negative zero-crossing rates The available file handling functions are: read.AsspDataObj(): read a SSFF or audio file into an AsspDataObj, which is the in-memory equivalent of the SSFF or audio file. write.AsspDataObj(): write an AsspDataObj to file (usually SSFF or audio file formats). See R’s help() function for a comprehensive list of every function and object provided by the wrassp package is required (see R code snippet below). help(package=&quot;wrassp&quot;) As the wrassp package can be used independently of the EMU-SDMS this chapter largely focuses on using it as an independent component. However, Section 8.7 provides an overview of how the package is integrated into the EMU-SDMS. Further, although the wrassp package has its own set of example audio files (which can be accessed in the directory provided by system.file('extdata', package='wrassp')), this chapter will use the audio and SSFF files that are part of the ae emuDB of the demo data provided by the emuR package. This is done primarily to provide an overview of what it is like using wrassp to work on files in an emuDB. The R code snippet below shows how to generate this demo data followed by a listing of the files contained in a directory of a single bundle called msajc003 (see Chapter @ref(chap:emuDB} for information about the emuDB format). The output of the call to list.files() shows four files where the .dft and .fms files are in the SSFF file format (see Appendix 15.1.3 for further details). The _annot.json file contains the annotation information, and the .wav file is one of the audio files that will be used in various signal processing examples in this chapter. # load the emuR package library(emuR) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # create path to bundle in database path2bndl = file.path(path2ae, &quot;0000_ses&quot;, &quot;msajc003_bndl&quot;) # list files in bundle directory list.files(path2bndl) ## [1] &quot;msajc003_annot.json&quot; &quot;msajc003.dft&quot; &quot;msajc003.fms&quot; ## [4] &quot;msajc003.wav&quot; 8.2 File I/0 and the AsspDataObj One of the aims of wrassp is to provide mechanisms for handling speech-related files such as audio files and derived and complementary signal files. To have an in-memory object that can hold these file types in a uniform way the wrassp package provides the AsspDataObj data type. The R code snippet below shows how the read.AsspDataObj() can be used to import a .wav audio file. # load the wrassp package library(wrassp) # create path to wav file path2wav = file.path(path2bndl, &quot;msajc003.wav&quot;) # read audio file au = read.AsspDataObj(path2wav) # show class class(au) ## [1] &quot;AsspDataObj&quot; # show print() output of object print(au) ## Assp Data Object of file /var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000gn/T//RtmpNwIJ55/emuR_demoData/ae_emuDB/0000_ses/msajc003_bndl/msajc003.wav. ## Format: WAVE (binary) ## 58089 records at 20000 Hz ## Duration: 2.904450 s ## Number of tracks: 1 ## audio (1 fields) As can be seen in the above R code snippet, the resulting au object is of the class AsspDataObj. The output of print provides additional information about the object, such as its sampling rate, duration, data type and data structure information. Since the file we loaded is audio only, the object contains exactly one track. Further, since it is a mono file, this track only has a single field. We will later encounter different types of data with more than one track and multiple fields per track. The R code snippet below shows function calls that extract the various attributes from the object (e.g., duration, sampling rate and the number of records). # show duration dur.AsspDataObj(au) ## [1] 2.90445 # show sampling rate rate.AsspDataObj(au) ## [1] 20000 # show number of records/samples numRecs.AsspDataObj(au) ## [1] 58089 # shorten filePath attribute # to 10 chars only to prettify output attr(au, &quot;filePath&quot;) = paste0(substr(attr(au, &quot;filePath&quot;), start = 1, stop = 45), &quot;...&quot;) # show additional attributes attributes(au) ## $names ## [1] &quot;audio&quot; ## ## $trackFormats ## [1] &quot;INT16&quot; ## ## $sampleRate ## [1] 20000 ## ## $filePath ## [1] &quot;/var/folders/yk/8z9tn7kx6hbcg_9n4c1sld980000g...&quot; ## ## $origFreq ## [1] 0 ## ## $startTime ## [1] 0 ## ## $startRecord ## [1] 1 ## ## $endRecord ## [1] 58089 ## ## $class ## [1] &quot;AsspDataObj&quot; ## ## $fileInfo ## [1] 21 2 The sample values belonging to a trackdata objects tracks are also stored within an AsspDataObj object. As mentioned above, the currently loaded object contains a single mono audio track. Accessing the data belonging to this track, in the form of a matrix, can be achieved using the track’s name in combination with the $ notation known from R’s common named list object. Each matrix has the same number of rows as the track has records and as many columns as the track has fields. The R code snippet below shows how the audio track can be accessed. # show track names tracks.AsspDataObj(au) ## [1] &quot;audio&quot; # or an alternative way to show track names names(au) ## [1] &quot;audio&quot; # show dimensions of audio attribute dim(au$audio) ## [1] 58089 1 # show first sample value of audio attribute head(au$audio, n = 1) ## [,1] ## [1,] 64 This data can, for example, be used to generate an oscillogram of the audio file as shown in the R code snippet below, which produces Figure 8.1. # calculate sample time of every 10th sample samplesIdx = seq(0, numRecs.AsspDataObj(au) - 1, 10) samplesTime = samplesIdx / rate.AsspDataObj(au) # extract every 10th sample using window() function samples = window(au$audio, deltat=10) # plot samples stored in audio attribute # (only plot every 10th sample to accelerate plotting) plot(samplesTime, samples, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;Audio samples (INT16)&quot;) Figure 8.1: Oscillogram generated from samples stored in the audio track of the object au. The export counterpart to read.AsspDataObj() function is write.AsspDataObj(). It is used to store in-memory AsspDataObj objects to disk and is particularly useful for converting other formats to or storing data in the SSFF file format as described in Section ??. To show how this function can be used to write a slightly altered version of the au object to a file, the R code snippet below initially multiplies all the sample values of au$audio by a factor of 0.5. The resulting AsspDataObj is then written to an audio file in a temporary directory provided by R’s tempdir() function. # manipulate the audio samples au$audio = au$audio * 0.5 # write to file in directory # provided by tempdir() write.AsspDataObj(au, file.path(tempdir(), &#39;newau.wav&#39;)) 8.3 Signal processing As mentioned in the introduction to this chapter, the wrassp package is capable of more than just the mere importing and exporting of specific signal file formats. This section will focus on demonstrating three of wrassp’s signal processing functions that calculate formant values, their corresponding bandwidths, the fundamental frequency contour and the RMS energy contour. Section 8.5 and ?? demonstrates signal processing to the audio file saved under path2wav, while Section 8.5.2 adresses processing all the audio files belonging to the ae emuDB. 8.4 The wrasspOutputInfos object The wrassp package comes with the wrasspOutputInfos object, which provides information about the various signal processing functions provided by the package. The wrasspOutputInfos object stores meta information associated with the different signal processing functions wrassp provides. The R code snippet below shows the names of the wrasspOutputInfos object which correspond to the function names listed in the introduction of this chapter. # show all function names names(wrasspOutputInfos) ## [1] &quot;acfana&quot; &quot;afdiff&quot; &quot;affilter&quot; &quot;cepstrum&quot; &quot;cssSpectrum&quot; ## [6] &quot;dftSpectrum&quot; &quot;ksvF0&quot; &quot;mhsF0&quot; &quot;forest&quot; &quot;lpsSpectrum&quot; ## [11] &quot;rfcana&quot; &quot;rmsana&quot; &quot;zcrana&quot; This object can be useful to get additional information about a specific wrassp function. It contains information about the default file extension ($ext), the tracks produced ($tracks) and the output file type ($outputType). The R code snippet below shows this information for the forest() function. # show output info of forest function wrasspOutputInfos$forest ## $ext ## [1] &quot;fms&quot; ## ## $tracks ## [1] &quot;fm&quot; &quot;bw&quot; ## ## $outputType ## [1] &quot;SSFF&quot; The examples that follow will make use of this wrasspOutputInfos object mainly to acquire the default file extensions given by a specific wrassp signal processing function. 8.5 Formants and their bandwidths The already mentioned forest() is wrassp’s formant estimation function. The default behavior of this formant tracker is to calculate the first four formants and their bandwidths. The R code snippet below shows the usage of this function. As the default behavior of every signal processing function provided by wrassp is to store its result to a file, the toFile parameter of forest() is set to FALSE to prevent this behavior. This results in the same AsspDataObj object as when exporting the result to file and then importing the file into R using read.AsspDataObj(), but circumvents the disk reading/writing overhead. # calculate formants and corresponding bandwidth values fmBwVals = forest(path2wav, toFile=F) # show class vector class(fmBwVals) ## [1] &quot;AsspDataObj&quot; # show track names tracks.AsspDataObj(fmBwVals) ## [1] &quot;fm&quot; &quot;bw&quot; # show dimensions of &quot;fm&quot; track dim(fmBwVals$fm) ## [1] 581 4 # check dimensions of tracks are the same all(dim(fmBwVals$fm) == dim(fmBwVals$bw)) ## [1] TRUE As can be seen in the above R code snippet, the object resulting from the forest() function is an object of class AsspDataObj with the tracks \"fm\" (formants) and \"bw\" (formant bandwidths), where both track matrices have four columns (corresponding to F1, F2, F3 and F4 in the \"fm\" track and F1bandwidth, F2bandwidth, F3bandwidth and F4bandwidth in the \"bw\" track) and 581 rows. To visualize the calculated formant values, the R code snippet below shows how R’s matplot() function can be used to produce Figure 8.2. # plot the formant values matplot(seq(0, numRecs.AsspDataObj(fmBwVals) - 1) / rate.AsspDataObj(fmBwVals) + attr(fmBwVals, &quot;startTime&quot;), fmBwVals$fm, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;Formant frequency (Hz)&quot;) # add legend startFormant = 1 endFormant = 4 legend(&quot;topright&quot;, legend = paste0(&quot;F&quot;, startFormant:endFormant), col = startFormant:endFormant, lty = startFormant:endFormant, bg = &quot;white&quot;) Figure 8.2: Matrix plot of formant values stored in the fm track of fmBwVals object. 8.5.1 Fundamental frequency contour The wrassp package includes two fundamental frequency estimation functions called ksvF0() and mhsF0(). The R code snippet below shows the usage of the ksvF0() function, this time not utilizing the toFile parameter but rather to show an alternative procedure, reading the resulting SSFF file produced by it. It is worth noting that every signal processing function provided by wrassp creates a result file in the same directory as the audio file it was processing (except if the outputDirectory parameter is set otherwise). The default extension given by the ksvF0() is stored in wrasspOutputInfos\\$ksvF0\\$ext, which is used in the R code snippet below to create the newly generated file’s path. # calculate the fundamental frequency contour ksvF0(path2wav) # create path to newly generated file path2f0file = file.path(path2bndl, paste0(&quot;msajc003.&quot;, wrasspOutputInfos$ksvF0$ext)) # read file from disk f0vals = read.AsspDataObj(path2f0file) By analogy with to the formant estimation example, the R code snippet below shows how the plot() function can be used to visualize this data as in Figure 8.3. # plot the fundamental frequency contour plot(seq(0,numRecs.AsspDataObj(f0vals) - 1) / rate.AsspDataObj(f0vals) + attr(f0vals, &quot;startTime&quot;), f0vals$F0, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;F0 frequency (Hz)&quot;) Figure 8.3: Plot of fundamental frequency values stored in the F0 track of f0vals object. 8.5.2 RMS energy contour The wrassp function for calculating the short-term root mean square (RMS) amplitude of the signal is called rmsana(). As its usage is analogous to the above examples, here we will focus on using it to calculate the RMS values for all the audio files of the ae emuDB. The R code snippet below initially uses the list.files() function to aquire the file paths for every .wav file in the ae emuDB. As every signal processing function accepts one or multiple file paths, these file paths can simply be passed in as the main argument to the rmsana() function. As all of wrassp’s signal processing functions place their generated files in the same directory as the audio file they process, the rmsana() function will automatically place every .rms into the correct bundle directory. # list all .wav files in the ae emuDB paths2wavFiles = list.files(path2ae, pattern = &quot;.*wav$&quot;, recursive = TRUE, full.names = TRUE) # calculate the RMS energy values for all .wav files rmsana(paths2wavFiles) # list new .rms files using # wrasspOutputInfos-&gt;rmsana-&gt;ext rmsFPs = list.files(path2ae, pattern = paste0(&quot;.*&quot;, wrasspOutputInfos$rmsana$ext), recursive = TRUE, full.names = TRUE) # read first RMS file rmsvals = read.AsspDataObj(rmsFPs[1]) The R code snippet below shows how the plot() function can be used to visualize this data as in Figure 8.4. # plot the RMS energy contour plot(seq(0, numRecs.AsspDataObj(rmsvals) - 1) / rate.AsspDataObj(rmsvals) + attr(rmsvals, &quot;startTime&quot;), rmsvals$rms, type = &quot;l&quot;, xlab = &quot;time (s)&quot;, ylab = &quot;RMS energy (dB)&quot;) Figure 8.4: Plot of RMS values stored in rms track of the rmsvals object. 8.6 Logging wrassp’s function calls As it can be extremely important to keep track of information about how certain files are created and calculated, every signal processing function provided by the wrassp package comes with the ability to log its function calls to a specified log file. The R code snippet below shows a call to the ksvF0() function where a single parameter was changed from its default value (windowShift = 10). The content of the created log files (shown by the call to readLines()) contains the function name, time stamp, parameters that were altered and processed file path information. It is worth noting that a log file can be reused for multiple function calls as the log function does not overwrite an existing file but merely appends new log information to it. # create path to log file in root dir of ae emuDB path2logFile = file.path(path2ae, &quot;wrassp.log&quot;) # calculate the fundamental frequency contour ksvF0(path2wav, windowShift = 10, forceToLog = T, optLogFilePath = path2logFile) ## [1] 1 # display content of log file (first 8 lines) readLines(path2logFile)[1:8] ## [1] &quot;&quot; ## [2] &quot;##################################&quot; ## [3] &quot;##################################&quot; ## [4] &quot;######## ksvF0 performed ########&quot; ## [5] &quot;Timestamp: 2019-05-10 18:02:17 &quot; ## [6] &quot;windowShift : 10 &quot; ## [7] &quot;forceToLog : T &quot; ## [8] &quot; =&gt; on files:&quot; 8.7 Using wrassp in the EMU-SDMS As shown in Section 8.5.2, the wrassp signal processing functions can be used to calculate SSFF files and place them into the appropriate bundle directories. The only thing that has to be done to make an emuDB aware of these files is to add an SSFF track definition to the emuDB as shown in the R code snippet below. Once added, this SSFF track can be referenced via the ssffTrackName parameter of the get_trackdata() function as shown in various examples throughout this documentation. It is worth noting that this strategy is not necessarily relevant for applying the same signal processing to an entire emuDB, as this can be achieved using the on-the-fly add_ssffTrackDefinition() method described in the according R code snippet below. However, it becomes necessary if certain bundles are to be processed using deviating function parameters. This can, for example, be relevant when setting the minimum and maximum frequencies that are to be considered while estimating the fundamental frequencies (e.g., the maxF and minF of ksvfF0()) for female versus male speakers. # load emuDB ae = load_emuDB(path2ae) # add SSFF track defintion # that references the .rms files # calculated above # (i.e. no new files are calculated and added to the emuDB) ext = wrasspOutputInfos$rmsana$ext colName = wrasspOutputInfos$rmsana$tracks[1] add_ssffTrackDefinition(ae, name = &quot;rms&quot;, fileExtension = ext, columnName = colName) A further way to utilize wrassp’s signal processing functions as part of the EMU-SDMS is via the onTheFlyFunctionName and onTheFlyParams parameters of the add_ssffTrackDefinition() and get_trackdata() functions. Using the onTheFlyFunctionName parameter in the add_ssffTrackDefinition() function automatically calculates the SSFF files while also adding the SSFF track definition. Using this parameter with the get_trackdata() function calls the given wrassp function with the toFile parameter set to FALSE and extracts the matching segments and places them in the resulting trackdata or emuRtrackdata object. In many cases, this avoids the necessity of having SSFF track definitions in the emuDB. In both functions, the optional onTheFlyParams parameter can be used to specify the parameters that are passed into the signal processing function. The R code snippet below shows how R’s formals() function can be used to get all the parameters of wrassp’s short-term positive and negative zero-crossing rate (ZCR) analysis function zrcana(). It then changes the default window size parameter to a new value and passes the parameters object into the add_ssffTrackDefinition() and get_trackdata() functions. # get all parameters of zcrana zcranaParams = formals(&quot;zcrana&quot;) # show names of parameters names(zcranaParams) ## [1] &quot;listOfFiles&quot; &quot;optLogFilePath&quot; &quot;beginTime&quot; ## [4] &quot;centerTime&quot; &quot;endTime&quot; &quot;windowShift&quot; ## [7] &quot;windowSize&quot; &quot;toFile&quot; &quot;explicitExt&quot; ## [10] &quot;outputDirectory&quot; &quot;forceToLog&quot; &quot;verbose&quot; # change window size from the default # value of 25 ms to 50 ms zcranaParams$windowSize = 50 # to have a segment list to work with # query all Phonetic &#39;n&#39; segments sl = query(ae, &quot;Phonetic == n&quot;) # get trackdata calculating ZCR values on-the-fly # using the above parameters. Note that no files # are generated. td = get_trackdata(ae, sl, onTheFlyFunctionName = &quot;zcrana&quot;, onTheFlyParams = zcranaParams, verbose = FALSE) # add SSFF track definition. Note that # this time files are generated. add_ssffTrackDefinition(ae, name = &quot;zcr&quot;, onTheFlyFunctionName = &quot;zcrana&quot;, onTheFlyParams = zcranaParams, verbose = FALSE) 8.8 Storing data in the SSFF file format One of the benefits gained by having the AsspDataObj in-memory object is that these objects can be constructed from scratch in R, as they are basically simple list objects. This means, for example, that any set of n-dimensional samples over time can be placed in a AsspDataObj and then stored as an SSFF file using the write.AsspDataObj() function. To show how this can be done, the R code snippet below creates an arbitrary data sample in the form of a single cycle sine wave between \\(0\\) and \\(2*pi\\) that is made up of 16000 samples and displays it in Figure 8.5. x = seq(0, 2 * pi, length.out = 16000) sineWave = sin(x) plot(x, sineWave, type = &#39;l&#39;, xlab = &quot;x from 0 to 2*pi&quot;, ylab = &quot;&quot;) Figure 8.5: A single cycle sine wave consisting of 16000 samples. Assuming a sample rate of 16 kHz sineWave would result in a sine wave with a frequency of 1 Hz and a duration of one second. The R code snippet below shows how a AsspDataObj can be created from scratch and the data in sineWave placed into one of its tracks. It then goes on to write the AsspDataObj object to an SSFF file. # create empty list object ado = list() # add sample rate attribute attr(ado, &quot;sampleRate&quot;) = 16000 # add start time attribute attr(ado, &quot;startTime&quot;) = 0 # add start record attribute attr(ado, &quot;startRecord&quot;) = as.integer(1) # add end record attribute attr(ado, &quot;endRecord&quot;) = as.integer(length(sineWave)) # set class of ado class(ado) = &quot;AsspDataObj&quot; # show available file formats AsspFileFormats ## RAW ASP_A ASP_B XASSP IPDS_M IPDS_S AIFF AIFC CSL ## 1 2 3 4 5 6 7 8 9 ## CSRE ESPS ILS KTH SWELL SNACK SFS SND AU ## 10 11 12 13 13 13 14 15 15 ## NIST SPHERE PRAAT_S PRAAT_L PRAAT_B SSFF WAVE WAVE_X XLABEL ## 16 16 17 18 19 20 21 22 24 ## YORK UWM ## 25 26 # set file format to SSFF # NOTE: assignment of &quot;SSFF&quot; also possible AsspFileFormat(ado) = as.integer(20) # set data format (1 == &#39;ascii&#39; and 2 == &#39;binary&#39;) AsspDataFormat(ado) = as.integer(2) # set track format specifiers # (available track formats for numbers # that match their C equivalent are: # &quot;UINT8&quot;; &quot;INT8&quot;; &quot;UINT16&quot;; &quot;INT16&quot;; # &quot;UINT24&quot;; &quot;INT24&quot;; &quot;UINT32&quot;; &quot;INT32&quot;; # &quot;UINT64&quot;; &quot;INT64&quot;; &quot;REAL32&quot;; &quot;REAL64&quot;); attr(ado, &quot;trackFormats&quot;) = c(&quot;REAL32&quot;) # add track ado = addTrack(ado, &quot;sine&quot;, sineWave, &quot;REAL32&quot;) # write AsspDataObj object to file write.AsspDataObj(dobj = ado, file = file.path(tempdir(), &quot;example.sine&quot;)) ## NULL Although somewhat of a generic example, the above R code snippet shows how to generate an AsspDataObj from scratch. This approach can, for example, be used to read in signal data produced by other software or signal data acquisition devices. Hence, this approach can be used to import many forms of data into the EMU-SDMS. Appendix @ref(sec:app-chap-wrassp-praatsSigProc} shows an example of how this approach can be used to take advantage of Praat’s signal processing capabilities and integrate its output into the EMU-SDMS. 8.9 Conclusion The wrassp packages enriches the R package landscape by providing functionality for handling speech signal files in most common audio formats and for performing signal analyses common in the phonetic and speech sciences. The EMU-SDMS utilizes the functionality that the wrassp package provides by allowing the user to calculate signals that match the segments of a segment list. This can either be done in real time or by extracting the signals from files. Hence, the wrassp package is an integral part of the EMU-SDMS but can also be used as a standalone package if so desired. Some examples of this chapter are adapted version of examples given in the wrassp_intro vignette of the wrassp package.↩ "],
["chap-emu-webApp.html", "9 The EMU-webApp16 9.1 Main layout 9.2 General usage 9.3 Configuring the EMU-webApp 9.4 Conclusion", " 9 The EMU-webApp16 The EMU-SDMS has a unique approach to its GUI in that it utilizes a web application as its primary GUI. This is known as the EMU-webApp . The EMU-webApp is a fully fledged browser-based labeling and correction tool that offers a multitude of labeling and visualization features. These features include unlimited undo/redo, formant correction capabilities, the ability to snap a preselected boundary to the nearest top/bottom boundary, snap a preselected boundary to the nearest zero crossing, and many more. The web application is able to render everything directly in the user’s browser, including the calculation and rendering of the spectrogram, as it is written entirely using HTML, CSS and JavaScript. This means it can also be used as a standalone labeling application, as it does not require any server-side calculations or rendering. Further, it is designed to interact with any websocket server that implements the EMU-webApp websocket protocol (see Section 13.1). This enables it to be used as a labeling tool for collaborative annotation efforts. Also, as the EMU-webApp is cached in the user’s browser on the first visit, it does not require any internet connectivity to be able to access the web application unless the user explicitly clears the browser’s cache. The URL of the current live version of the EMU-webApp is: http://ips-lmu.github.io/EMU-webApp/. 9.1 Main layout The main screen of the EMU-webApp can be split into five areas. Figure 9.1 shows a screenshot of the EMU-webApp’s main screen displaying these five areas while displaying a bundle of the ae demo database. This database is served to the EMU-webApp by invoking the serve() command as shown in the R code snippet below. The left side bar (area marked 1 in Figure 9.1) represents the bundle list side bar which, if connected to a database, displays the currently available bundles grouped by their sessions. The top and bottom menu bars (areas marked 2 and 5 in Figure 9.1) display the currently available menu options, where the bottom menu bar contains the audio navigation and playback controls and also includes a scrollable mini map of the oscillogram. Area 3 of Figure 9.1 displays the signal canvas area currently displaying the oscillogram and the spectrogram. Other signal contours such as formant frequency contours and fundamental frequency contours are also displayed in this area. Area 4 of Figure 9.1 displays the area in which levels containing time information are displayed. It is worth noting that the main screen of the EMU-webApp does not display any levels that do not contain time information. The hierarchical annotation can be displayed and edited by clicking the show hierarchy button in the top menu bar (see Figure 9.6 for an example of how the hierarchy is displayed). # serve ae emuDB to EMU-webApp serve(ae) Figure 9.1: Screenshot of EMU-webApp displaying the ae demo database with overlaid areas of the main screen of the web application (see text). 9.2 General usage This section introduces the labeling mechanics and general labeling workflow of the EMU-webApp. The EMU-webApp makes heavy use of keyboard shortcuts. Is is worth noting that most of the keyboard shortcuts are centered around the WASD keys, which are the navigation shortcut keys (W to zoom in; S to zoom out; A to move left and D to move right). For a full list of the available keyboard shortcuts see the EMU-webApp’s own manual, which can be accessed by clicking the EMU icon on the right hand side of the top menu bar (area 2 in Figure 9.1). 9.2.1 Annotating levels containing time information 9.2.1.1 Boundaries and events The EMU-webApp has slightly different labeling mechanics compared with other annotation software. Compared to the usual click and drag of segment boundaries and event markers, the web application continuously tracks the movement of the mouse in levels containing time information, highlighting the boundary or event marker that is closest to it by coloring it blue. Figure 9.2 displays this automatic boundary preselection. Figure 9.2: Screenshot of segment level as displayed by the EMU-webApp with superimposed mouse cursor displaying the automatic boundary preselection of closest boundary (boundary marked blue). Once a boundary or event is preselected, the user can perform various actions with it. She or he can, for example, grab a preselected boundary or event by holding down the SHIFT key and moving it to the desired position, or delete the current boundary or event by hitting the BACKSPACE key. Other actions that can be performed on preselected boundaries or events are: snap to closest boundary or event in level above (Keyboard Shortcut t), snap to closest boundary or event in level below (Keyboard Shortcut b), and snap to nearest zero crossing (Keyboard Shortcut x). To add a new boundary or event to a level the user initially has to select the desired level she or he wishes to edit. This is achieved either by using the up and down cursor keys or by single-left-clicking on the desired level. The current preselected level is marked in a darker shade of gray, as is displayed in Figure 9.3. Figure 9.3: Screenshot of two levels as displayed by the EMU-webApp, where the lower level is preselected (i.e., marked in a darker shade of gray). To add a boundary to the currently selected level one first has to select a point in time either in the spectrogram or the oscillogram by single-left-clicking on the desired location. Hitting the enter/return key adds a new boundary or event to the preselected level at the selected time point. Selecting a stretch of time in the spectrogram or the oscillogram (left-click-and-drag) and hitting enter will add a segment (not a boundary) to a preselected segment level. 9.2.1.2 Segments and events The EMU-webApp also allows segments and events to be preselected by single-left-clicking the desired item. The web application colors the preselected segments and events yellow to indicate their pre-selection as displayed in Figure 9.4. Figure 9.4: Screenshot of level as displayed by the EMU-webApp, where the /@/ segment is currently preselected as it is marked yellow. As with preselected boundaries or events the user can now perform multiple actions with these preselected items. She or he can, for example, edit the item’s label by hitting the enter/return key (which can also be achieved by double-left-clicking the item). Other actions that can be performed on preselected items are: Select next item in level (keyboard shortcut TAB), Select previous item in level (keyboard shortcut SHIFT plus TAB), Add time to selected item(s) end (keyboard shortcut +), Add time to selected item(s) start (keyboard shortcut SHIFT plus +), Remove time to selected item(s) end (keyboard shortcut -), Remove time to selected item(s) start (keyboard shortcut SHIFT plus -), and Move selected item(s) (hold down ALT Key and drag to desired position). By right-clicking adjacent segment or events (keyboard shortcut SHIFT plus left or right cursor keys), it is possible to select multiple items at once. 9.2.1.3 Parallel labels in segments and events If a level containing time information has multiple attribute definitions (i.e., multiple parallel labels per segment or event) the EMU-webApp automatically displays radio buttons underneath that level (see red square in Figure 9.5) that allow the user to switch between the parallel labels. Figure 9.5 displays a segment level with three attribute definitions. Figure 9.5: Screenshot of segment level with three attribute definitions. The radio buttons that switch between the parallel labels are highlighted by a red square. 9.2.1.4 Legal labels As mentioned in Section 5.2.3.2, an array of so-called legal labels can be defined for every level or, more specifically, for each attribute definition. The EMU-webApp enforces these legal labels by not allowing any other labels to be entered in the label editing text fields. If an illegal label is entered, the text field will turn red and the EMU-webApp will not permit this label to be saved. 9.2.2 Working with hierarchical annotations17 9.2.2.1 Viewing the hierarchy As mentioned in Section 9.1, pressing the show hierarchy button (keyboard shortcut h) in the top menu bar opens the hierarchy view modal window. As with most modal windows in the EMU-webApp, it can be closed by clicking on the close button, clicking the X circle icon in the top right hand corner of the modal or by hitting the ESCAPE key. By default, the hierarchy modal window displays a horizontal version of the hierarchy for a spatially economical visualization. As most people are more familiar with a vertical hierarchical annotation display, the hierarchy can be rotated by hitting the rotate by 90° button (keyboard shortcut r). Zooming in and out of the hierarchy can be achieved by using the mouse wheel, and moving through the hierarchy in time can be achieved by holding down the left mouse button and dragging the hierarchy in the desired direction. Figure 9.6 shows the hierarchy modal window displaying the hierarchical annotation of a single path (Utterance -&gt; Intonational -&gt; Intermediate -&gt; Word -&gt; Syllable -&gt; Phoneme -&gt; Phonetic) through a multi-path hierarchy of the ae emuDB in its horizontal form. Figure 9.6: Screenshot of the hierarchy modal window level displaying a path through the hierarchy of the ae emuDB in its horizontal form. 9.2.2.2 Selecting a path through the hierarchy As more complex databases have multiple hierarchical paths through their hierarchical annotation structure (see Figure 4.2 for an example of a multi-dimensional hierarchical annotation structure), the hierarchy modal offers a drop-down menu to choose the current path to be displayed. Area 2 in Figure 9.7 marks the hierarchy path drop-down menu of the hierarchy modal. Figure 9.7: Screenshot of top of hierarchy modal window of the EMU-webApp in which the area marked 1 shows the drop-down menus for selecting the parallel label for each level and area 2 marks the hierarchy path drop-down menu. It is worth noting that only non-partial paths can be selected in the hierarchy path drop-down menu. 9.2.2.3 Selecting parallel labels in timeless levels As timeless levels may also contain multiple parallel labels, the hierarchy path modal window provides a drop-down menu for each level to select which label or attribute definition is to be displayed. Area 1 of Figure 9.7 displays these drop-down menus. 9.2.2.4 Adding a new item The hierarchy modal window provides two methods for adding new annotation items to a level. This can either be achieved by pressing the blue and white + button next to the level’s name (which appends a new item to the end of the level) or by preselecting an annotation item (by hovering the mouse over it) and hitting either the n (insert new item before preselected item) or the m key (insert new item after preselected item). 9.2.2.5 Modifying an annotation item An item’s context menu18 is opened by single-left-clicking its node. The resulting context menu displays a text area in which the label of the annotation item can be edited, a play button to play the audio section associated with the item and a collapse arrow button allowing the user to collapse the sub-tree beneath the current item. Collapsing a sub-tree can be useful for masking parts of the hierarchy while editing. A screenshot of the context menu is displayed in Figure 9.8. Figure 9.8: Screenshot of the hierarchy modal window of the EMU-webApp displaying an annotation item’s context menu. 9.2.2.6 Adding a new link Adding a new link between two items can be achieved by hovering the mouse over one of the two items, holding down the SHIFT key and moving the mouse cursor to the other item. A green dashed line indicates that the link to be added is valid, while a red dashed line indicates it is not. A link’s validity is dependent on the database’s configuration (i.e., if there is a link definition present and the type of link definition) as well as the non-crossing constraint (Coleman and Local 1991) that essentially implies that links are not allowed to cross each other. If the link is valid (i.e., a green dashed line is present), releasing the SHIFT key will add the link to the annotation. 9.2.2.7 Deleting an annotation item or a link Items and links are deleted by initially preselecting them by hovering the mouse cursor over them. The preselected items are marked blue and preselected links yellow. A preselected link is removed by hitting BACKSPACE and a preselected item is deleted by hitting the y key. Deleting an item will also delete all links leading to and from it. 9.3 Configuring the EMU-webApp This section will give an overview of how the EMU-webApp can be configured. The configuration of the EMU-webApp is stored in the EMUwebAppConfig section of the _DBconfig.json of an emuDB (see Appendix 15.1.1 for details). This means that the EMU-webApp can be configured separately for every emuDB. Although it can be necessary for some advanced configuration options to manually edit the _DBconfig.json using a text editor (see Section 9.3.3), the most common configuration operations can be achieved using functions provided by the emuR package (see Section 9.3.1). A central concept for configuring the EMU-webApp are so-called perspectives. Essentially, a perspective is an independent configuration of how the EMU-webApp displays a certain set of data. Having multiple perspectives allows the user to switch between different views of the data. This can be especially useful when dealing with complex annotations where only showing certain elements for certain labeling tasks can be beneficial. Figure 9.9 displays a screenshot of the perspectives side bar menu of the EMU-webApp which displays the three perspectives of the ae emuDB19. The default perspective displays both the Phonetic and the Tone levels where as the Phonetic-only and the Tone-only only display these levels individually. Figure 9.9: Screenshot of the hierarchy modal window of the EMU-webApp displaying an annotation item’s context menu. 9.3.1 Basic configurations using emuR The R code snippet below shows how to create and load the demo data that will be used throughout the rest of this chapter. # load package library(emuR) # create demo data in directory provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) As mentioned above, the EMU-webApp subdivides different ways to look at an emuDB into so-called perspectives. Users can switch between these perspectives in the web application. They contain, for example, information on what levels are displayed, which SSFF tracks are drawn. The R code snippet below shows how the current perspectives can be listed using the list_perspectives() function. # list perspectives of ae emuDB list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default OSCI; SPEC Phonetic; Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone As it is sometimes necessary to add new or remove existing perspectives to or from a database, the R code snippet below shows how this can be achieved using emuR’s add/remove_perspective() functions. # add new perspective to ae emuDB add_perspective(ae, name = &quot;tmpPersp&quot;) # show added perspective list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default OSCI; SPEC Phonetic; Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone ## 4 tmpPersp OSCI; SPEC # remove newly added perspective remove_perspective(ae, name = &quot;tmpPersp&quot;) 9.3.2 Signal canvas and level canvas order As already mentioned, the above R code snippet shows that the ae emuDB contains three perspectives. The first perspective (default) displays the oscillogram (OSCI) followed by the spectrogram (SPEC) in the signal canvas area (area 3 of Figure 9.1) and the Phonetic and Tone levels in the level canvas area (area 4 of Figure 9.1). It is worth noting that OSCI (oscillogram) and SPEC (spectrogram) are predefined signal tracks that are always available. This is indicated by the capital letters indicating that they are predefined constants. The R code snippet below shows how the order of the signal canvases and level canvases can be changed using the get/set_signalCanvasesOrder() and get/set_levelCanvasesOrder(). # get order vector of signal canvases of default perspective sco = get_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;) # show sco vector sco ## [1] &quot;OSCI&quot; &quot;SPEC&quot; # reverse sco order # using R&#39;s rev() function scor = rev(sco) # set order vector of signal canvases of default perspective set_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = scor) # set order vector of level canvases of default perspective # to only display the &quot;Tone&quot; level set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Tone&quot;)) # list perspectives of ae emuDB # to show changes list_perspectives(ae) ## name signalCanvasesOrder levelCanvasesOrder ## 1 default SPEC; OSCI Tone ## 2 Phonetic-only OSCI; SPEC Phonetic ## 3 Tone-only OSCI; SPEC Tone After the changes made in the R code snippet above, the default perspective will show the spectrogram above the oscillogram in the signal canvas area and only the Tone level in the level canvas area. Only levels with time information are allowed to be displayed in the level canvas area, and the set_levelCanvasesOrder() will print an error if a level of type ITEM is added (see R code snippet below). # set level canvas order where a # level is passed into the order parameter # that is not of type EVENT or SEGMENT set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Syllable&quot;)) ## Error in set_levelCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;Syllable&quot;)): levelDefinition with name &#39;Syllable&#39; is not of type &#39;SEGMENT&#39; or &#39;EVENT&#39; The same mechanism used above can also be used to display any SSFF track that is defined for the database by referencing its name. The R code snippet below shows how the existing SSFF track called fm (containing formant values calculated by wrassp’s forest() function) can be added to the signal canvas area. # show currently available SSFF tracks list_ssffTrackDefinitions(ae) ## name columnName fileExtension ## 1 dft dft dft ## 2 fm fm fms # re-set order vector of signal canvases of default perspective # by appending the fm track set_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(scor, &quot;fm&quot;)) A screenshot of the current display of the default perspective can be seen in Figure 9.10. Figure 9.10: Screenshot of signal and level canvases displays of the EMU-webApp after the changes made in the above R code snippets. 9.3.3 Advanced configurations made by editing the _DBconfig.json Although the above configuration options cover the most common use cases, the EMU-webApp offers multiple other configuration options that are currently not configurable via functions provided by emuR. These advanced configuration options can currently only be achieved by manually editing the _DBconfig.json file using a text editor. As even the colors used in the EMU-webApp and every keyboard shortcut can be reconfigured, here we will focus on the more common advanced configuration options. A full list of the available configuration fields of the EMUwebAppConfig section of the _DBconfig.json including their meaning, can be found in Appendix 15.1.1. 9.3.3.1 Overlaying signal canvases To save space it can be beneficial to overlay one or more signal tracks onto other signal canvases. This can be achieved by manually editing the assign array of the EMUwebAppConfig:perspectives[persp_idx]:signalCanvases field in the _DBconfig.json. Listing ?? shows an example configuration that overlays the fm track on the oscillogram where the OSCI string can be replaced by any other entry in the EMUwebAppConfig:perspectives[persp_idx]:signalCanvases:order array. Figure 9.11 displays a screenshot of such an overlay. ... &quot;assign&quot;: [{ &quot;signalCanvasName&quot;: &quot;OSCI&quot;, &quot;ssffTrackName&quot;: &quot;fm&quot; }], ... Figure 9.11: Screenshot of signal canvases display of the EMU-webApp after the changes made in the R code snippets above. 9.3.3.2 Frequency-aligned formant contours spectrogram overlay The current mechanism for laying frequency-aligned formant contours over the spectrogram is to give the formant track the predefined name FORMANTS. If the formant track is called FORMANTS and it is assigned to be laid over the spectrogram (see Listing ??) the EMU-webApp will frequency-align the contours to the current minimum and maximum spectrogram frequencies (see Figure 9.12). ... &quot;assign&quot;: [{ &quot;signalCanvasName&quot;: &quot;SPEC&quot;, &quot;ssffTrackName&quot;: &quot;FORMANTS&quot; }], ... Figure 9.12: Screenshot of signal canvases area of the EMU-webApp displaying formant contours that are overlaid on the spectrogram and frequency-aligned. 9.3.3.3 Correcting formants The above configuration of the frequency-aligned formant contours will automatically allow the FORMANTS track to be manually corrected. Formants can be corrected by hitting the appropriate number key (1 = first formant, 2 = second formant, …). Similar to boundaries and events, the mouse cursor will automatically be tracked in the SPEC canvas and the nearest formant value preselected. Holding down the SHIFT key moves the current formant value to the mouse position, hence allowing the contour to be redrawn and corrected. 9.3.4 2D canvas The EMU-webApp has an additional canvas which can be configured to display two-dimensional data. Figure 9.13 shows a screenshot of the 2D canvas, which is placed in the bottom right hand corner of the level canvas area of the web application. The screenshot shows data representing EMA sensor positions on the mid sagittal plane. Listings ?? shows how the 2D canvas can be configured. Essentially, every drawn dot is configured by assigning a column in an SSFF track that specifies the X values and an additional column that specifies the Y values. Figure 9.13: Screenshot of 2D canvas of the EMU-webApp displaying two-dimensional EMA data. ... &quot;twoDimCanvases&quot;: { &quot;order&quot;: [&quot;DOTS&quot;], &quot;twoDimDrawingDefinitions&quot;: [{ &quot;name&quot;: &quot;DOTS&quot;, &quot;dots&quot;: [{ &quot;name&quot;: &quot;tt&quot;, &quot;xSsffTrack&quot;: &quot;tt_posy&quot;, &quot;xContourNr&quot;: 0, &quot;ySsffTrack&quot;: &quot;tt_posz&quot;, &quot;yContourNr&quot;: 0, &quot;color&quot;: &quot;rgb(255,0,0)&quot; }, ... &quot;connectLines&quot;: [{ &quot;fromDot&quot;: &quot;tt&quot;, &quot;toDot&quot;: &quot;tm&quot;, &quot;color&quot;: &quot;rgb(0,0,0)&quot; }, ... 9.3.4.1 EPG The 2D canvas of the EMU-webApp can also be configured to display EPG data as displayed in Figure 9.14. The SSFF file containing the EPG data has to be formated in a specific way. The format is a set of eight bytes per point in time, where each byte represents a row of electrodes on the artificial palate. Each binary bit value per byte indicates whether one of the eight sensors is activated or not (i.e., tongue contact was measured). If data in this format and an SSFF track with the predefined name EPG referencing the SSFF files are present, the 2D canvas can be configured to display this data by adding the EPG to the twoDimCanvases:order array as shown in Listing ??. Figure 9.14: Screenshot of 2D canvas of the EMU-webApp displaying EPG palate traces. &quot;twoDimCanvases&quot;: { &quot;order&quot;: [&quot;EPG&quot;] } 9.3.4.2 EMA gestural landmark recognition The EMU-webApp can also be configured to semi-automatically detect gestural landmarks of EMA contours. The functions implemented in the EMU-webApp are based on various Matlab scripts by Phil Hoole. For a description of which gestural landmarks are detected and how these are detected, see Bombien (2011) page 61 ff. Compared to the above configurations, configuring the EMU-webApp to semi-automatically detect gestural landmarks of EMA contours is done as part of the level definition’s configuration entries of the _DBconfig.json. Listing ?? shows the anagestConfig entry, which configures the tongueTipGestures event level for this purpose. Within the web application this level has to be preselected by the user and a region containing a gesture in the SSFF track selected (left click and drag). Hitting the ENTER/RETURN key then executes the semi-automatic gestural landmark recognition functions. If multiple candidates are recognized for certain landmarks, the user will be prompted to select the appropriate landmark. ... &quot;levelDefinitions&quot;: [{ { &quot;name&quot;: &quot;tongueTipGestures&quot;, &quot;type&quot;: &quot;EVENT&quot;, &quot;attributeDefinitions&quot;: [{ &quot;name&quot;: &quot;tongueTipGestures&quot;, &quot;type&quot;: &quot;STRING&quot; }], &quot;anagestConfig&quot;: { &quot;verticalPosSsffTrackName&quot;: &quot;tt_posz&quot;, &quot;velocitySsffTrackName&quot;: &quot;t_tipTV&quot;, &quot;autoLinkLevelName&quot;: &quot;ORT&quot;, &quot;multiplicationFactor&quot;: 1, &quot;threshold&quot;: 0.2, &quot;gestureOnOffsetLabels&quot;: [&quot;gon&quot;, &quot;goff&quot;], &quot;maxVelocityOnOffsetLabels&quot;: [&quot;von&quot;, &quot;voff&quot;], &quot;constrictionPlateauBeginEndLabels&quot;: [&quot;pon&quot;, &quot;poff&quot;], &quot;maxConstrictionLabel&quot;: &quot;mon&quot; } ... The user will be prompted to select an annotation item of the level specified in anagestConfig:autoLinkLevelName once the gestural landmarks are recognized. The EMU-webApp then automatically links all gestural landmark events to that item. 9.4 Conclusion This chapter provided an overview of the EMU-webApp by showing the main layout and configuration options and how its labeling mechanics work. To our knowledge, the EMU-webApp is the first client-side web-based annotation tool that is this feature rich. Being completely web-based not only allows it to be used within the context of the EMU-SDMS but also allows it to connect to any web server that implements the EMU-webApp-websocket-protocol (see Appendix 16 for details). This feature is currently being utilized, for example, by the IPS-EMUprot-nodeWSserver.js server side software package (see https://github.com/IPS-LMU/IPS-EMUprot-nodeWSserver), which allows emuDBs to be served to any number of clients for collaborative annotation efforts. Further, by using the URL Parameters (see Chapter 13 for details) the web application can also be used to display annotation data that is hosted on any web server.20 Because of these features, we feel the EMU-webApp is a valuable contribution to the speech and spoken language software tool landscape. References "],
["chap-emuRpackageDetails.html", "10 emuR - package functions 10.1 Import and conversion routines 10.2 emuDB interaction and configuration routines 10.3 EMU-webApp configuration routines 10.4 Data extraction routines 10.5 Central objects 10.6 Export routines 10.7 Conclusion", " 10 emuR - package functions This chapter gives an overview of the essential functions and central objects provided by the emuR package. It is not meant as a comprehensive list of every function and object provided by emuR, but rather tries to group the essential functions into meaningful categories for easier navigation. The categories presented in this chapter are: Import and conversion routines (Section 10.1), emuDB interaction and configuration routines (Section 10.2), EMU-webApp configuration routines (Section 10.3), Data extraction routines (Section 10.4), Central objects in emuR (Section 10.5), and Export routines (Section 10.6). If a comprehensive list of every function and object provided by the emuR package is required, R’s help() function (see R code snippet below) can be used. help(package=&quot;emuR&quot;) 10.1 Import and conversion routines As most people that are starting to use the EMU-SDMS will probably already have some form of annotated data, we will first show how to convert existing data to the emuDB format. For a guide to creating an emuDB from scratch and for information about this format see Chapter 5. 10.1.1 Legacy EMU databases For people transitioning to emuR from the legacy EMU system, emuR provides a function for converting existing legacy EMU databases to the new emuDB format. The R code snippet below shows how to convert a legacy database that is part of the demo data provided by the emuR package. # load the package library(emuR) # create demo data in directory provided by the tempdir() function create_emuRdemoData(dir = tempdir()) # get the path to a .tpl file of # a legacy EMU database that is part of the demo data tplPath = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;legacy_ae&quot;, &quot;ae.tpl&quot;) # convert this legacy EMU database to the emuDB format convert_legacyEmuDB(emuTplPath = tplPath, targetDir = tempdir()) This will create a new emuDB in a temporary directory, provided by R’s tempdir() function, containing all the information specified in the .tpl file. The name of the new emuDB is the same as the basename of the .tpl file from which it was generated. In other words, if the template file of the legacy EMU database has path A and the directory to which the converted database is to be written has path B, then convert_legacyEmuDB(emuTplPath = \"A\", targetdir = \"B\") will create an emuDB directory in B from the information stored in A. 10.1.2 TextGrid collections A further function provided is the convert_TextGridCollection() function. This function converts an existing .TextGrid and .wav file collection to the emuDB format. In order to pair the correct files together the .TextGrid files and the .wav files must have the same name (i.e., file name without extension). A further restriction is that the tiers contained within all the .TextGrid files have to be equal in name and type (equal subsets can be chosen using the tierNames argument of the function). For example, if all .TextGrid files contain the tiers Syl: IntervalTier, Phonetic: IntervalTier and Tone: TextTier the conversion will work. However, if a single .TextGrid of the collection has the additional tier Word: IntervalTier the conversion will fail, although it can be made to work by specifying the equal tier subset equalSubset = c('Syl', 'Phonetic', 'Tone') and passing it into the function argument convert\\_TextGridCollection(..., tierNames = equalSubset, ...). The R code snippet below shows how to convert a TextGrid collection to the emuDB format. # get the path to a directory containing # .wav &amp; .TextGrid files that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;TextGrid_collection&quot;) # convert this TextGridCollection to the emuDB format convert_TextGridCollection(path2directory, dbName = &quot;myTGcolDB&quot;, targetDir = tempdir()) The above R code snippet will create a new emuDB in the directory tempdir() called myTGcolDB. The emuDB will contain all the tier information from the .TextGrid files but will not contain hierarchical information, as .TextGrid files do not contain any linking information. It is worth noting that it is possible to semi-automatically generate links between time-bearing levels using the autobuild_linkFromTimes() function. An example of this was given in Chapter 3. The above R code snippet creates a new emuDB in the directory tempdir() called myTGcolDB. The emuDB contains all the tier information from the .TextGrid files no hierarchical information, as .TextGrid files do not contain any linking information. Further, it is possible to semi-automatically generate links between time-bearing levels using the autobuild_linkFromTimes() function. An example of this was given in Chapter 3. 10.1.3 BPF collections Similar to the convert_TextGridCollection() function, the emuR package also provides a function for converting file collections consisting of BPF and .wav files to the emuDB format. The R code snippet below shows how this can be achieved. # get the path to a directory containing # .wav &amp; .par files that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;BPF_collection&quot;) # convert this BPFCollection to the emuDB format convert_BPFCollection(path2directory, dbName = &#39;myBPF-DB&#39;, targetDir = tempdir(), verbose = F) As the BPF format also permits annotation items to be linked to one another, this conversion function can optionally preserve this hierarchical information by specifying the refLevel argument. 10.1.4 txt collections A further conversion routine provided by the emuR package is the convert_txtCollection() function. As with other file collection conversion functions, it converts file pair collections but this time consisting of plain text .txt and .wav files to the emuDB format. Compared to other conversion routines it behaves slightly differently, as unformatted plain text files do not contain any time information. It therefore places all the annotations of a single .txt file into a single timeless annotation item on a level of type ITEM called bundle. # get the path to a directory containing .wav &amp; .par # files that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;txt_collection&quot;) # convert this txtCollection to the emuDB format convert_txtCollection(sourceDir = path2directory, dbName = &quot;txtCol&quot;, targetDir = tempdir(), attributeDefinitionName = &quot;transcription&quot;, verbose = F) Using this conversion routine creates a bare-bone, single route node emuDB which either can be further manually annotated or automatically hierarchically annotated using the runBASwebservice_*21 functions of emuR. It is worth noting that these functions are already part of the emuR package; however, they are still considered to have a beta status which is why they are omitted from this documentation. In future versions of this documentation a section or chapter will be dedicated to using the BAS Webservices (Kisler, Schiel, and Sloetjes 2012) to automatically generate a hierarchical annotation structure for an entire emuDB. 10.2 emuDB interaction and configuration routines This section provides a tabular overview of all the emuDB interaction routines provided by the emuR package and also provides a short description of each function or group of functions. ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Table 10.1: Overview of the emuDB interaction routines provided by emuR. Functions Description add/list/remove_attrDefLabelGroup() Add / list / remove label group to / of / from attributeDefinition of emuDB add/list/remove_labelGroup() Add / list / remove global label group to / of / from emuDB add/list/remove_levelDefinition() Add / list / remove level definition to / of / from emuDB add/list/remove_linkDefinition() Add / list / remove link definition to / of / from emuDB add/list/ remove_ssffTrackDefinition() Add / list / remove SSFF track definition to / of / from emuDB add/list/rename/remove_attributeDefinition() Add / list / rename / remove attribute definition to / of / from emuDB add_files() Add files to emuDB autobuild_linkFromTimes() Autobuild links between two levels using their time information emuDB create_emuDB() Create empty emuDB duplicate_level() Duplicate level import_mediaFiles() Import media files to emuDB list_bundles() List bundles of emuDB list_files() List files of emuDB list_sessions() List sessions of emuDB load_emuDB() Load emuDB replace_itemLabels() Replace item labels set/get/remove_legalLabels() Set / get / remove legal labels of attribute definition of emuDB rename_emuDB() Rename emuDB 10.3 EMU-webApp configuration routines This section provides a tabular overview of all the EMU-webApp configuration routines provided by the emuR package and also provides a short description of each function or group of functions. See Chapter 9 for examples of how to use these functions. Table 10.2: Overview of the EMU-webApp configuration functions provided by emuR. Functions Description add/list/remove_perspective() Add / list / remove perspective to / of / from emuDB set/get_levelCanvasesOrder() Set / get level canvases order for EMU-webApp of emuDB set/get_signalCanvasesOrder() Set / get signal canvases order for EMU-webApp of emuDB It is worth noting that the legal labels configuration of the emuDB configuration will also affect how the EMU-webApp behaves, as it will not permit any other labels to be entered except those defined as legal labels. 10.4 Data extraction routines This section provides a tabular overview of all the data extraction routines provided by the emuR package and also provides a short description of each function or group of functions. See Chapter 6 and Chapter 7 for multiple examples of how the various data extraction routines can be used. Table 10.3: Overview of the data extraction functions provided by emuR. Functions Description query() Query emuDB requery_hier() Requery hierarchical context of a segment list in an emuDB requery_seq() Requery sequential context of segment list in an emuDB get_trackdata() Get trackdata from loaded emuDB An overview of how the various data extraction functions in the emuR package interact is displayed in Figure 10.1. It is an updated version of a figure presented in Harrington (2010) on page 121 that additionally shows the output type of various post-processing functions (e.g., dcut()). Figure 10.1: Relationship between various key functions in emuR and their output. Figure is an updated version of Figure 5.7 in Harrington (2010) on page 121. 10.5 Central objects This section provides a tabular overview of the central objects provided by the emuR package and also provides a short description of each object. See Chapter 6 and 7 for examples of functions returning these objects and how they can be used. Table 10.4: Overview of the central objects of the emuR package. Object Description emuRsegs A emuR segment list is a list of segment descriptions. Each segment descriptions describes a sequence of annotation items. The list is usually a result of an emuDB query using the query() function. trackdata A track data object is the result of get_trackdata() and usually contains the extracted signal data tracks belonging to segments of a segment list. emuRtrackdata A emuR track data object is the result of get_trackdata() if the resultType parameter is set to emuRtrackdata or the result of an explicit call to create_emuRtrackdata. Compared to the trackdata object it is a sub-class of a data.table/data.frame which is meant to ease integration with other packages for further processing. It can be viewed as an amalgamation of an emuRsegs and a trackdata object as it contains the information stored in both objects (see also ?create_emuRtrackdata()). 10.6 Export routines Although associated with data loss, the emuR package provides an export routine to the common TextGrid collection format called export_TextGridCollection(). While exporting is sometimes unavoidable, it is essential that users are aware that exporting to other formats which do not support or only partially support hierarchical annotations structures will lead to the loss of the explicit linking information. Although the autobuild_linkFromTimes() can partially recreate some of the hierarchical structure, it is advised that the export routine be used with extreme caution. The R code snippet below shows how export_TextGridCollection() can be used to export the levels Text, Syllable and Phonetic of the ae demo emuDB to a TextGrid collection. Figure 10.2 show the content of the created msajc003.TextGrid file as displayed by Praat’s \"Draw visible sound and Textgrid...\" procedure. # get the path to &quot;ae&quot; emuDB path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load &quot;ae&quot; emuDB ae = load_emuDB(path2ae) # export the levels &quot;Text&quot;, &quot;Syllable&quot; # and &quot;Phonetic&quot; to a TextGrid collection export_TextGridCollection(ae, targetDir = tempdir(), attributeDefinitionNames = c(&quot;Text&quot;, &quot;Syllable&quot;, &quot;Phonetic&quot;)) Figure 10.2: TextGrid annotation generated by the export_TextGridCollection() function containing the tiers (from top to bottom): Text, Syllable, Phonetic. Depending on user requirements, additional export routines might be added to the emuR in the future. 10.7 Conclusion This chapter provided an overview of the essential functions and central objects, grouped into meaningful categories, provided by the emuR package. It is meant as a quick reference for the user to quickly find functions she or he is interested in. References "],
["chap-querysys-impl.html", "11 Implementation of the query system22 11.1 Query expression parser 11.2 Redundant links", " 11 Implementation of the query system22 Compatibly with other query languages, the EQL defines the user a front-end interface and infers the query’s results from its semantics. However, a query language does not define any data structures or specify how the query engine is to be implemented. As mentioned in Chapter 2, a major user requirement was database portability, simple package installation, tolerable run times over complex queries, and a system that did not rely on external software at runtime. The only available back-end implementation that met those needs and was also available as an R package at the time was (R)SQLite (Hipp and Kennedy (2007), Wickham, James, and Falcon (2014)). As (R)SQLite is a relational database management system, emuR’s query system could not be implemented so as to use directly the primary data sources of an emuDB, that is, the JSON files described in Chapter 5. A syncing mechanism that maps the primary data sources to a relational form for querying purposes had to be implemented. This relational form is referred to as the emuDBcache in the context of an emuDB. The data sources are synchronized while an emuDB is being loaded and when changes are made to the annotation files. To address load time issues, we implemented a file check-sum mechanism which only reloads and synchronizes annotation files that have a changed MD5-sum (Rivest 1992). Figure 11.1 is a schematic representation of how the various emuDB interaction functions interact with either the file representation or the relational cache. Figure 11.1: Schematic architecture of emuDB interaction functions of the emuR package. textcolor{three-color-c2}{Orange} paths show examples of functions interacting with the files of the emuDB, while extcolor{three-color-c1}{green} paths show functions accessing the relational annotation structure. Actions like saving a changed annotation using the EMU-webApp first save the _annot.json to disk then update the relational annotation structure. Despite the disadvantages of cache invalidation problems, there are several advantages to having an object relational mapping between the JSON-based annotation structure of an emuDB and a relation table representation. One is that the user still has full access to the files within the directory structure of the emuDB. This means that external tools can be used to script, manipulate or simply interact with these files. This would not be the case if the files were stored in databases in a way that requires (semi-)advanced programming knowledge that might be beyond the capabilities of many users. Moreover, we can provide expert users with the option of using other relational database engines such as PostgreSQL, including all their performance-tweaking abilities, as their relational cache. This is especially valuable for handling very large speech databases. The relational form of the annotation structure is split into six tables in the relational database to avoid data redundancy. The six tables are: emu_db: containing emuDB information (columns: uuid, name), uuid name 0fc618dc-8980-414d-8c7a-144a649ce199 ae session: containing session information (columns: db_uuid, name), db_uuid name 0fc618dc-8980-414d-8c7a-144a649ce199 0000 bundle: containing bundle information (columns: db_uuid, session, name, annotates, sample_rate, md5\\_annot\\_json), db_uuid session name annotates sample_rate md5_annot_json 0fc618dc-8980-414d-8c7a-144a649ce199 0000 msajc003 msajc003.wav 20000 785c7cdb6d4bd5e8b5cd7c56a5946ddf … … … … … … items: containing all annotation items of emuDB (columns: db_uuid, session, bundle, item_id, level, type, seq_idx, sample_rate, sample_point, sample_start, sample_dur), db_uuid session bundle item_id level type seq_idx sample_rate sample_point sample_start sample_dur 0fc618dc-8980-414d-8c7a-144a649ce199 0000 msajc003 147 Phonetic SEGMENT 1 20000 NA 3749 1389 … … … … … … … … … … … labels: containing all labels belonging to all items (columns: db_uuid, session, bundle, item_id, label_idx, name, label), and db_uuid session bundle item_id label_idx name label 0fc618dc-8980-414d-8c7a-144a649ce199 0000 msajc003 147 1 Phonetic V … … … … … … … links: containing all links between annotation items of emuDB (columns: db_uuid, session, bundle, from_id, to_id, label). db_uuid session bundle from_id to_id label 0fc618dc-8980-414d-8c7a-144a649ce199 0000 msajc003 8 7 NA … … … … 7 NA While performing a query the engine uses an aggregate key to address every annotation item and its labels (db_uuid, session, bundle, item_id) and a similar aggregate key to dereference the links (db_uuid, session, bundle, from_id / to_id) which connect items. As the records in relational tables are not intrinsically ordered a further aggregate key is used to address the annotation item via its index and level (uuid, session, bundle, level / seq_idx). This is used, for example, during sequential queries to provide an ordering of the individual annotation items. It is worth noting that a plethora of other tables are created at query time to store various temporary results of a query. However, these tables are created as temporary tables during the query and are deleted on completion which means they are not permanently stored in the emuDBcache. 11.1 Query expression parser The query engine parses an EQL query expression while simultaneously executing partial query expressions. This ad-hoc string evaluation parsing strategy is different from multiple other query systems which incorporate a query planner stage to pre-parse and optimize the query execution stage (e.g., Hipp and Kennedy (2007), Conway et al. (2016)). Although no pre-optimization can be performed, this strategy simplifies the execution of a query as it follows a constant heuristic evaluation strategy. This section describes this heuristic evaluation and parsing strategy based on the EQL expression [[Syllable == W -&gt; Syllable == W] ^ [Phoneme == @ -&gt; #Phoneme == s]]. The main strategy of the query expression parser is to recursively parse and split an EQL expression into left and right sub-expressions until a so-called Simple Query (SQ term is found and can be executed (see EBNF in Appendix 17 for more information on the elements comprising the EQL). This is done by determining the operator which is the first to be evaluated on the current expression. This operator is determined by the sub-expression grouping provided by the bracketing. Each sub-expression is then considered to be a fully valid EQL expression and once again parsed. Figure 11.2, which is split into seven stages (marked S1-S7), shows the example EQL expression being parsed (S1-S3) and the resulting items being merged to meet the requirements of the individual operator (S4-S6) of the original query. S1 to S3 show the splitting operator character (e.g., textcolor{three_color_c3}{-&gt;} in purple) which splits the expression into a textcolor{three_color_c1}{left} (green) and textcolor{three_color_c2}{right} (orange) sub-expression. Figure 11.2: Example of how the query expression parser parses and evaluates an EQL expression and merges the result according to the respective EQL operators. The result modifier symbol (#) is noteworthy for its extra treatment by the query engine as it places an exact copy of the items marked by it into its own intermediary result storage (see #sitems node on S7 in Figure 11.2). After performing the database operations necessary to do the various merging operation which are performed on the intermediary results, this storage is updated by removing items from it that are no longer present due to the merging operation. As a final step, the query engine evaluates if there are items present in the intermediary result storage created by the presence of the result modifier symbol. If so, these items are used to create an emuRsegs object by deriving the time information and extracting the necessary information from the intermediate result storage. If no items are present in the result modifier storage, the query engine uses the items provided by the final merging procedure in S3 instead (which is not the case in the example used in Figure 11.2). A detailed description of how this query expression parser functions is presented in a pseudo code representation in Algorithms ?? and ??23. For simplicity, this representation ignores the treatment of the result modifier symbol (#) and focuses on the parsing and evaluation strategy of the query expression parser. As stated previously, the presence of the result modifier before an SQ triggers the query engine to place a copy of the result of that SQ into an additional result table, which is then updated throughout the rest of the query. The starting point for every query is the query() function (see line ?? in Algorithm ??). This function places the filtered items, links and labels entries that are relevant for the current query into temporary tables. Depending on which query terms and operators are found, the EQL query engine uses the various sub-routines displayed in Algorithms ?? and ?? to parse and evaluate the EQL expression. Figure 11.3: Pseudo Code for Query Engine Algorithm - Part 1 Figure 11.4: Pseudo Code for Query Engine Algorithm - Part 2 11.2 Redundant links A noteworthy difference between the legacy and the new EMU system is how hierarchies are stored. The legacy system stored the linking information of a hierarchy in so-called hierarchical label files, which were plain text files that used the .hlb extensions. Within the label files this information was stored in space/blank separated lines: \\textcolor{three_color_c1}{111} \\textcolor{three_color_c2}{139 140 141 173 174 175 185} \\textcolor{three_color_c1}{112} \\textcolor{three_color_c2}{142 143 176 177} \\textcolor{three_color_c1}{113} \\textcolor{three_color_c2}{144 145 146 178 179 180} \\textcolor{three_color_c1}{114} \\textcolor{three_color_c2}{147} \\textcolor{three_color_c1}{115} \\textcolor{three_color_c2}{148} \\textcolor{three_color_c1}{116} \\textcolor{three_color_c2}{149}, where the textcolor{three_color_c1}{first number} (green) of each line was the parent’s ID and the textcolor{three_color_c2}{following numbers} (orange) indicated the annotation items the parent was linked to. However, it was not just links to the items on the child level that were stored in each line. Rather, a link to all children of all levels below the parent level was stored for each parent item. This was likely due to performance benefits in parsing and mapping onto the internal structures used by the legacy query engine. A schematic representation of this form of linking is displayed in Figure 11.5A. As these redundant links are prone to errors while updating the data model and lead to a convoluted annotation structure models (see excessive use of dashed lines in Figure 11.5A), we chose to eliminate them and opted for the cleaner, non-redundant representation displayed in Figure 11.5B. Although this led to a more complex query parser engine for hierarchical queries and functions, we feel it is a cleaner, more accurate and more robust data representation. Figure 11.5: Schematic of hierarchy graph ae; extbf{A}: legacy redundant strategy vs. extbf{A}: cleaner non-redundant strategy. References "],
["chap-wrassp-impl.html", "12 wrassp implementation 12.1 The libassp port", " 12 wrassp implementation The libassp was originally written by Michel Scheffers as a C library which could be linked against or compiled into separate executable signal processing command line tools. To extend the legacy EMU system, the libassp it was integrated into it by using the Tcl Extension Architecture (TEA) to create a native extension to the Tcl programming language. The bulk of this work was done by Lasse Bombien in collaboration with Michel Scheffers. Lasse Bombien also implemented the tkassp user interface module as part of the legacy EMU system to allow the user full access to the functionality of the libassp from a GUI. The wrassp R package was written by Lasse Bombien and Raphael Winkelmann based on a similar approach as the tclassp port using the TEA. Since the libassp was put under the GPL version 3 (see https://www.gnu.org/licenses/gpl-3.0.en.html) by Michel Scheffers, the wrassp also carries this license. 12.1 The libassp port Here, we briefly describe our strategy for porting the libassp to R. The port of the libassp to the R eco-system was achieved using the foreign language interface provided by the R system as is described in the R Extensions manual (see https://cran.r-project.org/doc/manuals/r-release/R-exts.htmlWriting). To port the various signal processing routines provided by the libassp and to avoid code redundancy a single C function called performAssp() was created. This function acts as a C wrapper function interface to libassp’s internal functions and handles the data conversion between libassp’s internal and R’s data structures. However, to provide the user with a clear and concise API we chose to implement separate R functions for every signal processing function. This also allowed us to formulate more concise manual entries for each of the signal processing function provided by wrassp. The R code snippet below is a pseudo-code example of the layout of each signal processing function wrassp provides. ##&#39; roxygen2 documentation for genericWrasspFun genericWrasspSigProcFun = function(listOfFiles, ..., forceToLog = useWrasspLogger){ ########################### # perform parameter checks if (is.null(listOfFiles)) { stop(paste(&quot;listOfFiles is NULL! ...&quot;)) } # ... # call performAssp externalRes = invisible(.External(&quot;performAssp&quot;, listOfFiles, fname = &quot;forest&quot;, ...)) ############################ # write options to options log file if (forceToLog){ optionsGivenAsArgs = as.list(match.call( expand.dots = TRUE)) wrassp.logger(optionsGivenAsArgs[[1]], optionsGivenAsArgs[-1], optLogFilePath, listOfFiles) } return(externalRes) } To provide access to the file handling capabilities of the libassp, we implemented two C interface functions called getDObj2() (where 2 is simply used as a function version marker) and writeDObj(). These functions use libassp’s asspFOpen(), asspFFill(), asspFWrite() and asspFClose() function to read and write files supported by the libassp from and to files on disk into R. The public API functions read.AsspDataObj() and write.AsspDataObj() are the R wrapper functions around getDObj2() and writeDObj(). To be able to access some of libassp’s internal variables further wrapper functions were implemented. It was necessary to have access to these variables to be able to perform adequate parameter checks in various functions. The R code snippet below shows these functions. # load the wrassp package library(wrassp) # show AsspWindowTypes AsspWindowTypes() ## [1] &quot;RECTANGLE&quot; &quot;TRIANGLE&quot; &quot;PARABOLA&quot; &quot;COS&quot; &quot;HANN&quot; ## [6] &quot;COS_3&quot; &quot;COS_4&quot; &quot;HAMMING&quot; &quot;BLACKMAN&quot; &quot;BLACK_X&quot; ## [11] &quot;BLACK_3&quot; &quot;BLACK_M3&quot; &quot;BLACK_4&quot; &quot;BLACK_M4&quot; &quot;NUTTAL_3&quot; ## [16] &quot;NUTTAL_4&quot; &quot;GAUSS2_5&quot; &quot;GAUSS3_0&quot; &quot;GAUSS3_5&quot; &quot;KAISER2_0&quot; ## [21] &quot;KAISER2_5&quot; &quot;KAISER3_0&quot; &quot;KAISER3_5&quot; &quot;KAISER4_0&quot; # show wrasspOutputInfos AsspLpTypes() ## [1] &quot;ARF&quot; &quot;LAR&quot; &quot;LPC&quot; &quot;RFC&quot; # show wrasspOutputInfos AsspSpectTypes() ## [1] &quot;DFT&quot; &quot;LPS&quot; &quot;CSS&quot; &quot;CEP&quot; The wrassp package provides two R objects that contain useful information regarding the supported file format types (AsspFileFormats) and the output created by the various signal processing functions. The R code snippet below shows the content of these two objects. # show AsspFileFormats AsspFileFormats ## RAW ASP_A ASP_B XASSP IPDS_M IPDS_S AIFF AIFC CSL ## 1 2 3 4 5 6 7 8 9 ## CSRE ESPS ILS KTH SWELL SNACK SFS SND AU ## 10 11 12 13 13 13 14 15 15 ## NIST SPHERE PRAAT_S PRAAT_L PRAAT_B SSFF WAVE WAVE_X XLABEL ## 16 16 17 18 19 20 21 22 24 ## YORK UWM ## 25 26 # show first element of wrasspOutputInfos wrasspOutputInfos[[1]] ## $ext ## [1] &quot;acf&quot; ## ## $tracks ## [1] &quot;acf&quot; ## ## $outputType ## [1] &quot;SSFF&quot; As a final remark, it is worth noting that porting the C library libassp to R enables the functions provided by the wrassp package to run at near native speeds on every platform supported by R and avoids almost any interpreter overhead. "],
["chap-emu-webAppImplementation.html", "13 EMU-webApp implementation 13.1 Communication protocol24 13.2 URL parameters", " 13 EMU-webApp implementation Here, we briefly describe our strategy for implementing the EMU-webApp. The EMU-webApp is written entirely in HTML, Javascript and CSS. To ease testing and to enable easy integration and extendability we chose to use the AngularJS Javascript framework (Google 2014). Most of the components of the EMU-webApp (e.g., the spectrogram display) are implemented as so-called Angular directives. This means that, apart from dependencies on data service classes that have to be made available, these components are reusable and can be integrated into other web applications. The EMU-webApp makes extensive use of Angular data bindings to keep the display and the various data services in sync with each other. It is also worth noting that we chose to use the SASS (see http://sass-lang.com/) preprocessor to compile .sass files to CSS. This enabled us to use things like mixins, variables and inheritance for a more concise stylesheet management and generation. The main reason we chose the JSON file format as the main file type for the EMU-SDMS is because we wanted a web application as the main GUI of the new system. Using JSON files enables the EMU-webApp to directly use the annotation and configuration files that are part of an emuDB without manipulating or reformatting the data. The rest of this chapter will focus on the communication protocol and the URL parameters provided by the EMU-webApp. These should be of special interest to developers as they describe how to communicate with the web application and how to use the web application to display data that is hosted on other http web servers. 13.1 Communication protocol24 A large benefit gained by choosing the browser as the user interface is the ability to easily interact with a server using standard web protocols, such as http, https or websockets. In order to standardize the data exchange with a server, we have developed a simple request-response communication protocol on top of the websocket standard. This decision was strongly guided by the availability of the httpuv R package (RStudio and Inc. 2015). Our protocol defines a set of JSON objects for both the requests and responses. A subset of the request-response actions, most of them triggered by the client after connection, are displayed in Table ??. Protocol_Command Comments GETPROTOCOL Check if the server implements the correct protocol GETDOUSERMANAGEMENT See if the server handles user management (if yes, then this prompts a login dialog $ ightarrow$ LOGONUSER GETGLOBALDBCONFIG Request the configuration file for the current connection GETBUNDLELIST Request the list of available bundles for current connection GETBUNDLE Request data belonging to a specific bundle name SAVEBUNDLE Save data belonging to a specific bundle name This protocol definition makes collaborative annotation efforts possible, as developers can easily implement servers for communicating with the EMU-webApp. Using this protocol allows a database to be hosted by a single server anywhere on the globe that then can be made available to a theoretically infinite number of users working on separate accounts logging individual annotations, time and date of changes and other activities such as comments added to problematic cases. Tasks can be allocated to and unlocked for each individual user by the project leader. As such, user management in collaborative projects is substantially simplified and trackable compared with other currently available software for annotation. The emuR package implements this websocket protocol as part of the serve() function utilizing the httpuv package. Further example implementations of this websocket protocol are provided as part of the source code repository of the EMU-webApp (see https://github.com/IPS-LMU/EMU-webApp/tree/master/exampleServers). A in-depth description of the protocol which includes descriptions of each request and response JSON object can be found in Appendix 16. 13.2 URL parameters The EMU-webApp currently implements several URL parameters (see https://en.wikipedia.org/wiki/Query_string for more information) as part of its URL query string. This section describes the currently implemented parameters and gives some accompanying examples. 13.2.1 Websocket server parameters The current URL parameters that affect the websocket server connection are: serverUrl=URL is a URL pointing to a websocket server that implements the EMU-webApp websocket protocol, and autoConnect=true / false automatically connects to a websocket server URL specified in the serverUrl parameter. If the serverUrl parameter is not set the web application defaults to the entry in its default_emuwebappConfig.json. 13.2.2 Examples auto connect to local wsServer: http://ips-lmu.github.io/EMU-webApp/?autoConnect=true&amp;serverUrl=ws:%2F%2Flocalhost:17890 13.2.3 Label file preview parameters The current URL parameters for using the EMU-webApp to visualize files that are hosted on other http servers are: audioGetUrl=URL GET URL that will respond with .wav file, labelGetUrl=URL GET URL that will respond with label/annotation file, DBconfigGetURL=URL GET URL that will respond with _DBconfig.json file, and labelType=TEXTGRID / annotJSON specifies the type of annotation file. This mechanism is, for example, currently being used by the WebMAUS web-services of the BASWebServices (see https://clarin.phonetik.uni-muenchen.de/BASWebServices) to provide a preview of the automatically segmented speech files. 13.2.4 Examples TextGrid example: http://ips-lmu.github.io/EMU-webApp/?audioGetUrl=https://raw.githubusercontent.com/IPS-LMU/EMU-webApp/master/app/testData/oldFormat/msajc003/msajc003.wav&amp;labelGetUrl=https://raw.githubusercontent.com/IPS-LMU/EMU-webApp/master/app/testData/oldFormat/msajc003/msajc003.TextGrid&amp;labelType=TEXTGRID annotJSON example: http://ips-lmu.github.io/EMU-webApp/?audioGetUrl=https://raw.githubusercontent.com/IPS-LMU/EMU-webApp/master/app/testData/newFormat/ae/0000_ses/msajc003_bndl/msajc003.wav&amp;labelGetUrl=https://raw.githubusercontent.com/IPS-LMU/EMU-webApp/master/app/testData/newFormat/ae/0000_ses/msajc003_bndl/msajc003_annot.json&amp;labelType=annotJSON References "],
["app-chap-useCases.html", "14 Use cases 14.1 What is the average length of all n phonetic segments in the ae emuDB? 14.2 What does the F1 and F2 distribution of all phonetic segments that contain the labels I, o:, u:, V or @ look like? 14.3 What words do the phonetic segments that carry the labels s, z, S or Z in the ae emuDB occur in and what is their phonetic context? 14.4 Do the phonetic segments labeled s, z, S or Z in the ae emuDB differ with respect to their first spectral moment?\\footnote{The original version of this use case was written by Florian Schiel as part of the emuR_intro vignette that is part of the emuR package.", " 14 Use cases To add to the tutorial of Chapter 3, this chapter will present a few short use cases extracted and updated from the emuR_intro vignette. These use cases are meant as practical guides to answering research questions and are to be viewed as generic template procedures that can be altered and applied to similar research questions. They are meant to give practical examples of what it is like working with the EMU-SDMS to answer research questions common in speech and spoken language research. Every use case will start off by asking a question about the ae demo database and will continue by walking through the process of answering this question by using the mechanics the emuR package provides. The four questions this chapter will address are: Section 14.1: What is the average length of all n phonetic segments in the ae emuDB? Section 14.2: What does the F1 and F2 distribution of all phonetic segments that contain the labels I, o:, u:, V or @ look like? Section 14.3: What words do the phonetic segments that carry the labels s, z, S or Z in the ae emuDB occur in and what is their phonetic context? Section 14.4: Do the phonetic segments that carry the labels s, z, S or Z in the ae emuDB differ with respect to their first spectral moment? \\end{itemize} The R code snippet below shows how the emuR demo data used in this chapter is created. # load the package library(emuR) # create demo data in directory provided by the tempdir() function create_emuRdemoData(dir = tempdir()) # get the path to emuDB called &#39;ae&#39; that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load emuDB into current R session ae = load_emuDB(path2directory) 14.1 What is the average length of all n phonetic segments in the ae emuDB? The first thing that has to be done to address this fairly simple question is to query the database for all n segments. This can be achieved using the query() function as shown in the R code snippet below. # query segments sl = query(ae, query = &quot;Phonetic == n&quot;) # show first row of sl head(sl, n = 1) ## segment list from database: ae ## query was: Phonetic == n ## labels start end session bundle level type ## 1 n 1031.925 1195.925 0000 msajc003 Phonetic SEGMENT The second argument of the query() contains a string that represents an EQL statement. This fairly simple EQL statement consists of ==, which is the equality operator of the EQL, and on the right hand side of the operator the label n that we are looking for. The query() function returns an object of the class emuRsegs that is a superclass of the well known data.frame. The various columns of this object should be fairly self-explanatory: labels displays the extracted labels, start and end are the start time and end times in milliseconds of each segment and so on. We can now use the information in this object to calculate the mean durations of these segments as shown in the R code snippet below. # calculate durations d = dur(sl) # calculate mean mean(d) ## [1] 67.05833 14.2 What does the F1 and F2 distribution of all phonetic segments that contain the labels I, o:, u:, V or @ look like? Once again we will initially query the emuDB to retrieve the segments we are interested in as shown in the R code snippet below. # query emuDB sl = query(ae, query = &quot;Phonetic == I|o:|u:|V|@&quot;) Now that the necessary segment information has been extracted, the get\\_trackdata() function can be used to calculate the formant values for these segments as displayed in the R code snippet below. # get formant values for these segments td = get_trackdata(ae, sl, onTheFlyFunctionName = &quot;forest&quot;, resultType = &quot;emuRtrackdata&quot;) In this example, the get_trackdata() function uses a formant estimation function called forest() to calculate the formant values in real time. This signal processing function is part of the wrassp package, which is used by the emuR package to perform signal processing duties with the get_trackdata() command (see Chapter 8 for details). If the resultType parameter is set to emuRtrackdata in the call to get\\_trackdata(), an object of the class emuRtrackdata is returned. The class vector of the td object is displayed in the R code snippet below. # show class vector of td class(td) ## [1] &quot;emuRtrackdata&quot; &quot;data.frame&quot; # show first line of td head(td, n = 1) ## sl_rowIdx labels start end session bundle level type ## 1 1 V 187.425 256.925 0000 msajc003 Phonetic SEGMENT ## times_orig times_rel times_norm T1 T2 T3 T4 ## 1 187.5 0 0 0 1293 2424 3429 As the emuRtrackdata class is a superclass to the common data.table and data.frame classes, packages like ggplot2 can be used to visualize our F1 and F2 distribution as shown in the R code snippet below (see Figure 14.1 for the resulting plot). # load package library(ggplot2) # scatter plot of F1 and F2 values using ggplot ggplot(td, aes(x=T2, y=T1, label=td$labels)) + geom_text(aes(colour=factor(labels))) + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2(Hz)&quot;, y = &quot;F1(Hz)&quot;) + guides(colour=FALSE) Figure 14.1: F1 by F2 distribution for I, o:, u:, V and *(???). 14.3 What words do the phonetic segments that carry the labels s, z, S or Z in the ae emuDB occur in and what is their phonetic context? As with the previous use cases, the initial step is to query the database to extract the relevant segments as shown in the R code snippet below. # query segments sibil = query(ae,&quot;Phonetic==s|z|S|Z&quot;) # show first element of sibil head(sibil, n = 1) ## segment list from database: ae ## query was: Phonetic==s|z|S|Z ## labels start end session bundle level type ## 1 s 483.425 566.925 0000 msajc003 Phonetic SEGMENT The requery_hier() function can now be used to perform a hierarchical requery using the set resulting from the initial query. This requery follows the hierarchical links of the annotations in the database to find the linked annotation items on a different level. The R code snippet below shows how this can achieved. # perform requery words = requery_hier(ae, sibil, level = &quot;Word&quot;) # show first element of words head(words, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 C 187.425 674.175 0000 msajc003 Word ITEM As seen in the above R code snippet, the result is not quite what one would expect as it does not contain the orthographic word transcriptions but a classification of the words into content words (C) and function words (F). Calling the summary() function on the emuDBhandle object ae would show that the Words level has multiple attribute definitions indicating that each annotation item in the Words level has multiple parallel labels defined for it. The R code snippet below shows an additional requery that queries the Text attribute definition instead. # perform requery words = requery_hier(ae, sibil, level = &quot;Text&quot;) # show first element of words head(words, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 amongst 187.425 674.175 0000 msajc003 Text ITEM As seen in the above R code snippet, the first segment in sibil occurred in the word amongst, which starts at 187.475 ms and ends at 674.225 ms. It is worth noting that this two-step querying procedure (query() followed by requery_hier()) can also be completed in a single hierarchical query using the dominance operator (^). As we have answered the first part of the question, the R code snippet below will extract the context to the left of the extracted sibilants by using the requery_seq() function. # get left context by off setting the # annotation items in sibil one unit to the left leftContext = requery_seq(ae, sibil, offset = -1) # show first element of leftContext head(leftContext, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 N 426.675 483.425 0000 msajc003 Phonetic SEGMENT The R code snippet below attempts to extract the right context in the same manner as above R code snippet, but in this case we encounter a problem. # get right context by off-setting the # annotation items in sibil one unit to the right rightContext = requery_seq(ae, sibil, offset = 1) ## Error in requery_seq(ae, sibil, offset = 1): 4 of the requested sequence(s) is/are out of boundaries. ## Set parameter &#39;ignoreOutOfBounds=TRUE&#39; to get residual result segments that lie within the bounds. As can be seen by the error message in the above R code snippet, four of the sibilants occur at the very end of the recording and therefore have no phonetic post-context. The remaining post-contexts can be retrieved by setting the ignoreOutOfBounds argument to TRUE as displayed in the R code snippet below. rightContext = requery_seq(ae, sibil, offset = 1, ignoreOutOfBounds = TRUE) ## Warning in requery_seq(ae, sibil, offset = 1, ignoreOutOfBounds = TRUE): ## Found missing items in resulting segment list! Replacing missing rows with ## NA values. # show first element of rightContext head(rightContext, n = 1) ## segment list from database: ae ## query was: FROM REQUERY ## labels start end session bundle level type ## 1 t 566.925 596.675 0000 msajc003 Phonetic SEGMENT However, the resulting rightContext and the original sibil objects are not aligned any more. It is therefore dangerous to use this option by default, as one often relies on the rows in multiple emuRsegs objects that were created from each other by using either requery_hier() or requery_seq() to be aligned with each other (i.e., that the same row index implicitly indicates a relationship). 14.4 Do the phonetic segments labeled s, z, S or Z in the ae emuDB differ with respect to their first spectral moment?\\footnote{The original version of this use case was written by Florian Schiel as part of the emuR_intro vignette that is part of the emuR package. Once again, the segments of interest are queried first. The R code snippet below shows how this can be achieved, this time using the new regular expression operand of the EQL (see Chapter 6 for details). sibil = query(ae,&quot;Phonetic =~ &#39;[szSZ]&#39;&quot;) The R code snippet below shows how the get_trackdata() function can be used to calculate the Discrete Fourier Transform values for the extracted segments. dftTd = get_trackdata(ae, seglist = sibil, onTheFlyFunctionName = &#39;dftSpectrum&#39;) As the resultType parameter was not explicitly set, an object of the class trackdata is returned. This object, just like an object of the class emuRtrackdata, contains the extracted trackdata information. Compared to the emuRtrackdata class, however, the object is not “flat” and in the form of a data.table or data.frame but has a more nested structure (see ?trackdata for more details). Since we want to analyze sibilant spectral data we will now reduce the spectral range of the data to 1000 - 10000 Hz. This is due to the fact that there is a lot of unwanted noise in the lower bands that is irrelevant for the problem at hand and can even skew the end results. To achieve this we can use a property of a trackdata object that also carries the class spectral, which means that it is indexed using frequencies. The R code snippet below shows how to use this feature to extract the relevant spectral frequencies of the trackdata object. dftTdRelFreq = dftTd[, 1000:10000] The R code snippet below shows how the fapply() function can be used to apply the moments() function to all elements of dftTdRelFreq. dftTdRelFreqMom = fapply(dftTdRelFreq, moments, minval = T) The resulting dftTdRelFreqMom object is once again a trackdata object of the same length as the dftTdRelFreq trackdata object. It contains the first four spectral moments as shown in the R code snippet below. # show first row of data belonging # to first element of dftTdRelFreqMom dftTdRelFreqMom[1]$data[1,] ## [1] 5.335375e+03 6.469573e+06 6.097490e-02 -1.103308e+00 The information stored in the dftTdRelFreqMom and sibil objects can now be used to plot a time-normalized version of the first spectral moment trajectories, color coded by sibilant class, using emuR’s dplot() function. The R code snippet below shows the R code that produces Figure 14.2. dplot(dftTdRelFreqMom[, 1], sibil$labels, normalise = TRUE, xlab = &quot;Normalized Time [%]&quot;, ylab = &quot;1st spectral moment [Hz]&quot;) Figure 14.2: Time-normalized first spectral moment trajectories color coded by sibilant class. As one might expect, the first spectral moment (the center of gravity) is significantly lower for postalveolar S and Z (green and blue lines) than for alveolar s and z (black and red lines). The R code snippet below shows how to create an alternative plot (see Figure 14.3) that averages the trajectories into ensemble averages per sibilant class by setting the average parameter of dplot() to TRUE. dplot(dftTdRelFreqMom[,1], sibil$labels, normalise = TRUE, average = TRUE, xlab = &quot;Normalized Time [%]&quot;, ylab = &quot;1st spectral moment [Hz]&quot;) Figure 14.3: Time-normalized first spectral moment ensemble average trajectories per sibilant class. As can be seen from the previous two plots (Figure 14.2 and 14.3), transitions to and from a sort of steady state around the temporal midpoint of the sibilants are clearly visible. To focus on this steady state part of the sibilant we will now extract those spectral moments that fall between the proportional timepoints 0.2 and 0.8 of each segment (i.e., the central 60%) using the dcut() function as is shown in the R code snippet below. # cut out the middle 60% portion dftTdRelFreqMomMid = dcut(dftTdRelFreqMom, left.time = 0.2, right.time = 0.8, prop = T) Finally, the R code snippet below shows how to calculate the averages of these trajectories using the trapply() function. meanFirstMoments = trapply(dftTdRelFreqMomMid[,1], fun = mean, simplify = T) As the resulting meanFirstMoments vector has the same length as the initial sibil segment list, we can now easily visualize these values in the form of a boxplot. The R code below shows the R code that produces Figure 14.4. boxplot(meanFirstMoments ~ sibil$labels, xlab = &quot;Sibilant class labels&quot;, ylab = &quot;First spectral moment values [Hz]&quot;) Figure 14.4: Boxplots of the first spectral moments grouped by their sibilant class. As final remark, it is worth noting that using the emuRtrackdata resultType (not the trackdata resultType) of get_trackdata() function we could have performed a comparable analysis by utilizing packages such as dplyr for data.table or data.frame manipulation and lattice or ggplot2 for data visualisation. "],
["app-chap-fileFormats.html", "15 File Formats 15.1 File descriptions 15.2 Example files", " 15 File Formats 15.1 File descriptions 15.1.1 _DBconfig.json The _DBconfig.json file contains the configuration options of the database. People familiar with the legacy EMU system will recognize this as the replacement file for the legacy template (.tpl) file. By convention, variables or strings written entirely in capital letters indicate a constant variable that usually has a special meaning. This is also the case with strings like this found in the _DBconfig.json (\"STRING\", \"ITEM\" ,\"SEGMENT\", \"EVENT\", \"OSCI\", … ). The _DBconfig.json file contains the following fields: \"name\" specifies the name of the database \"UUID\" a unique ID given to each database \"mediafileExtension\" the main mediafileExtension (currently only uncompressed .wav files are supported in every component of the EMU system. This is also the recommended audio format for the emu-sdms.) \"ssffTrackDefinitions\" an array of definitions defining the SSFF tracks of the database. Each ssffTrackDefinition consists of: \"name\" the name of the ssffTrackDefinition \"columnName\" the name of the column of the matching SSFF file. For more information on the columns the various functions of the wrassp produce, see the track fields of the wrasspOutputInfos object that is part of the wrassp package. Further, although the SSFF is a binary file format, it has a plain text header. This means that if you open a SSFF file in the text editor of your choice, you will be able to see the columns contained within it. Another way of accessing column information about a specific SSFF file is to use the wrassp function res = read.AsspDataObj(/path/2/SSFF/file) to read the file from the file system. names(res) will then give you the names of the columns present in this file. In the context of the SSFF, we use the term “column”, while in the context of the EMU system we use either “track” or “SSFF track”. Both refer to the same data. \"fileExtention\" the file extension of the matching SSFF file. See also ?wrasspOutputInfos for the default extensions produced by the wrassp functions. \"levelDefinitions\" array of definitions defining the levels of the database. Each \"levelDefinitions\" consists of: \"name\" The name of the levelDefinition. \"type\" Specifies the type of level (either \"ITEM\" | \"EVENT\" | \"SEGMENT\"). \"attributeDefinitions\" an array of definitions defining the attributes of the level. Each attributeDefinition consists of: \"name\" The name of the \"attributeDefinition\". \"type\" Specifies the type of the attribute (currently only \"STRING\" permitted) \"labelGroups\" An (optional) array containing label group definitions. These can be used as a shorthand notation for querying certain groups of labels and comprise the following: \"name\" The name of the label group. This will be the value used in a query to refer to this group. \"values\" An array of strings representing the labels. \"legalLabels\" An (optional) array of strings specifying which labels are valid or legal for this attribute definition. \"anagestConfig\" If specified (optional), this will convert the level into a special type of level for labeling articulatory data. This will also serve as a marker for the EMU-webApp to treat this level differently. This optional field may only be set for levels of the type \"EVENT\". \"verticalPosSsffTrackName\" The name of the SSFF track containing the vertical position data. \"velocitySsffTrackName\" The name of the SSFF track containing the velocity data. \"autoLinkLevelName\" The name of the level to which created events will be linked. \"multiplicationFactor\" The factor to multiply with (either -1 1). \"threshold\" A value between 0 and 1 defining the absolute threshold. \"gestureOnOffsetLabels\" An array containing two strings that specify the on- and offset labels. \"maxVelocityOnOffsetLabels\" An array containing two strings that specify the on- and offset labels. \"constrictionPlateauBeginEndLabels\" An array containing two strings that specify the begin- and end labels. \"maxConstrictionLabel\" A string specifying the maximum constriction label. \"linkDefinitions\" An array of the definitions defining the links between levels of the database. The combination of all link definitions specifies the hierarchy of the database. Each linkDefinition consists of: \"type\" Specifies the type of link (either \"ONE_TO_MANY\" \"MANY_TO_MANY\" \"ONE_TO_ONE\"). \"superlevelName\" Specifies the name of the super-level. \"sublevelName\" Specifies the name of the sub-level. \"labelGroups\" An (optional) array containing label group definitions. These can be used as a shorthand notation for querying certain groups of labels. Compared to the \"labelGroups\", which can be defined within an attributeDefinition, the labelGroups defined here are globally defined for the entire database as follows: \"name\" The name of the label group. \"values\" An array of strings containing labels. \"EMUwebAppConfig\" Specifies the configuration options intended for the EMU-webApp (i.e., how the database is to be displayed). This field can contain all the configurations options that are specified in the EMU-webApp’s configuration schema (see https://github.com/IPS-LMU/EMU-webApp/tree/master/dist/schemaFiles). The \"EMUwebAppConfig\" contains the following fields: \"main\" Main behavior options: \"autoConnect\": Auto connect to the \"serverUrl\" on the initial load of the webApp to automatically load a database (mainly used for development). \"serverUrl\": The default server URL that is displayed in the connect modal window (and used if \"autoConnect\" is set to true). The default: \"ws://localhost:17890\" points to the server started by the serve() function of the emuR package. \"serverTimeoutInterval\": The maximum amount of time the EMU-webApp waits (in milliseconds) for the server to respond. \"comMode\": Specifies the communication mode the EMU-webApp is in. Currently the only option that is available is \"WS\" (websocket). \"catchMouseForKeyBinding\": Check if mouse has to be in labeler for key bindings to work. \"keyMappings\" Keyboard shortcut definitions. For the sake of brevity, not every key-code is shown (see schema: https://github.com/IPS-LMU/EMU-webApp/blob/master/dist/schemaFiles/emuwebappConfigSchema.json for extensive list). \"toggleSideBarLeft\" integer value that represents the key-code that toggles the left side bar (== bundleList side bar) \"toggleSideBarRight\" integer value that represents the key-code that toggles the right side bar (== perspective side bar) … \"spectrogramSettings\" Specifies the default settings of the spectrogram. The possible settings are: \"windowSizeInSecs\" Specifies the window size in seconds. \"rangeFrom\" Specifies the lowest frequency (in Hz) that will be displayed by the spectrogram. \"rangeTo\" Specifies the highest frequency (in Hz) that will be displayed by the spectrogram. \"dynamicRange\" Specifies the dynamic rang for maximum decibel dynamic range. \"window\" Specifies the window type (\"BARTLETT\" \"BARTLETTHANN\" \"BLACKMAN\" \"COSINE\" \"GAUSS\" \"HAMMING\" \"HANN\" \"LANCZOS\" \"RECTANGULAR\" \"TRIANGULAR\"). \"preEmphasisFilterFactor\" Specifies the preemphasis factor (in formula: s’(k) = s(k) - preEmphasisFilterFactor * s(k-1) ). \"transparency\" Specifies the transparency of the spectrogram (integer from 0 to 255). \"drawHeatMapColors\" (optional) Defines whether the spectrogram should be drawn using heat-map colors (either true or false) \"heatMapColorAnchors\" (optional) Specifies the heat-map color anchors (array of the form [[255, 0, 0], [0, 255, 0], [0, 0, 255]]) \"perspectives\" An array containing perspective configurations. Each \"perspective\" consists of: \"name\" Name of the perspective. \"signalCanvases\" Configuration options for the signalCanvases. \"order\" An array specifying the order in which the ssff tracks are to be displayed. Note that the ssff track names “OSCI” and “SPEC” are always available in addition to the ssff track defined in the database. \"assign\" An array of configuration options that assign one ssff track to another, effectively creating a visual overlay of one track over another. Each array element consists of: \"signalCanvasName\" The name of the signal specified in the \"order\" array. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"ssffTrackName\" The name of the SSFF track to overlay onto \"signalCanvasName\". (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"minMaxValLims\" An array of configuration options to limit the y-axis range that is displayed for a specified SSFF track. \"ssffTrackName\": A name specifying which ssffTrack should be limited. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"minVal\": The minimum value which defines the lower y-axis limit. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"horizontalLines\": Add horizontal lines to ssffTrack \"ssffTrackName\": The name of the SSFF track to draw horizontal lines on. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"yValues\": array of number specifying the y values of the horizontal lines (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"contourLims\" An array containing contour limit values that specify an index range that is to be displayed. As a track or column can contain multi-dimensional data (e.g. four formant values per time stamp, 256 DFT values per time stamp, etc.) it is possible to specify an index range that specifies which values should be displayed (e.g., display formant 2 through 4). \"ssffTrackName\" A name specifying which ssffTrack should be limited. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"minContourIdx\" The minimum contour index to display (starts at index 0). (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"maxContourIdx\" The maximum contour index to display. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"contourColors\" An array to specify colors of individual contours. This overrides the default of automatically calculating distinct colors for each contour. \"ssffTrackName\" The name of the ssffTrackName for which colors are defined. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"colors\" An array of RGB strings (e.g. [\"rgb(238,130,238)\", \"rgb(127,255,212)\"]) that specify the color of the contour (first value = first contour color and so on). (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"levelCanvases\" Configuration options for the levelCanvases: \"order\" An array specifying the order in which the levels are to be displayed. Note that only levels of the type EVENT or SEGMENT can be displayed as levelCanvases. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"twoDimCanvases\" Configuration options for the 2D canvas. \"order\" An array specifying the order in which the levels are to be displayed. Note that currently only a single twoDimDrawingDefinition can be displayed so this array can currently only contain a single element. \"twoDimDrawingDefinitions\" An array containing two dimensional drawing definitions. Each two dimensional drawing definition consists of: \"dots\" An array containing dot definitions. Each dot definition consist of: (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"name\" The name of the dot.(NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xSsffTrack\" The ssffTrackName of the track that contains the x axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xContourNr\" The contour number of the track that contains the x-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"ySsffTrack\" The ssffTrackName of the track that contains the y-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"yContourNr\" The contour number of the track that contains the y-axis values.(NOTE: indentation level reduced by 2 to avoid max indentation problems) \"color\" The RGB color string specifying the color given to dot. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"connectLines\" An array specifying which of the dots specified in the \"dots\" definition array should be connected by a line. (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"fromDot\" The dot from which the line should start. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"toDot\" The dot at which the line should end. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"color\" The RGB string defining the color of the line. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"staticDots\" An array containing static dot definitions:(NOTE: indentation level reduced by 1 to avoid max indentation problems) \"name\" The name of the static dots. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xNameCoordinate\" An x-coordinate specifying the location at which name should be drawn. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"yNameCoordinate\" y-coordinate specifying the location at which name should be drawn. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xCoordinates\" An array of x-coordinates (e.g. [300, 300, 900, 900, 300]). (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"yCoordinates\" An array of y-coordinates (e.g. [880, 2540, 2540, 880, 880]). (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"connect\" A boolean value that specifies whether or not to connect the static dots with lines. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"color\" An RGB string specifying the color of static dots. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"staticContours\" An array containing static contour definitions: (NOTE: indentation level reduced by 1 to avoid max indentation problems) \"name\" The name of static contour. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xSsffTrack\" The ssffTrackName of the track that contains the x-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"xContourNr\" The contour number of the track that contains the x-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"ySsffTrack\" The ssffTrackName of the track that contains the y-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"yContourNr\" The contour number of the track that contains the y-axis values. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"connect\" A boolean value that specifies whether or not to connect the static dots with lines. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"color\" An RGB string specifying color of the static contour. (NOTE: indentation level reduced by 2 to avoid max indentation problems) \"labelCanvasConfig\" Configuration options for the label canvases: \"addTimeMode\" The mode in which time to boundaries is added and subtracted (\"absolute\" or \"relative\"). \"addTimeValue\": The amount of samples added to or subtracted from boundaries. \"newSegmentName\" The value given to the default label if a new SEGMENT is added (default is \"\" == empty string). \"newEventName\" The value given to the default label if a new EVENT is added (default is \"\" == empty string). \"restrictions\": \"playback\" A boolean value specifying whether to allow audio playback. \"correctionTool\" A boolean value specifying whether correction tools are available. \"editItemSize\" A boolean value specifying whether to allow the size of a SEGMENT or EVENT to be changed (i.e., move boundaries). \"editItemName\" A boolean value specifying whether to allow the label of an ITEM to be changed. \"deleteItemBoundary\" A boolean value specifying whether to allow the deletion of boundaries. \"deleteItem\" A boolean value specifying whether to allow the deletion of entire ITEMs \"deleteLevel\" A boolean value specifying whether to allow the deletion of entire levels. \"addItem\" A boolean value specifying whether to allow new ITEMs to be added. \"drawCrossHairs\" A boolean value specifying whether to draw the cross hairs on signal canvases. \"drawSampleNrs\" A boolean value specifying whether to draw the sample numbers in the OSCI canvas if zoomed in close enough to see samples (mainly for debugging and development purposes). \"drawZeroLine\" A boolean value specifying whether to draw the zero value line in OSCI canvas. \"bundleComments\" A boolean value specifying whether to allow the annotator to add comments to bundles she or he has annotated. A bundle comment field will show up in the bundle list side bar for each bundle if this is set to true. Note that the server has to support saving these comments, which the serve() function of the emuR package does not. \"bundleFinishedEditing\" A boolean value specifying whether to allow the annotator to mark when she or he has finished annotating a bundle. A finished editing toggle button will show up in the bundle list side bar for each bundle if this is set to true. Note that the server has to support saving these comments which the serve() function of the emuR package does not. \"showPerspectivesSidebar\" A boolean value specifying whether to show the perspectives side bar. \"activeButtons\" Specifies which top- or bottom-menu buttons should be displayed by the EMU-webApp. \"addLevelSeg\" A boolean value specifying whether to show the add SEGMENT level button in the top menu bar. \"addLevelEvent\" A boolean value specifying whether to show the add EVENT level button in the top menu bar. \"renameSelLevel\" A boolean value specifying whether to allow the user to rename the currently selected level. \"downloadTextGrid\" A boolean value specifying whether to allow the user to download the current annotation as a .TextGrid file by displaying a download TextGrid button in the top menu bar. \"downloadAnnotation\" A boolean value specifying whether to allow the user to download the current annotation as an _annot.json file by displaying a download annotJSON button in the top menu bar. \"specSettings\" A boolean value specifying whether to display the spec. settings button in the top menu bar. \"connect\" A boolean value specifying whether to display the connect button in the top menu bar. \"clear\" A boolean value specifying whether to display the clear button in the top menu bar. \"deleteSingleLevel\" A boolean value specifying whether to allow the user to delete a level containing time information. \"resizeSingleLevel\" A boolean value specifying whether to allow the user to resize a level. \"saveSingleLevel\" A boolean value specifying whether to allow the user to download a single level in the ESPS/waves+ format. \"resizeSignalCanvas\" A boolean value specifying whether to allow the user to resize the signalCanvases (\"OSCI\", \"SPEC\", …). \"openDemoDB\" A boolean value specifying whether to show the open demoDB button. \"saveBundle\" A boolean value specifying whether to show the save button in bundle list side bar for each bundle. \"openMenu\" A boolean value specifying whether open bundle list side bar button is displayed. \"showHierarchy\" A boolean value specifying whether to display the show hierarchy button. \"demoDBs\" An array of strings specifying which demoDBs to display in the open demo drop-down menu. Currently available demo databases are [\"ae\", \"ema\", \"epgdorsal\"]. 15.1.2 _annot.json The _annot.json files contain the actual annotation information as well as the hierarchical linking information. Legacy EMU users should note that all the information that used to be split into several ESPS/waves+ label files and a .hlb file is now contained in this single file. The _annot.json file contains the following fields: \"name\" Specifies the name of the annotation file (has to be equal to the bundle directory prefix as well as the _annot.json prefix). \"annotates\" Specifies the (relative) media file path that this _annot.json file annotates. \"sampleRate\" Specifies the sample rate of the annotation (should be the same as the sample rate of the file listed in \"annotates\"). \"levels\" Contains an array of level annotation informations. Each element consists of: \"name\" Specifies the name of the level. \"items\" An array containing the annotation items of the level. \"id\" The unique ID of the item (only unique within an _annot.json file or bundle, not globally for the emuDB). \"sampleStart\" Contains start sample value of SEGMENT item. \"sampleDur\" Contains sample duration value of SEGMENT item. Note that the EMU-webApp does not support overlapping SEGMENTs or SEGMENT sequences containing gaps. This implies that each sample is explicitly and unambiguously associated with a single SEGMENT. This means that the sampleStart value of a following SEGMENT has to be sampleStart + sampleDur + 1 of the previous SEGMENT. When converting the sample values to time values, the start time value is calculated with the formula \\(start = \\frac{sampleStart}{sampleRate} - \\frac{0.5}{sampleRate}\\) and the end time value with the formula \\(end = \\frac{sampleStart + sampleDur}{sampleRate} + \\frac{0.5}{ sampleRate}\\). This is done to have gapless time values for successive SEGMENTs. To avoid a negative time value when dealing with the first sample of an audio file (sampleStart value of \\(0\\)), the start time value is simply set to \\(0\\) in this case. One implication of this design is that there can be no zero-length segments. If for example a SEGMENT has sampleStart: 1420 and sampleDur: 0, it will be considered to start at the start time of sample \\(1420 - \\frac{0.5}{sampleRate}\\) and end at the end time of sample \\(1420 + \\frac{0.5}{sampleRate}\\). The actual duration in seconds is therefore expressed as \\(\\frac{sampleDur + 1}{sampleRate}\\). The start and end time value calculation is performed by both the query engine of emuR if the calcTimes parameter is set to TRUE and the EMU-webApp to display the time information in the signal canvases. \"samplePoint\" Contains sample point values of EVENT items. When calculating the start time values for EVENTs the following formula is used: \\(start = \\frac{samplePoint}{sampleRate}\\) \"labels\" An array containing labels that belong to this item. Each element consists of: \"name\" Specifies the attributeDefinition that this label is for. \"value\" Specifies the label value. \"links\" An array containing links between two items. These links have to adhere to the links specified in linkDefinitions of the corresponding emuDB. Each link consists of: \"fromID\" The ID value of the item to link from (i.e., item in super-level). \"toID\" The ID value of item to link to (i.e., item in sub-level). 15.1.3 The SSFF file format The SSFF file format is a binary file format which has a plain text header. This means that the header is human-readable and can be viewed with any text editor including common UNIX command line tools such as less or cat. Within R it is possible to view the header by using R’s readLines() function as displayed in the R code snippet below. # load the emuR and wrassp packages library(emuR, warn.conflicts = FALSE) library(wrassp) # create demo data in directory # provided by tempdir() create_emuRdemoData(dir = tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # create path to bundle in database path2bndl = file.path(path2ae, &quot;0000_ses&quot;, &quot;msajc003_bndl&quot;) # create path to .fms file path2fmsFile = file.path(path2bndl, paste0(&quot;msajc003.&quot;, wrasspOutputInfos$forest$ext)) # read in first 8 lines of .fms file. # Note that the header length may vary in other SSFF files. readLines(path2fmsFile, n = 8) ## [1] &quot;SSFF -- (c) SHLRC&quot; &quot;Machine IBM-PC&quot; ## [3] &quot;Record_Freq 200.0&quot; &quot;Start_Time 0.0025&quot; ## [5] &quot;Column fm SHORT 4&quot; &quot;Column bw SHORT 4&quot; ## [7] &quot;Original_Freq DOUBLE 20000.0&quot; &quot;-----------------&quot; The general line item structure of the plain text head of an SSFF file can be described as follows: SSFF – (c) SHLRC (required line): File type marker. Machine IBM-PC (required line): System architecture of the machine that generated the file. This is mainly used to specify the endianness of the data block (see below). Machine IBM-PC indicates little-endian and Machine SPARC indicates big-endian. To date, we have not encountered other machine types. Record_Freq SR (required line): Sample rate of current file in Hz. If, for example, SR is 200.0 (see the above R code snippet) then the sample rate is 200 Hz. Start_Time ST (required line): Time of first sample block in data block in seconds. This often deviates from 0.0 as wrassp’s windowed signal processing functions start with the first window centered around windowShift / 2. If the windowShift parameter’s default value is 5 ms, the start time ST of the first sample block will be 0.0025 sec (see above R code snippet). Column CN CDT CDL (required line(s)): A Column line entry contains four space-separated values, where Column is the initial key word value. The second value, CN (fm in above R code snippet), specifies the name for the column; the third, CDT (SHORT in the above R code snippet), indicates the column’s data type; and the fourth, CDL (4 in the above R code snippet), is the column’s data length in bytes. As can be seen in the R code snippet above, it is quite common for SSFF files to have multiple column entries. The sequence of these entries is relevant, as it specifies the sequence of the data in the binary data block (see below). NAME DT DV (optional line(s)): Optional single value definitions that have a NAME, a data type DT and a data value DV (see Original_Freq DOUBLE 20000.0 in the R code snippet above specifying the original sample rate of the audio file the .fms file was generated from). Comment CHAR string of variable length (optional line(s)): The Comment CHAR allows for comment strings to be added to the header. —————– (required line): marks the end of the plain text header. The binary data block of the SSFF file format stores its data in a so-called interleaved fashion. This means it does not store the binary data belonging to every column in a separate data block. Rather, it interleaves the columns to form sample blocks that occur at the same point in time. Figure 15.1 displays a sequence of short integer values where the subscript text indicates the index in the sequence. This sequence represents a schematic representation of the data block of the .fms file of R Example 15.1. The first four INT161-4 (green) blocks represent the first four INT16 formant values that belong to the fm column and the next four INT165-8 (orange) represent the first four bandwidth values belonging to the bm column. Therefore, the dashed square marks the first sample block (i.e., the first eight F1, F2, F3 and F4; and F1bandwidth, F2bandwidth, F3bandwidth and F4bandwidth values) that occur at the time specified by the Start_Time 0.0025 header entry. The time of all subsequent sample blocks of eight INT16 values (e.g., INT169-16) can be calculated as follows: 0.0025 (== Start_Time) + 1 / 200.0 (== Record_Freq) * sample block index. Figure 15.1: Schematic representation of the data block of the msajc003.fms file of the above R code snippet. 15.2 Example files 15.2.1 _bundleList.json Compared to the _DBconfig.json and _annot.json files, the _bndl.json format is not part of the emuDB database specification. Rather, it is part of the EMU-webApp-websocket-protocol and is used as a standardized format to transport information about all the available bundles to the EMU-webApp. It is not meant as an on-disk file format but rather should be generated on-demand by the server implementing the EMU-webApp-websocket-protocol. A schematic example of a _bndl.json file is displayed in Listing ??. [ { &quot;name&quot;: &quot;msajc003&quot;, &quot;session&quot;: &quot;0000&quot;, &quot;finishedEditing&quot;: false, &quot;comment&quot;: &quot;&quot;, &quot;timeAnchors&quot;: [ { &quot;sample_start&quot;: 1000, &quot;sample_end&quot;: 2000 }, ... ] }, { &quot;name&quot;: &quot;msajc010&quot;, &quot;session&quot;: &quot;0000&quot;, &quot;finishedEditing&quot;: false, &quot;comment&quot;: &quot;&quot; } ] 15.2.2 _bndl.json Compared to the _DBconfig.json and _annot.json files, the _bndl.json format is not part of the emuDB database specification. Rather, it is part of the EMU-webApp-websocket-protocol and is used as a standardized format to transport all the data belonging to a single bundle to the EMU-webApp. It is not meant as an on-disk file format by rather should generated on-demand by the server implementing the EMU-webApp-websocket-protocol. A schematic example of a _bndl.json file is displayed in Listing ??. { &quot;ssffFiles&quot;: [ { &quot;fileExtension&quot;: &quot;fms&quot;, &quot;encoding&quot;: &quot;BASE64&quot;, &quot;data&quot;: &quot;U1N...&quot; } ], &quot;mediaFile&quot;: { &quot;encoding&quot;: &quot;BASE64&quot;, &quot;data&quot;: &quot;Ukl...&quot; }, &quot;annotation&quot;: contentOfAnnot.json } contentOfAnnot.json in Listing ?? refers to the content of a _annot.json file. "],
["app-chap-wsProtocol.html", "16 The EMU-webApp-websocket-protocol Version 2.025 16.1 Protocol overview 16.2 Protocol commands", " 16 The EMU-webApp-websocket-protocol Version 2.025 This chapter describes the EMU-webApp-websocket-protocol in its current version. 16.1 Protocol overview The EMU-webApp-websocket-protocol consists of a set of request-response JSON files that control the interaction between the client (the EMU-webApp) and a server supporting the protocol. A graph depicting the protocol is shown in the Figure 16.1. Figure 16.1: Schematic of the EMU-webApp-websocket-protocol. 16.2 Protocol commands 16.2.1 GETPROTOCOL Initial request to see if client and server speak the same protocol. Request (sent JSON file): { &#39;type&#39;: &#39;GETPROTOCOL&#39;, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: { &#39;protocol&#39;: &#39;EMU-webApp-websocket-protocol&#39;, &#39;version&#39;: &#39;0.0.2&#39; }, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.2 GETDOUSERMANAGEMENT Ask server if it wishes to perform user management (will toggle user login modal window if data is YES). Request (sent JSON file): { &#39;type&#39;: &#39;GETDOUSERMANAGEMENT&#39;, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: &#39;NO&#39; &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.3 LOGONUSER Ask server to log user on. Username and password are sent to server (please only use wss to avoid password being sent in plain text!). This protocol command is sent by the user login modal window. Request (sent JSON file): { &#39;type&#39;: &#39;LOGONUSER&#39;, &#39;data&#39;: { &#39;userName&#39;: &#39;smith&#39;, &#39;pwd&#39;:&#39;mySecretPwd&#39; }, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: &#39;BADUSERNAME&#39; | &#39;BADPASSWORD&#39; | &#39;LOGGEDON&#39; &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.4 GETGLOBALDBCONFIG Request the _DBconfig.json file. Request (sent JSON file): { &#39;type&#39;: &#39;GETGLOBALDBCONFIG&#39;, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: configData, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } In the above Listing, configData represents the Javascript object that is the _DBconfig.json file of the respective database. 16.2.5 GETBUNDLELIST Next a _bundleList.json is requested containing the available bundles. The information contained in this file is what is displayed in the bundle list side bar. An example of a _bundleList.json file is shown in Appendix 15.2.1. Request (sent JSON file): { &#39;type&#39;: &#39;GETBUNDLELIST&#39;, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: bundleList, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.6 GETBUNDLE After receiving the _bundleList.json file by default, the first bundle in the file is requested in the form of a _bndl.json file. This request is also sent when the user clicks a bundle in the bundle list side bar of the EMU-webApp. Request (sent JSON file): { &#39;type&#39;: &#39;GETBUNDLE&#39;, &#39;name&#39;: &#39;msajc003&#39;, &#39;session&#39;: &#39;0000&#39;, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;data&#39;: bundleData, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } In the Listing above bundleData is the Javascript object containing all SSFF files (encoded as a base64 strings) and audio (encoded as a base64 string) and _annot.json that are associated with this bundle. An example of _bndl.json is given in Appendix 15.2.2. 16.2.7 SAVEBUNDLE This function should be called if the user saves a loaded bundle (by pushing the save button in the bundle list side bar). Request (sent JSON file): { &#39;type&#39;: &#39;SAVEBUNDLE&#39;, &#39;data&#39;: bundleData, &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } In the Listing above bundleData is a Javascript object that corresponds to a _bndl.json file. As currently only annotations and formant tracks can be altered by the EMU-webApp, only the _annot.json and formant track SSFF file (if applicable) are sent to the server to be saved. Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.8 DISCONNECTWARNING Function that tells the server that it is about to disconnect. This is currently needed because the httpuv R package cannot listen to the websocket’s own ``close’’ event. Request (sent JSON file): { &#39;type&#39;: &#39;DISCONNECTWARNING&#39; &#39;callbackID&#39;: &#39;xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx&#39; } Response (sent JSON file): { &#39;callbackID&#39;: request.callbackID, &#39;status&#39;: { &#39;type&#39;: &#39;SUCCESS&#39;, &#39;message&#39;: &#39;&#39; } } 16.2.9 Error handling If an error occurs with any of the request types above, a response should still be sent to the client. The status of this response should be set to ERROR and an error message should be given in the message field. This message will then be displayed by the EMU-webApp. ERROR response: { &#39;callbackID&#39;: request.callbackID, &#39;status&#39;: { &#39;type&#39;: &#39;ERROR&#39;, &#39;message&#39;: &#39;An error occured trying to read a file from disk. Please make sure: /path/to/file exists or check the config... } } This appendix chapter is an updated version of a similar description that is part of the EMU-webApp manual.↩ "],
["app-chap-EQL-EBNF.html", "17 EQL EBNF26 17.1 Terminal symbols of EQL2 (operators) and their meaning 17.2 Terminal symbols of EQL2 (brackets) and their meanings. 17.3 Terminal symbols of EQL2 (functions) and their meanings. 17.4 Formal description of EMU Query Language Version 2 17.5 Restrictions", " 17 EQL EBNF26 This chapter presents the EBNF (Garshol 2003) that describes version 2 of the EQL. As the original EBNF adapted from John (2012) was written in German, some of the abbreviation terms were translated into English abbreviations (e.g., DOMA is the abbreviation for the German term Dominanzabfrage and the newly translated DOMQ is the abbreviation for the English term domination query). 17.1 Terminal symbols of EQL2 (operators) and their meaning The terminal symbols described below are listed in descending order by their binding priority. Symbol Meaning # Result modifier (projection) , Parameter list separator == Equality (new in version 2 of the EQL; added for cleaner syntax) = Equality (optional; for backwards compatibility) != Inequality =~ Regular expression matching !~ Regular expression non-matching &amp;gt; Greater than &amp;gt;= Equal to or greater than &amp;lt; Less than &amp;gt;= Equal to or less than | Alternatives separator &amp;amp; Conjunction of equal rank ^ Dominance conjunction -&amp;gt; Sequence operator 17.2 Terminal symbols of EQL2 (brackets) and their meanings. Symbol Meaning ' Quotes literal string ( Function parameter list opening bracket ) Function parameter list closing bracket [ Sequence or dominance-enclosing opening bracket ] Sequence or dominance-enclosing closing bracket 17.3 Terminal symbols of EQL2 (functions) and their meanings. Symbol Meaning Start Start Medial Medial End Final Num Count 17.4 Formal description of EMU Query Language Version 2 EBNF_term Abbreviation Conditions EQL = CONJQ | SEQQ | DOMQ; EMU Query Language DOMQ = &amp;quot;[&amp;quot;, ( CONJQ | DOMQ | SEQQ ), &amp;quot;^&amp;quot;, ( CONJQ | DOMQ | SEQQ ), &amp;quot;]&amp;quot;; dominance query levels must be hierarchically associated SEQQ = &amp;quot;[&amp;quot;, ( CONJQ | SEQQ | DOMQ ), &amp;quot;-&amp;gt;&amp;quot;, ( CONJQ | SEQQ | DOMQ ), &amp;quot;]&amp;quot;; sequential query levels must be linearly associated CONJQ = { &amp;quot;[&amp;quot; }, SQ, { &amp;quot;&amp;amp;&amp;quot;, SQ }, { &amp;quot;]&amp;quot; }; conjunction query levels must be linearly associated SQ = LABELQ | FUNCQ; simple query LABELQ = [ &amp;quot;#&amp;quot; ], LEVEL, ( &amp;quot;=&amp;quot; | &amp;quot;==&amp;quot; | &amp;quot;!=&amp;quot; | &amp;quot;=~&amp;quot; | &amp;quot;!~&amp;quot; ), LABELALTERNATIVES; label query FUNCQ = POSQ | NUMQ; function query POSQ = POSFCT, &amp;quot;(&amp;quot;, LEVEL, &amp;quot;,&amp;quot;, LEVEL, &amp;quot;)&amp;quot;, &amp;quot;=&amp;quot;, &amp;quot;0&amp;quot; | &amp;quot;1&amp;quot;; position query levels must be hierarchically associated; second level determines semantics NUMQ = &amp;quot;Num&amp;quot;, &amp;quot;(&amp;quot;, LEVEL, &amp;quot;,&amp;quot;, LEVEL, &amp;quot;)&amp;quot;, COP, INTPN; number query levels must be hierarchically associated; first level determines semantics LABELALTERNATIVES = LABEL , { &amp;quot;|&amp;quot;, LABEL }; label alternatives LABEL = LABELING | ( &amp;quot;'&amp;quot;, LABELING, &amp;quot;'&amp;quot; ); label levels must be part of the database structure; LABELING is an arbitrary character string or a label group class configured in the emuDB; result modifier # may only occur once POSFCT = &amp;quot;Start&amp;quot; | &amp;quot;Medial&amp;quot; | &amp;quot;End&amp;quot;; position function COP = &amp;quot;=&amp;quot; | &amp;quot;==&amp;quot; | &amp;quot;!=&amp;quot; | &amp;quot;&amp;gt;&amp;quot; | &amp;quot;&amp;lt;&amp;quot; | &amp;quot;&amp;lt;=&amp;quot; | &amp;quot;&amp;gt;=&amp;quot;; comparison operator INTPN = &amp;quot;0&amp;quot; | INTP; integer positive with null INTP = DIGIT-&amp;quot;0&amp;quot;, { DIGIT }; integer positive DIGIT = &amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | &amp;quot;3&amp;quot; | &amp;quot;4&amp;quot; | &amp;quot;5&amp;quot; | &amp;quot;6&amp;quot; | &amp;quot;7&amp;quot; | &amp;quot;8&amp;quot; | &amp;quot;9&amp;quot;; digit INFO: The LABELING term used in the LABEL EBNF term can represent any character string that is present in the annotation. As this can be any combination of Unicode characters, we chose not to explicitly list them as part of the EBNF. 17.5 Restrictions A query may only contain a single result modifier # (hashtag). References "],
["app-chap-eql.html", "18 EQL: further examples 18.1 Simple equality, inequality, matching and non-matching queries (single-argument) 18.2 Sequence queries using the -&gt; sequence operator 18.3 Subsequent sequence queries using nesting of the -&gt; sequence operator 18.4 Conjunction operator &amp; 18.5 Domination operator ^ (hierarchical queries) 18.6 Position 18.7 Combinations 18.8 A few more questions and answers 18.9 Differences to the legacy EMU query language 18.10 Bugs in legacy EMU function emu.query()", " 18 EQL: further examples Below are examples of query strings that have been adapted from Cassidy and Harrington (2001) and Harrington and Cassidy (2002) and which are displayed as questions and answers. All examples use the ae demo emuDB, which is provided by the emuR package, and were extracted from the EQL vignette of the emuR package. Descriptions (some of them duplicates of those in Chapter 6) of the various syntaxes and query components are also included for easier reading. The R code snippet below shows how to access the ae demo emuDB. # load the package library(emuR) # create demo data in directory provided by the tempdir() function create_emuRdemoData(dir = tempdir()) # get the path to emuDB called &#39;ae&#39; that is part of the demo data path2directory = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # load emuDB into current R session ae = load_emuDB(path2directory) 18.1 Simple equality, inequality, matching and non-matching queries (single-argument) The syntax of a simple, equality, inequality, matching and non-matching query is [L OPERATOR A] where L specifies a level (or alternatively the name of a parallel attribute definition), OPERATOR is one of the following operators: == (equality); != (inequality); =~ (matching) or !~ (non-matching), and A is an expression specifying the labels of the annotation items of L. Example questions and answers: Q: What is the query to retrieve all items containing the label “m” in the “Phonetic” level? A: query(emuDBhandle = ae, query = &quot;[Phonetic == m]&quot;) Q: What is the query to retrieve all items containing the label “m” or “n” in the “Phonetic” level? A: query(emuDBhandle = ae, query = &quot;[Phonetic == m | n]&quot;) Q: What is the query to retrieve all items that do not contain the label “m” or “n”? A: query(emuDBhandle = ae, query = &quot;[Phonetic != m | n]&quot;) Q: What is the query to retrieve all items in the “Syllable” level? A: query(emuDBhandle = ae, query = &quot;[Syllable =~ .*]&quot;) Q: What is the query to retrieve all items that begin with “a” in the “Text” level? A: query(ae, &quot;[Text =~ a.*]&quot;) Q: What is the query to retrieve all items that do not begin with “a” in the “Text” level? A: query(ae, &quot;[Text !~ a.*]&quot;) The above examples use three operators that are new to the EQL as of version 2. One is the == equal operator, which has the same meaning as the = operator of the EQL version 1 (which is also still available) while providing a cleaner, more precise syntax. The other two are =~ and !~, which are the new matching and non-matching regular expression operators. Further, it is worth noting that the use of parentheses, blanks or characters that represent operands used by the EQL (see EBNF in Appendix 17) as part of a label matching string (the string on the right hand side of one of the operands mentioned above), must be placed in additional single quotation marks to escape these characters. For example, searching for the items containing the labels O_' on the Phonetic level could not be written as \"[Phonetic == O_']\" but would have to be written as \"[Phonetic == 'O_'']\". Reversing the order of single vs. double quotation marks is currently not supported, that is '[Phonetic == \"O_'\"]' will currently not work. Hence, to avoid this issue only double quotation marks for the outer wrapping of the query string should be used. 18.2 Sequence queries using the -&gt; sequence operator The syntax of a query string using the -&gt; sequence operator is [L == A -&gt; L == B], where item A on level L precedes item B on level L. For a sequential query to work both arguments must be on the same level (alternatively, parallel attribute definitions of the same level may also be chosen). Example Q &amp; A’s: Q: What is the query to retrieve all sequences of items containing the label “@” followed by items containing the label “n” on the “Phonetic” level? A: # NOTE: all row entries in the resulting # segment list have the start time of &quot;@&quot;, the # end time of &quot;n&quot;, and their labels will be &quot;@-&gt;n&quot; query(ae, &quot;[Phonetic == @ -&gt; Phonetic == n]&quot;) Q: Same as the question above but this time we are only interested in the items containing the label “@” in the sequences. A: # NOTE: all row entries in the resulting # segment list have the start time of &quot;@&quot;, the # end time of &quot;@&quot; and their labels will also be &quot;@&quot; query(ae, &quot;[#Phonetic == @ -&gt; Phonetic == n]&quot;) Q: Same as the first question but this time we are only interested in the items containing the label “n”. A: # NOTE: all row entries in the resulting # segment list have the start time of &quot;n&quot;, the # end time of &quot;n&quot; and their labels will also be &quot;n&quot; query(ae, &quot;[Phonetic == @ -&gt; #Phonetic == n]&quot;) 18.3 Subsequent sequence queries using nesting of the -&gt; sequence operator The general strategy for constructing a query string that retrieves subsequent sequences of labels is to nest multiple sequences while paying close attention to the correct placement of the parentheses. An abstract version of such a query string for the subsequent sequence of arguments A1, A2, A3 and A4 would be: [[[[A1 -&gt; A2] -&gt; A3] -&gt; A4] -&gt; A5] where each argument (e.g. A1) represents an equality, inequality, matching or non-matching expression on the same level (alternatively, parallel attribute definitions of the same level may also be chosen). Example questions and answers: Q: What is the query to retrieve all sequences of items containing the labels “@”, “n” and “s” on the “Phonetic” level? A: query(ae, &quot;[[Phonetic == @ -&gt; Phonetic == n] -&gt; Phonetic == s]&quot;) Q: What is the query to retrieve all sequences of items containing the labels “to”, “offer” and “any” on the “Text” level? A: query(ae, &quot;[[Text == to -&gt; Text == offer] -&gt; Text == any]&quot;) Q: What is the query to retrieve all sequences of items containing labels “offer” followed by two arbitrary labels followed by “resistance”? A: # NOTE: usage of paste0() is optional # as it is only used for formating purposes query(ae, paste0(&quot;[[[Text == offer -&gt; Text =&gt; .*] &quot;, &quot;-&gt; Text =&gt; .*] -&gt; Text == resistance]&quot;)) As the EQL1 did not have a regular expression operator, users often resorted to using queries such as [Phonetic != XXX] (where XXX is a label that was not part of the label set of the Phonetic level) to match every label on the Phonetic level. Although this is still possible in the EQL2, we strongly recommend using regular expressions as they provide a much clearer and more precise syntax and are less error-prone. 18.4 Conjunction operator &amp; The syntax of a query string using the conjunction operator can schematically be written as: [L == A &amp; L_a2 == B &amp; L_a3 == C &amp; L_a4 == D &amp; ... &amp; L_an == N], where items on level L have the label A (technically belonging to the first attribute of that level, i.e., L_a1, which per default has the same name as its level) also have the attributes B, C, D, {}, N. As with the sequence operator all expressions must be on the same level (i.e., parallel attribute definitions of the same level indicated by the a2 - an may to be chosen). The conjunction operator is used to combine query conditions on the same level. This makes sense in two cases: when to combining different attributes of the same level: \"[Phonetic == l &amp; sonorant == T]\" when Sonorant is an additional attribute of level Phonetic; when combining a basic query with a function (see sections Position and Count below): \"[phonetic == l &amp; Start(word, phonetic) == 1]\". Example questions and answers: Q: What is the query to retrieve all items containing the label “always” in the “Text” attribute definition which also have the label “C” on a parallel attribute definition called “Word”? A: query(ae, &quot;[Text == always &amp; Word == C]&quot;) Q: What is the query to retrieve all items of the attribute definition “Text” of the level “Word” that were also labeled as function words (labeled “F” in the “Word” level)? A: query(ae, &quot;[Text =~ .* &amp; Word == F]&quot;) Q: What is the query to retrieve all items of the attribute definition “Text” of the level “Word” that were also labeled as content words (labeled “C” in the “Word” level) and as accented (labeled “S” in the attribute definition “Accent” of the same level)? A: query(ae, &quot;[Text =~ .* &amp; Word == C &amp; Accent == S]&quot;) 18.5 Domination operator ^ (hierarchical queries) A schematic representation of a simple domination query string that retrieves all items containing label A of level L1 that are dominated by (i.e., are directly or indirectly linked to) items containing the label B in level L2 is [L1 == A ^ L2 == B]. The domination operator is not directional, meaning that either items in L1 dominate items in L2 or items in L2 dominate items in L1. Note that link definitions that specify the validity of the domination have to be present in the emuDB for this to work. 18.5.1 Simple domination Example questions and answers: Q: What is the query to retrieve all items containing the label “p” in the “Phoneme” level that occur in strong syllables (i.e., dominated by/linked to items of the level “Syllable” that contain the label “S”)? A: query(ae, &quot;[Phoneme == p ^ Syllable == S]&quot;) Q: What is the query to retrieve all syllable items which contain a Phoneme item labeled “p”? A: query(ae, &quot;[Syllable =~ .* ^ Phoneme == p]&quot;) # or query(ae, &quot;[Phoneme == p ^ #Syllable =~ .*]&quot;) Q: What is the query to retrieve all syllable items that have links to Phoneme items that are not labeled “k” or “p” or “t”? A: query(ae, &quot;[Syllable =~ .* ^ Phoneme != p | t | k]&quot;) # or query(ae, &quot;[Phoneme != p | t | k ^ #Syllable =~ .*]&quot;) # note: every syllable with a link to not p | t | k is returned # including those with links to multiple phonemes where a single linked # item doesn&#39;t contain the label &#39;p&#39;, &#39;t&#39; or &#39;k&#39; Even though the domination operator is not directional, what you place to the left and right of the operator does have an impact on the result. If no result modifier (the hash tag \\#) is used, the query engine will automatically assume that the expression to the left of the operator specifies what is to be returned. This means that the schematic query string [L1 == A ^ L2 == B] is semantically equal to the query string [\\#L1 == A ^ L2 == B]. As it is more explicit to mark the desired result we recommend you always use the result modifier where possible. 18.5.2 Multiple domination The general strategy when constructing a query string that specifies multiple domination relations of items is to nest multiple domination expressions while paying close attention to the correct placement of the parentheses. A dominance relationship sequence or the arguments A1, A2, A3, A4, can therefore be noted as: \"[[[[A1 ^ A2] ^ A3] ^ A4] ^ A5]\" where A1 is dominated by A2 and A3 and so on. Example questions and answers: Q: What is the query to retrieve all items on the “Phonetic” level that are part of a strong syllable (labeled “S”) and belong to the words “amongst” or “beautiful”? A: # NOTE: usage of R&#39;s paste0() function is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[#Phonetic =~ .* ^ Syllable == S] &quot;, &quot;^ Text == amongst | beautiful]&quot;)) Q: The same as the question above but this time we want the “Text” items. A: # NOTE: usage of R&#39;s paste0() function is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[Phonetic =~ .* ^ Syllable == S] &quot;, &quot;^ #Text == amongst | beautiful]&quot;)) 18.6 Position The EQL has three function terms to specify where in a domination relationship a child level item is allowed to occur. The three function terms are Start(), End() and Medial(). 18.6.1 Simple usage of Start(), End() and Medial() A schematic representation of a query string representing a simple usage of the Start(), End() and Medial() function would be: \"POSFCT(L1, L2) == 1\" or \"POSFCT(L1, L2) == TRUE\". In this representation POSFCT is a placeholder for one of the three functions where the level L1 must dominate level L2. The == 1 / == TRUE part of the query string indicates that if a match is found (match is TRUE or == 1), the according item of level L2 is returned. If this expression is set to == 0 / == FALSE (FALSE), all the items that do not match the condition of L2 will be returned. A visualization of what is returned by the various options of the three functions is displayed in Figure 6.4. As using 1 and 0 for TRUE and FALSE is not that intuitive to many R users, the EQL version 2 optionally allows for the values TRUE/T and FALSE/F to be used instead of 1 and 0. This syntax should be more familiar to most R users. Example questions and answers: Q: What is the query to retrieve all word-initial syllables? A: query(ae, &quot;[Start(Word, Syllable) == TRUE]&quot;) Q: What is the query to retrieve all word-initial phonemes? A: query(ae, &quot;[Start(Word, Phoneme) == TRUE]&quot;) Q: What is the query to retrieve all non-word-initial syllables? A: query(ae, &quot;[Start(Word, Syllable) == FALSE]&quot;) Q: What is the query to retrieve all word-final syllables? A: query(ae, &quot;[End(Word, Syllable) == TRUE]&quot;) Q: What is the query to retrieve all word-medial syllables? A: query(ae, &quot;[Medial(Word, Syllable) == TRUE]&quot;) 18.6.2 Position and boolean &amp; The syntax for combining a position function with the boolean operator is [L == E &amp; Start(L, L2) == TRUE], where item E on level L occurs at the beginning of item L. Once again, L has to dominate L2 (optionally, parallel attribute definitions of the same level may also be chosen). Example questions and answers: Q: What is the query to retrieve all “n” Phoneme items at the beginning of a syllable? A: query(ae, &quot;[Phoneme == n &amp; Start(Syllable, Phoneme) == 1]&quot;) Q: What is the query to retrieve all word-final “m” Phoneme items? A: query(ae, &quot;[Phoneme == m &amp; End(Word, Phoneme) == 1]&quot;) Q: What is the query to retrieve all non-word-final “S” syllables? A: query(ae, &quot;[Syllable == S &amp; End(Word, Syllable) == 0]&quot;) 18.6.3 Position and boolean ^ The syntax for combining a position function with the boolean hierarchical operator is [L == E ^ Start(L1, L2) == 1], where level L and level L2 refer to different levels where either L dominates L2, or L2 dominates L. Example questions and answers: Q: What is the query to retrieve all “p” Phoneme items which occur in the first syllable of the word? A: query(ae, &quot;[Phoneme == p ^ Start(Word, Syllable) == 1]&quot;) Q: What is the query to retrieve all phonemes which do not occur in the last syllable of the word? A: query(ae, &quot;[Phoneme =~ .* ^ End(Word, Syllable) == 0]&quot;) Count A schematic representation of a query string using the count mechanism looks like [Num(L1, L2) == N], where L1 contains N items in L2. For this type of query to work, L1 has to dominate L2. As the query matches a number (N), it is also possible to use the operators &gt;} (more than), &lt; (less than) and != (not equal). The resulting segment list contains items of L1. Example questions and answers: Q: What is the query to retrieve all words that contain four syllables? A: query(ae, &quot;[Num(Word, Syllable) == 4]&quot;) Q: What is the query to retrieve all syllables that contain more than six phonemes? A: query(ae, &quot;[Num(Syllable, Phoneme) &gt; 6]&quot;) 18.6.4 Count and boolean &amp; A schematic representation of a query string combining the count and the boolean operators looks like [L == E &amp; Num(L1, L2) == N], where items E on level L are dominated by L1 and L1 contains N L2 items. Further, L1 dominates L2 on the condition that L and L1 (not L2) refer to the same level (parallel attribute definitions of the same level may also be chosen). Example questions and answers: Q: What is the query to retrieve the “Text” of all words which consist of more than five phonemes? A: query(ae, &quot;[Text =~ .* &amp; Num(Text, Phoneme) &gt; 5]&quot;) # or query(ae, &quot;[Text =~ .* &amp; Num(Word, Phoneme) &gt; 5]&quot;) Q: What is the query to retrieve all strong syllables that contain five phonemes? A: query(ae, &quot;[Syllable == S &amp; Num(Syllable, Phoneme) == 5]&quot;) 18.6.5 Count and ^ A schematic representation of a query string combining the count and the boolean operators is [L == E ^ Num(L1, L2) == N] where items E on level L are dominated by L1 and L1 contains N L2 items. Further, L1 dominates L2 on the condition that L and L1 do not refer to the same level. Example questions and answers: Q: What is the query to retrieve all “m” phonemes in three-syllable words? A: query(ae, &quot;[Phoneme == m ^ Num(Word, Syllable) == 3]&quot;) Q: What is the query to retrieve all “W” syllables in words of three syllables or less? A: query(ae, &quot;[Syllable = W ^ Num(Word, Syllable) &lt;= 3]&quot;) Q: What is the query to retrieve all words containing syllables which contain four phonemes? A: query(ae, &quot;[Text =~ .* ^ Num(Syllable, Phoneme) == 4]&quot;) 18.7 Combinations 18.7.1 ^ and -&gt; (domination and sequence) A schematic representation of a query string combining the domination and the sequence operators is [[A1 ^ A2] -&gt; A3], where A1 and A3 refer to the same level (parallel attribute definitions of the same level may also be chosen). Example questions and answers: Q: What is the query to retrieve all “m” preceding “p” when “m” is part of an “S” syllable? A: query(ae, &quot;[[Phoneme == m -&gt; Phoneme =~ p] ^ Syllable == S]&quot;) Q: What is the query to retrieve all “s” preceding “t” when “t” is part of a “W” syllable? A: query(ae, &quot;[Phoneme == s -&gt; [Phoneme == t ^ Syllable == W]]&quot;) Q: What is the query to retrieve all “S” syllables, containing an “s” phoneme and preceding an “S” syllable? A: query(ae, &quot;[[#Syllable == S ^ Phoneme == s] -&gt; Syllable == S]&quot;) Q: Same question as above but this time we want all “s” items where “s” is part of a “S” syllable and the “S” syllable precedes another “S” syllable. A: \"[[Phoneme == s ^ Syllable == S] -&gt; Syllable == S]\" would cause an error as Phoneme == s and Syllable == S are not on the same level. Therefore, the correct answer is: query(ae, &quot;[[Syllable == S ^ #Phoneme == s] -&gt; Syllable == S]&quot;) 18.7.2 ^ and -&gt; and &amp; (domination and sequence and boolean &amp;) Example questions and answers: Q: What is the query to retrieve the “Text” of all words beginning with a “@” on the “Phoneme” level? A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[Text =~ .* ^ Phoneme == @ &quot;, &quot;&amp; Start(Text, Phoneme) == 1]&quot;)) Q: What is the query to retrieve all word-initial “m” items in a “S” syllable preceding “o:”? A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[Phoneme == m &amp; Start(Word, Phoneme) == 1 &quot;, &quot;-&gt; Phoneme == o:] ^ Syllable == S]&quot;)) Q: Same question as the question above, but this time we want the “Text” items.\\ A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[[Phoneme == m &amp; Start(Word, Phoneme) == 1 &quot;, &quot;-&gt; Phoneme == o:] ^ Syllable == S] &quot;, &quot;^ #Text =~ .*]&quot;)) 18.8 A few more questions and answers Q: What is the query to retrieve all “m” or “n” phonemes which occur in the word-medial position? A: query(ae, &quot;[Phoneme == m | n &amp; Medial(Word, Phoneme) == 1]&quot;) Q: What is the query to retrieve all “H” phonetic segments followed by an arbitrary segment and then by either “I” or “U”? A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[Phonetic == H -&gt; Phonetic =~ .*] &quot;, &quot;-&gt; Phonetic == I | U]&quot;)) Q: What is the query to retrieve all syllables which do not occur in word-medial positions? A: query(ae, &quot;[Syllable =~ .* &amp; Medial(Word, Syllable) == 0]&quot;) Q: What is the query to retrieve the “Text” items of all words containing two syllables? A: query(ae, &quot;[Text =~ .* &amp; Num(Text, Syllable) == 2]&quot;) Q: What is the query to retrieve the “Text” items of all accented words following “the”? A: query(ae, &quot;[Text == the -&gt; #Text =~ .* &amp; Accent == S]&quot;) Q: What is the query to retrieve all “S” (strong) syllables consisting of five phonemes? A: query(ae, &quot;[Syllable = S ^ Num(Word, Phoneme) == 5]&quot;) Q: What is the query to retrieve all “W” (weak) syllables containing a “@” phoneme? A: query(ae, &quot;[Syllable == W ^ Phoneme == @]&quot;) Q: What is the query to retrieve all Phonetic items belonging to a “W” (weak) syllable?\\ A: query(ae,&quot;[Phonetic =~ .* ^ #Syllable == W]&quot;) Q: What is the query to retrieve “W” (weak) syllables in word-final position occurring in three-syllable words? A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[Syllable == W &amp; End(Word, Syllable) == 1 &quot;, &quot;^ Num(Word, Syllable) == 3]&quot;)) Q: What is the query to retrieve all phonemes dominating “H” Phonetic items at the beginning of a syllable and occurring in accented (“S”) words? A: # NOTE: usage of paste0() is optional # as it is only used for formatting purposes query(ae, paste0(&quot;[[[Phoneme =~ .* ^ Phonetic == H] &quot;, &quot;^ Start(Word, Syllable) == 1] ^ Accent == S]&quot;)) 18.9 Differences to the legacy EMU query language In this section summarizes the major changes concerning the query mechanics of emuR compared to the legacy R package emu Version 4.2. This section is mainly aimed at users transitioning to emuR from the legacy system. 18.9.1 Function call syntax In emuR it is necessary to load an emuDB into the current R session before being able to use the query() function. This is achieved using the load_emuDB() function. This was not necessary using the legacy emu.query() function. 18.9.2 Empty result The query function of emuR returns an empty segment list (row count is zero) if the query does not match any items. If the legacy EMU function emu.query() did not find any matches it, returned an error with the message: ## Can&#39;t find the query results in emu.query: there may have ## been a problem with the query command. 18.9.3 The result modifier hash tag # Compared to the legacy EMU system, which allowed multiple occurrences of the hash tag \\# to be present in a query string, the query() function only allows a single result modifier. This ensures that only consistent result sets are returned (i.e., all items belong to a single level). However, if multiple result sets in one segment list are desired, this can easily be achieved by concatenating the result sets of separate queries using the rbind() function. 18.9.4 Interpretation of the hash tag # in conjunction operator queries 18.9.5 legacy EMU emu.query(template = &quot;andosl&quot;, pattern = &quot;*&quot;, query = &quot;[Text=spring &amp; #Accent=S]&quot;)} yielded: ## moving data from Tcl to R ## Read 1 records ## segment list from database: andosl ## query was: [Text=spring &amp; #Accent=S] ## labels start end utts ## 1 spring 2288.959 2704.466 msajc094 and emu.query(template = &quot;andosl&quot;, pattern = &quot;*&quot;, query = &quot;[#Text=spring &amp; #Accent=S]&quot;) yielded the identical: ## moving data from Tcl to R ## Read 1 records ## segment list from database: andosl ## query was: [#Text=spring &amp; #Accent=S] ## labels start end utts ## 1 spring 2288.959 2704.466 msajc094 Hence, the hash tag # had no effect. 18.9.5.1 emuR query(emuDBhandle = andosl, query = &quot;[Text == spring &amp; #Accent == S]&quot;, resultType = &quot;emusegs&quot;) ## segment list from database: andosl ## query was: [Text=spring &amp; #Accent=S] ## labels start end utts ## 1 S 2288.975 2704.475 0000:msajc094 Returns the same item but with the label of the hashed attribute definition name. The second legacy example is not a valid emuR query (two hash tags) and will return an error message. query(dbName = &quot;andosl&quot;, query = &quot;[#Text == spring &amp; #Accent == S]&quot;) ## Error in query.database.eql.KONJA(dbConfig, qTrim) : ## Only one hashtag allowed in linear query term: #Text=spring &amp; #Accent=S 18.10 Bugs in legacy EMU function emu.query() 18.10.1 Alternative labels in inequality queries Example: 18.10.2 legacy EMU It appears that the OR operator | was mistakenly ignored when used in conjunction with the inequality operator !=: emu.query(template = &quot;ae&quot;, pattern = &quot;*&quot;, query = &quot;[Text != beautiful | futile ^ Phoneme = u:]&quot;) yielded: ## moving data from Tcl to R ## Read 4 records ## segment list from database: ae ## query was: [Text!=beautiful|futile ^ Phoneme=u:] ## labels start end utts ## 1 new 475.802 666.743 msajc057 ## 2 futile 571.999 1091.000 msajc010 ## 3 to 1091.000 1222.389 msajc010 ## 4 beautiful 2033.739 2604.489 msajc003 18.10.3 emuR The query engine of the emuR package respects the presence of the OR operator in such queries: query(emuDBhandle = ae, query = &quot;[Text != beautiful | futile ^ Phoneme == u:]&quot;, resultType = &quot;emusegs&quot;) ## segment list from database: ae ## query was: [Text!=beautiful|futile ^ Phoneme=u:] ## labels start end utts ## 1 to 1091.025 1222.375 0000:msajc010 ## 2 new 475.825 666.725 0000:msajc057 18.10.4 Errors caused by missing or superfluous blanks or parentheses Some queries in the legacy EMU system required blanks around certain operators to be present or absent as well as parentheses to be present or absent. If this was not the case the legacy query engine sometimes returned cryptic errors, sometimes crashing the current R session. The query engine of the emuR package is much more robust against missing or superfluous blanks or parentheses. 18.10.5 Order of result segment list To our knowledge, the order of a segment list in the legacy EMU system was never predictable or explicitly defined. In the new system, if the result type of the query() function is set to \"emuRsegs\" the resulting list is ordered by UUID, session, bundle and sample start position. If the parameter calcTimes is set to FALSE it is ordered by UUID, session, bundle, level, seq_idx. If it is set to \"emusegs\" the resulting list is ordered by the fields utts and start. 18.10.6 Additional features The query mechanics of emuR accepts the double equal character string == (recommended) as well as the single = equal character string as an equal operator. The EQL2 is capable of querying labels by matching regular expressions using the =~ (matching) and !~ (non-matching) operators. It is worth noting that the regular expression pattern is always meant to consume the entire label string i.e. wrapping the RegEx in \"^ RegEx $\" is not necessary. If substring matching is desired this must be formulated explicitly (e.g. \".*substring.*\") For example: query(\"andosl\", \"Text =~ .*tz.*\") References "],
["app-chap-wrassp.html", "19 wrassp 19.1 Using Praat’s signal processing routines in the EMU-SDMS 19.2 Using OpenSMILE signal processing routines in the EMU-SDMS", " 19 wrassp 19.1 Using Praat’s signal processing routines in the EMU-SDMS The R code snippet below shows how generating an AsspDataObj from scratch can be used in a function to place data from other sources into SSFF files. In this case it uses the PraatR R package (see http://www.aaronalbin.com/praatr/index.html) to execute Praat’s \"To Formant (burg)...\" function to then store the data to a comma separated file using \"Down to Table...\". The generated table is then read into R and the appropriate columns are placed into tracks of a AsspDataObj object. The PraatToFormants2AsspDataObj can be viewed as a template function as it can easily be adapted to use other functions provided by Praat or even other external tools. ################################### # uncomment and execute the next # two lines to install PraatR # library(devtools) # install_github(&#39;usagi5886/PraatR&#39;) library(PraatR) library(wrassp) library(tools) PraatToFormants2AsspDataObj &lt;- function(path, command = &quot;To Formant (burg)...&quot;, arguments = list(0.0, 5, 5500, 0.025, 50), columnNames = c(&quot;fm&quot;, &quot;bw&quot;)){ tmp1FileName = &quot;tmp.ooTextFile&quot; tmp2FileName = &quot;tmp.table&quot; tmp1FilePath = file.path(tempdir(), tmp1FileName) tmp2FilePath = file.path(tempdir(), tmp2FileName) # remove tmp files if they already exist unlink(file.path(tempdir(), tmp1FileName)) unlink(file.path(tempdir(), tmp2FileName)) # generate ooTextFile praat(command = command, input=path, arguments = arguments, output = tmp1FilePath) # convert to Table praat(&quot;Down to Table...&quot;, input = tmp1FilePath, arguments = list(F, T, 6, F, 3, T, 3, T), output = tmp2FilePath, filetype=&quot;comma-separated&quot;) # get vals df = read.csv(tmp2FilePath, stringsAsFactors=FALSE) df[df == &#39;--undefined--&#39;] = 0 fmVals = df[,c(3, 5, 7, 9, 11)] fmVals = sapply(colnames(fmVals), function(x){ as.integer(fmVals[,x]) }) colnames(fmVals) = NULL bwVals = data.matrix(df[,c(4, 6, 8, 10, 12)]) bwVals = sapply(colnames(bwVals), function(x){ as.integer(bwVals[,x]) }) colnames(bwVals) = NULL # get start time startTime = df[1,1] # create AsspDataObj ado = list() attr(ado, &quot;trackFormats&quot;) =c(&quot;INT16&quot;, &quot;INT16&quot;) if(arguments[[1]] == 0){ sR = 1 / (0.25 * arguments[[4]]) }else{ sR = 1 / arguments[[1]] } attr(ado, &quot;sampleRate&quot;) = sR tmpObj = read.AsspDataObj(path) attr(ado, &quot;origFreq&quot;) = attr(tmpObj, &quot;sampleRate&quot;) attr(ado, &quot;startTime&quot;) = startTime # attr(ado, &quot;startRecord&quot;) = as.integer(1) attr(ado, &quot;endRecord&quot;) = as.integer(nrow(fmVals)) class(ado) = &quot;AsspDataObj&quot; AsspFileFormat(ado) &lt;- &quot;SSFF&quot; AsspDataFormat(ado) &lt;- as.integer(2) ado = addTrack(ado, columnNames[1], fmVals, &quot;INT16&quot;) ado = addTrack(ado, columnNames[2], bwVals, &quot;INT16&quot;) # add missing values at the start as Praat sometimes # has very late start values which causes issues # in the SSFF file format as this sets the startRecord # depending on the start time of the first sample if(startTime &gt; 1/sR){ nr_of_missing_samples = floor(startTime / (1/sR)) missing_fm_vals = matrix(0, nrow = nr_of_missing_samples, ncol = ncol(ado$fm)) missing_bw_vals = matrix(0, nrow = nr_of_missing_samples, ncol = ncol(ado$bw)) # prepend values ado$fm = rbind(missing_fm_vals, ado$fm) ado$bw = rbind(missing_fm_vals, ado$bw) # fix start time attr(ado, &quot;startTime&quot;) = startTime - nr_of_missing_samples * (1/sR) } return(ado) } ######################################## # Use of function on &#39;ae&#39; emuDB library(emuR) # create demo data in tempdir() create_emuRdemoData(tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # list all .wav files in the ae emuDB paths2wavFiles = list.files(path2ae, pattern = &quot;*.wav$&quot;, recursive = TRUE, full.names = TRUE) # loop through files for(fp in paths2wavFiles){ ado = PraatToFormants2AsspDataObj(fp) newPath = paste0(file_path_sans_ext(fp), &#39;.praatFms&#39;) # print(paste0(fp, &#39; -&gt; &#39;, newPath)) # uncomment for simple log write.AsspDataObj(ado, file = newPath) } # load emuDB ae = load_emuDB(path2ae, verbose = FALSE) # add SSFF track definition add_ssffTrackDefinition(ae, name = &quot;praatFms&quot;, columnName = &quot;fm&quot;, fileExtension = &quot;praatFms&quot;) # test query + get_trackdata sl = query(ae, &quot;Phonetic == n&quot;) td = get_trackdata(ae, sl, ssffTrackName = &quot;praatFms&quot;, verbose = F) 19.2 Using OpenSMILE signal processing routines in the EMU-SDMS library(wrassp) library(tools) ##&#39; convert CSV output of SMILExtract to AsspDataObject ##&#39; @param path path to wav file ##&#39; @param SMILExtractPath path to SMILExtract executable ##&#39; @param configPath path to openSMILE config file ##&#39; @param columsAsTracks if TRUE -&gt; every column will be placed in it&#39;s own track ##&#39; if FALSE -&gt; every column is placed into a single track called SMILExtractAll SMILExtract2AsspDataObj &lt;- function(path, SMILExtractPath, configPath, columsAsTracks = TRUE){ tmp1FileName = &quot;tmp.csv&quot; tmp1FilePath = file.path(tempdir(), tmp1FileName) # remove tmp file if it already exists unlink(file.path(tempdir(), tmp1FileName)) system(paste0(SMILExtractPath, &quot; -C &quot;, configPath, &quot; -I &quot;, path, &quot; -O &quot;, tmp1FilePath), ignore.stdout = T, ignore.stderr = T) # get vals df = suppressMessages(readr::read_delim(tmp1FilePath, delim = &quot;;&quot;)) # extract + remove frameIndex/frameTime frameIndex = df$frameIndex frameTime = df$frameTime df$frameIndex = NULL df$frameTime = NULL df = as.matrix(df) colNames = colnames(df) # get start time startTime = frameTime[1] # create AsspDataObj ado = list() attr(ado, &quot;sampleRate&quot;) = 1/frameTime[2] # second frameTime should be stepsize tmpObj = read.AsspDataObj(path) attr(ado, &quot;origFreq&quot;) = attr(tmpObj, &quot;sampleRate&quot;) attr(ado, &quot;startTime&quot;) = startTime # attr(ado, &quot;startRecord&quot;) = as.integer(1) attr(ado, &quot;endRecord&quot;) = as.integer(nrow(df)) class(ado) = &quot;AsspDataObj&quot; AsspFileFormat(ado) &lt;- &quot;SSFF&quot; AsspDataFormat(ado) &lt;- as.integer(2) # add every column as new track if(columsAsTracks){ attr(ado, &quot;trackFormats&quot;) = rep(&quot;REAL32&quot;, ncol(df)) for(i in 1:ncol(df)){ ado = addTrack(ado, trackname = colNames[i], data = df[,i], format = &quot;REAL32&quot;) } }else{ attr(ado, &quot;trackFormats&quot;) = &quot;REAL32&quot; ado = addTrack(ado, trackname = &quot;SMILExtractAll&quot;, data = df, format = &quot;REAL32&quot;) } return(ado) } ######################################## # Use of function on &#39;ae&#39; emuDB library(emuR) # create demo data in tempdir() create_emuRdemoData(tempdir()) # create path to demo database path2ae = file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;) # list all .wav files in the ae emuDB paths2wavFiles = list.files(path2ae, pattern = &quot;*.wav$&quot;, recursive = TRUE, full.names = TRUE) # loop through files for(fp in paths2wavFiles){ ado = SMILExtract2AsspDataObj(fp, SMILExtractPath = &quot;~/programs/opensmile-2.3.0/bin/SMILExtract&quot;, configPath = &quot;~/programs/opensmile-2.3.0/config/demo/demo1_energy.conf&quot;) newPath = paste0(file_path_sans_ext(fp), &#39;.SMILExtract&#39;) # print(paste0(fp, &#39; -&gt; &#39;, newPath)) # uncomment for simple log write.AsspDataObj(ado, file = newPath) } # load emuDB ae = load_emuDB(path2ae, verbose = FALSE) # add SSFF track definition add_ssffTrackDefinition(ae, name = &quot;SMILExtract&quot;, columnName = &quot;pcm_LOGenergy&quot;, fileExtension = &quot;SMILExtract&quot;) # test query + get_trackdata sl = query(ae, &quot;Phonetic == n&quot;) td = get_trackdata(ae, sl, ssffTrackName = &quot;SMILExtract&quot;, verbose = F) # test display set_signalCanvasesOrder(ae, perspectiveName = &quot;default&quot;, order = c(&quot;OSCI&quot;, &quot;SPEC&quot;, &quot;SMILExtract&quot;)) # serve(ae) "],
["about.html", "20 About", " 20 About This section contains various independent tutorials / best practices on common challenges/issues regarding various aspects of the EMU-SDMS. "],
["spectral-analysis.html", "21 Spectral analysis 21.1 How to quantify differences between spectra? 21.2 Discrete Cosine Transform (DCT) 21.3 DCT coefficients 21.4 Spectral moments", " 21 Spectral analysis (Adapted from WP 4.1 Sprachtechnologie (Vertiefung) course material by Jonathan Harrington and Ulrich Reubold) First of all, we need to install the package gridExtra, which allows for arranging several plots from ggplot2 into a single figure: install.packages(&quot;gridExtra&quot;) Let us now load the needed libraries and create a demo emuDB to play with: library(gridExtra) library(emuR) library(tidyverse) # containing - amongst others - dplyr, purrr, tibble, and ggplot2 # create demo data in directory provided by the tempdir() function # (of course other directory paths may be chosen) create_emuRdemoData(dir = tempdir()) # create path to demo data directory, which is # called &quot;emuR_demoData&quot; demo_data_dir = file.path(tempdir(), &quot;emuR_demoData&quot;) # create path to ae_emuDB which is part of the demo data path2ae = file.path(demo_data_dir, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) list_ssffTrackDefinitions(ae) The ae emuDB has ssffTrackDefinitions for pre-calculated so-called dft-files containing dft data. DFT stands for “Discrete Fourier Transform” which converts a signal into a time-series of spectra. This transformation can be done with wrassp’s function dftSpectrum() which - despite of its name - actually uses a Fast Fourier Transform algorithm. The function produces a “short-term spectral analysis of the signal in using the Fast Fourier Transform. The default is to calculate an unsmoothed narrow-band spectrum with the size of the analysis window equal to the length of the FFT. The output from the FFT will be converted to a power spectrum in dB from 0 Hz up to and including the Nyquist rate. Analysis results will be written to a file with the base name of the input file and the spectrum type in lower case as extension (e.g. ‘.dft’). Default output is in SSFF format with the spectrum type in lower case as track name.” (cited from ?wrassp::dftSpectrum). The procedure to calculate your own dft-files is identical to that of calculating formants, or fundamental frequency, or any other function that is available in wrassp (see also this document ): add_ssffTrackDefinition(ae, name = &quot;dft&quot;, onTheFlyFunctionName = &quot;dftSpectrum&quot;, verbose = FALSE) In this case, there is no need to subset the data in order to use speaker specific settings, not only because there is only one speaker in our database, but also because dftSpectrum() simply doesn’t need to be adjusted to speaker-specific features. We then query for a segment list and then get the trackdata: sS.sl = query(ae, &quot;[Phonetic == s|S]&quot;) sS.dft = get_trackdata(ae, seglist = sS.sl, ssffTrackName = &quot;dft&quot;, resultType = &quot;tibble&quot;, cut = 0.5) The following command can be used to look at the resulting data #view it in RStudio View(sS.dft) Let us now list the column names of the extracted trackdata object: names(sS.dft) ## [1] &quot;sl_rowIdx&quot; &quot;labels&quot; &quot;start&quot; ## [4] &quot;end&quot; &quot;utts&quot; &quot;db_uuid&quot; ## [7] &quot;session&quot; &quot;bundle&quot; &quot;start_item_id&quot; ## [10] &quot;end_item_id&quot; &quot;level&quot; &quot;start_item_seq_idx&quot; ## [13] &quot;end_item_seq_idx&quot; &quot;type&quot; &quot;sample_start&quot; ## [16] &quot;sample_end&quot; &quot;sample_rate&quot; &quot;times_orig&quot; ## [19] &quot;times_rel&quot; &quot;times_norm&quot; &quot;T1&quot; ## [22] &quot;T2&quot; &quot;T3&quot; &quot;T4&quot; ## [25] &quot;T5&quot; &quot;T6&quot; &quot;T7&quot; ## [28] &quot;T8&quot; &quot;T9&quot; &quot;T10&quot; ## [31] &quot;T11&quot; &quot;T12&quot; &quot;T13&quot; ## [34] &quot;T14&quot; &quot;T15&quot; &quot;T16&quot; ## [37] &quot;T17&quot; &quot;T18&quot; &quot;T19&quot; ## [40] &quot;T20&quot; &quot;T21&quot; &quot;T22&quot; ## [43] &quot;T23&quot; &quot;T24&quot; &quot;T25&quot; ## [46] &quot;T26&quot; &quot;T27&quot; &quot;T28&quot; ## [49] &quot;T29&quot; &quot;T30&quot; &quot;T31&quot; ## [52] &quot;T32&quot; &quot;T33&quot; &quot;T34&quot; ## [55] &quot;T35&quot; &quot;T36&quot; &quot;T37&quot; ## [58] &quot;T38&quot; &quot;T39&quot; &quot;T40&quot; ## [61] &quot;T41&quot; &quot;T42&quot; &quot;T43&quot; ## [64] &quot;T44&quot; &quot;T45&quot; &quot;T46&quot; ## [67] &quot;T47&quot; &quot;T48&quot; &quot;T49&quot; ## [70] &quot;T50&quot; &quot;T51&quot; &quot;T52&quot; ## [73] &quot;T53&quot; &quot;T54&quot; &quot;T55&quot; ## [76] &quot;T56&quot; &quot;T57&quot; &quot;T58&quot; ## [79] &quot;T59&quot; &quot;T60&quot; &quot;T61&quot; ## [82] &quot;T62&quot; &quot;T63&quot; &quot;T64&quot; ## [85] &quot;T65&quot; &quot;T66&quot; &quot;T67&quot; ## [88] &quot;T68&quot; &quot;T69&quot; &quot;T70&quot; ## [91] &quot;T71&quot; &quot;T72&quot; &quot;T73&quot; ## [94] &quot;T74&quot; &quot;T75&quot; &quot;T76&quot; ## [97] &quot;T77&quot; &quot;T78&quot; &quot;T79&quot; ## [100] &quot;T80&quot; &quot;T81&quot; &quot;T82&quot; ## [103] &quot;T83&quot; &quot;T84&quot; &quot;T85&quot; ## [106] &quot;T86&quot; &quot;T87&quot; &quot;T88&quot; ## [109] &quot;T89&quot; &quot;T90&quot; &quot;T91&quot; ## [112] &quot;T92&quot; &quot;T93&quot; &quot;T94&quot; ## [115] &quot;T95&quot; &quot;T96&quot; &quot;T97&quot; ## [118] &quot;T98&quot; &quot;T99&quot; &quot;T100&quot; ## [121] &quot;T101&quot; &quot;T102&quot; &quot;T103&quot; ## [124] &quot;T104&quot; &quot;T105&quot; &quot;T106&quot; ## [127] &quot;T107&quot; &quot;T108&quot; &quot;T109&quot; ## [130] &quot;T110&quot; &quot;T111&quot; &quot;T112&quot; ## [133] &quot;T113&quot; &quot;T114&quot; &quot;T115&quot; ## [136] &quot;T116&quot; &quot;T117&quot; &quot;T118&quot; ## [139] &quot;T119&quot; &quot;T120&quot; &quot;T121&quot; ## [142] &quot;T122&quot; &quot;T123&quot; &quot;T124&quot; ## [145] &quot;T125&quot; &quot;T126&quot; &quot;T127&quot; ## [148] &quot;T128&quot; &quot;T129&quot; &quot;T130&quot; ## [151] &quot;T131&quot; &quot;T132&quot; &quot;T133&quot; ## [154] &quot;T134&quot; &quot;T135&quot; &quot;T136&quot; ## [157] &quot;T137&quot; &quot;T138&quot; &quot;T139&quot; ## [160] &quot;T140&quot; &quot;T141&quot; &quot;T142&quot; ## [163] &quot;T143&quot; &quot;T144&quot; &quot;T145&quot; ## [166] &quot;T146&quot; &quot;T147&quot; &quot;T148&quot; ## [169] &quot;T149&quot; &quot;T150&quot; &quot;T151&quot; ## [172] &quot;T152&quot; &quot;T153&quot; &quot;T154&quot; ## [175] &quot;T155&quot; &quot;T156&quot; &quot;T157&quot; ## [178] &quot;T158&quot; &quot;T159&quot; &quot;T160&quot; ## [181] &quot;T161&quot; &quot;T162&quot; &quot;T163&quot; ## [184] &quot;T164&quot; &quot;T165&quot; &quot;T166&quot; ## [187] &quot;T167&quot; &quot;T168&quot; &quot;T169&quot; ## [190] &quot;T170&quot; &quot;T171&quot; &quot;T172&quot; ## [193] &quot;T173&quot; &quot;T174&quot; &quot;T175&quot; ## [196] &quot;T176&quot; &quot;T177&quot; &quot;T178&quot; ## [199] &quot;T179&quot; &quot;T180&quot; &quot;T181&quot; ## [202] &quot;T182&quot; &quot;T183&quot; &quot;T184&quot; ## [205] &quot;T185&quot; &quot;T186&quot; &quot;T187&quot; ## [208] &quot;T188&quot; &quot;T189&quot; &quot;T190&quot; ## [211] &quot;T191&quot; &quot;T192&quot; &quot;T193&quot; ## [214] &quot;T194&quot; &quot;T195&quot; &quot;T196&quot; ## [217] &quot;T197&quot; &quot;T198&quot; &quot;T199&quot; ## [220] &quot;T200&quot; &quot;T201&quot; &quot;T202&quot; ## [223] &quot;T203&quot; &quot;T204&quot; &quot;T205&quot; ## [226] &quot;T206&quot; &quot;T207&quot; &quot;T208&quot; ## [229] &quot;T209&quot; &quot;T210&quot; &quot;T211&quot; ## [232] &quot;T212&quot; &quot;T213&quot; &quot;T214&quot; ## [235] &quot;T215&quot; &quot;T216&quot; &quot;T217&quot; ## [238] &quot;T218&quot; &quot;T219&quot; &quot;T220&quot; ## [241] &quot;T221&quot; &quot;T222&quot; &quot;T223&quot; ## [244] &quot;T224&quot; &quot;T225&quot; &quot;T226&quot; ## [247] &quot;T227&quot; &quot;T228&quot; &quot;T229&quot; ## [250] &quot;T230&quot; &quot;T231&quot; &quot;T232&quot; ## [253] &quot;T233&quot; &quot;T234&quot; &quot;T235&quot; ## [256] &quot;T236&quot; &quot;T237&quot; &quot;T238&quot; ## [259] &quot;T239&quot; &quot;T240&quot; &quot;T241&quot; ## [262] &quot;T242&quot; &quot;T243&quot; &quot;T244&quot; ## [265] &quot;T245&quot; &quot;T246&quot; &quot;T247&quot; ## [268] &quot;T248&quot; &quot;T249&quot; &quot;T250&quot; ## [271] &quot;T251&quot; &quot;T252&quot; &quot;T253&quot; ## [274] &quot;T254&quot; &quot;T255&quot; &quot;T256&quot; ## [277] &quot;T257&quot; sS.dft contains spectral data, i.e. in this case 257 amplitude values per frame (in our case, we only have one frame per segment). So there are track colums T1 … T257 (and there could be even more - depending on the Nyquist frequency - see below) instead of only one track column (as it would be the case in e.g. fundamental frequency data) or 4 or 5 signal tracks as it would be the case for formants. Not only is it hard to plot data that is structured like this, we still miss some important information: the frequencies with which the amplitude values in T1…T257 are associated with. We could calculate these frequencies by dividing the sample rate of the signals unique(sS.dft$sample_rate) ## [1] 20000 by 2, in order to calculate the Nyquist frequency, and then by creating 257-1 equal-sized steps between 0 and this Nyquist frequency. freqs = seq(from=0,to=unique(sS.dft$sample_rate),length.out = 257) freqs[1:5] ## [1] 0.000 78.125 156.250 234.375 312.500 freqs[252:257] ## [1] 19609.38 19687.50 19765.62 19843.75 19921.88 20000.00 However, we don’t have to do so, as another function of emuR called convert_wideToLong() does that for us when we set the parameter calcFreq to TRUE. As the name of the function suggests, the 257 columns that contain amplitudes will be transformed to 257 observations per frame in one single column: sS.dftlong = convert_wideToLong(sS.dft,calcFreqs = T) sS.dftlong ## # A tibble: 5,397 x 23 ## sl_rowIdx labels start end utts db_uuid session bundle start_item_id ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 2 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 3 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 4 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 5 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 6 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 7 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 8 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 9 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## 10 1 s 483. 567. 0000… 0fc618… 0000 msajc… 151 ## # … with 5,387 more rows, and 14 more variables: end_item_id &lt;int&gt;, ## # level &lt;chr&gt;, start_item_seq_idx &lt;int&gt;, end_item_seq_idx &lt;int&gt;, ## # type &lt;chr&gt;, sample_start &lt;int&gt;, sample_end &lt;int&gt;, sample_rate &lt;int&gt;, ## # times_orig &lt;dbl&gt;, times_rel &lt;dbl&gt;, times_norm &lt;dbl&gt;, track_name &lt;chr&gt;, ## # track_value &lt;dbl&gt;, freq &lt;dbl&gt; So, instead of columns named T1, T2, … Tn, we now have three other columns: track_name: contains “T1” … “Tn” track_value: contains the (in this case 257) amplitudes per frame freq: the frequencies with which the aforementioned amplitudes are associated with As all observations (i.e. the amplitudes) are now in one column (and frequency information in another column), we can easily plot this as xy-plot with ggplot2: # plot the spectral slices ggplot(sS.dftlong) + aes(x = freq, y = track_value,col=labels) + geom_line() + facet_wrap( ~ sl_rowIdx + labels) You could also use geom_area() (but be careful: use geom_area() only if you intend to plot individual slices): ggplot(sS.dftlong) + aes(x = freq, y = track_value) + geom_area() + facet_wrap( ~ sl_rowIdx + labels) We can also summarise this easily to one averaged slice per fricative type: sS.dftlong.mean = sS.dftlong%&gt;% group_by(labels,freq)%&gt;% summarise(track_value=mean(track_value)) ggplot(sS.dftlong.mean) + aes(x = freq, y = track_value, col=labels) + geom_line() 21.1 How to quantify differences between spectra? When we look at the figure above, showing average spectra at the temporal midpoints of two fricative categories, it seems that one easy way to distinguish between the two would be to concentrate on the differences in the amplitudes in the 2000 - 3000 Hz range. We could e.g. take the mean in that frequency range across all tokens of the two types in order to check whether it is consistently the case that the alveolar contains much less energy in that frequency range than its postalveolar counterpart: sS2to3thousandHz = sS.dftlong%&gt;% filter(freq&gt;=2000 &amp; freq &lt;=3000)%&gt;% group_by(labels,sl_rowIdx)%&gt;% summarise(amplitudes_2000_3000Hz = mean(track_value)) ggplot(sS2to3thousandHz)+ aes(x=labels,y=amplitudes_2000_3000Hz,col=labels)+ geom_boxplot() However, it would be more “elegant” to consider some aspects of the general “form” of a spectral slice. Each spectral slice is simply a vector of numbers (that has some hidden association to another verctor of the same size, i.e. the frequencies with which the amplitude values are associated with). One way of quantifying a numeric vector by a (usually smaller) set of numbers is the Discrete Cosine Transform. 21.2 Discrete Cosine Transform (DCT) (See also this document.) A discrete cosine transform (DCT) expresses a finite sequence of n data points in terms of a sum of cosine functions oscillating at different frequencies. The amplitudes of the cosine functions, k0, k1, k2, … kn-1, are called DCT coefficients. k0: the amplitude of a cosine with a frequency of 0 k1: the amplitude of a cosine with a frequency of 0.5 k2: the amplitude of a cosine with a frequency of 1 … kn-1: the amplitude of a cosine with a frequency of 0.5*(n-1) If you sum up all these DCT coefficients, you will reconstruct exactly the very same signal that was input for the DCT analysis. Higher DCT coefficients correspond to the details of the “finite sequence of n data points”, whereas lower coefficients represent the more general characteristics. At least the three lowest ones, k0, k1, and k2, correspond (but are not equal) to the following three statistical descriptive features: k0 is linearly related to the sequence’s mean, k1 to the sequence’s slope, and k2 to its curvature. See e.g.: Because we are dealing with straight lines, k2 is here always 0 (and is therefore not shown in the fourth panel of the figure). However, the next plot shows k2 of four quadratic polynomials: So, if we want to get rid of too much detail (e.g. in signals frequency perturbations like jitter or error measurements), we can use the lower numbers of DCT to smooth the signal. We can apply DCT to a signal by means of the emuR function dct(...,m=NULL,fit=TRUE), with ... being one of the columns of an emuRtrackdata tibble: # calculate spectra reconstructed by dct() sS.dftlong.mean = sS.dftlong.mean %&gt;% group_by(labels) %&gt;% mutate(reconstructed = emuR::dct(track_value, fit = T)) #plot the reconstructed spectral slices ggplot(sS.dftlong.mean) + aes(x = freq, y = reconstructed, col = labels) + geom_line() # this is obviously exactly the same as the original data: ggplot(sS.dftlong.mean) + aes(x = freq, y = track_value,col=labels) + geom_line() However, if we use the parameter m in order to reduce the complexity of the spectral slices, they will become smoother: sS.dftlong.mean = sS.dftlong.mean %&gt;% group_by(labels) %&gt;% mutate(#you can&#39;t use m=0 in order to calculate k0 only smoothed_k0tok1 = emuR::dct(track_value, m = 1, fit = T), smoothed_k0tok2 = emuR::dct(track_value, m = 2, fit = T), smoothed_k0tok3 = emuR::dct(track_value, m = 3, fit = T), smoothed_k0tok4 = emuR::dct(track_value, m = 4, fit = T), smoothed_k0tok5 = emuR::dct(track_value, m = 5, fit = T), smoothed_k0tok6 = emuR::dct(track_value, m = 6, fit = T)) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok6, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 7 DCT-coefficients ()&quot;) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok5, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 6 DCT-coefficients (m=5)&quot;) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok4, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 5 DCT-coefficients (m=4)&quot;) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok3, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 4 DCT-coefficients (m=3)&quot;) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok2, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 3 DCT-coefficients (m=2)&quot;) ggplot(sS.dftlong.mean) + aes(x = freq, y = smoothed_k0tok1, col = labels) + geom_line() + ggtitle(&quot;Smoothed with 2 DCT-coefficients (m=1)&quot;) Remember, that the last figure shows only two (inverted) cosine functions of a certain amplitude with frequency 0.5. This is obviously not the best representation of the spectra of /s/ and /ʃ/. We need to find a compromise between too much and too less information. In this specific case, m = 4 (= 5 DCT-coefficients) seems to be the best compromise. We can, of course, apply the dct-function also to the non-averaged data: # plot the spectral slices ggplot(sS.dftlong) + aes(x = freq, y = track_value, col = labels) + geom_line() + facet_wrap( ~ sl_rowIdx + labels) sS.dftlong = sS.dftlong %&gt;% group_by(sl_rowIdx) %&gt;% mutate(smoothed = emuR::dct(track_value, m = 4, fit = T)) # plot the smoothed slices ggplot(sS.dftlong) + aes(x = freq, y = smoothed, col = labels) + geom_line() + facet_wrap( ~ sl_rowIdx + labels) # or plot original and smoothed slices ggplot(sS.dftlong) + aes(x = freq, y = track_value, col = labels) + geom_line() + geom_line(aes(y = smoothed), lwd = 1.2) + facet_wrap( ~ sl_rowIdx + labels) 21.3 DCT coefficients Until now, we have applied dct() always with the parameter fit set to TRUE, i.e. we have always analysed and resynthesized the data in one step. We haven’t seen so far the outcome of the analysis, i.e. the coefficients of the DCT. They might, as the example above has shown, be capable of a simple quantification of certain features of the signal/spectral slice (i.e. the mean, the slope, and the curvature of the signal). Let’s have a look how useful these coefficients may be. In order to calculate only a couple of coefficients, we will have to learn a new method of data-wrangling in dplyr, as we cannot use summarise() (as this verb transforms many values into one value) or mutate() (which transformes N values into N other values). The verb to use is called do(). It can handle any function (not only a few, as it is the case with summarise()). There a two specialties of do(): input has to be a special dataframe, so we have to use data_frame() you cannot call a column only by it’s name ColumnName, but have to use .$ColumnName, where . means “the current dataframe”. However, this will not be enough: we then have a tibble with m + 1 observations (dct-coefficients in one column); our goal, however, is to have one column per DCT coefficient. In order to do so, we will have to convert the long format to the wide format by means of the spread() function. In order to being able to use this function, we have to introduce another column containing the indexical information which value in column DCT is which DCT-coefficient. Quite complicated, huh? E.g. # calculate 6 dct coefficients for each token of s or S sS.dctCoefficients = sS.dftlong %&gt;% group_by(labels, sl_rowIdx) %&gt;% do(data_frame(DCT = emuR::dct(.$track_value, m = 5, fit = F))) %&gt;% mutate(DCTCOEF = paste0(&quot;k&quot;, 0:(table(sl_rowIdx) - 1))) %&gt;% tidyr::spread(DCTCOEF, DCT) sS.dctCoefficients ## # A tibble: 21 x 8 ## # Groups: labels, sl_rowIdx [21] ## labels sl_rowIdx k0 k1 k2 k3 k4 k5 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s 1 48.9 -4.21 -7.41 0.804 1.04 2.58 ## 2 s 3 49.4 -11.7 -7.73 2.43 2.31 4.28 ## 3 s 4 52.1 -10.3 -5.03 2.54 1.99 2.67 ## 4 s 5 46.7 -9.69 -5.28 3.76 1.16 0.648 ## 5 s 8 49.7 -10.0 -5.26 2.43 0.162 1.67 ## 6 s 9 44.0 -12.2 -5.15 0.494 0.611 1.35 ## 7 s 10 48.6 -13.3 -4.32 2.23 1.98 0.738 ## 8 s 11 42.4 -9.84 -5.85 -0.190 -0.131 5.35 ## 9 s 13 46.4 -9.95 -7.75 -0.655 0.909 4.43 ## 10 s 15 48.0 -11.1 -7.09 0.728 0.608 2.54 ## # … with 11 more rows After this quite complicated procedure, we can finally have a look at the importance of the first three coefficients as far as the power to divide categories is concerned. Let do it in reverse order: #plot k2 (the curvature): ggplot(sS.dctCoefficients) + aes(x = labels, y = k2) + geom_boxplot() Okay, the curvature seems to be different, but are we sure what this means? A bit more intuitive may be k1, the slope: #plot k1 (the slope): ggplot(sS.dctCoefficients) + aes(x = labels, y = k1) + geom_boxplot() Recall that k1 is inversely correlated with the spectral slopes, so /s/ has a steeper positive slope than /ʃ/ (/ʃ/’s slope is close to zero anyway). This simply means that in the range of 0 to 10000 Hz, /s/ has more energy in the high frequency range than in the low frequency range, whereas the energy is more evenly distributed in that frequency range in /ʃ/. What about k0? #plot k0 (the slope): ggplot(sS.dctCoefficients) + aes(x = labels, y = k0) + geom_boxplot() k0 simply corresponds to the mean of the energy in the whole frequency range. This only allows us to find out which of the categories is generally “louder”. As we can see, the mean of the energy is of no use if we want to devide between these two fricatives; it would be much more conveniant to have a function that is able to find the mean of the distribution along the frequency axis (and not along the amplitude axis). There is such a function, which is called spectral moments and which we will discuss next week. The only thing we could do is to use dct-k0 only in a certain frequency range (e.g. 2000 to 3000 Hz). However, this is equivalent to taking the mean of the energy in that frequency range, as we already had done above: #repetition: take the mean of the energy in a certain range: sS2to3thousandHz = sS.dftlong %&gt;% filter(freq &gt;= 2000 &amp; freq &lt;= 3000) %&gt;% group_by(labels,sl_rowIdx) %&gt;% summarise(amplitudes_2000_3000Hz = mean(track_value)) a = ggplot(sS2to3thousandHz) + aes(x = labels, y = amplitudes_2000_3000Hz, col = labels) + geom_boxplot() # or calculate k0 in the same frequency range: sS.dctCoefficients2to3thousandHz = sS.dftlong %&gt;% filter(freq&gt;=2000 &amp; freq &lt;= 3000) %&gt;% group_by(labels,sl_rowIdx) %&gt;% do(data_frame(DCT = emuR::dct(.$track_value, m = 5, fit = F))) %&gt;% mutate(DCTCOEF = paste0(&quot;k&quot;, 0:(table(sl_rowIdx) - 1))) %&gt;% tidyr::spread(DCTCOEF, DCT) b = ggplot(sS.dctCoefficients2to3thousandHz)+ aes(x = labels, y = k0, col = labels)+ geom_boxplot() grid.arrange(a, b, ncol = 2) 21.3.1 P.S.: Use DCT in order to smooth formant trajectories Another use-case for dct-smoothing are bumpy formant tracks. Consider e.g. this case: ae = load_emuDB(path2ae, verbose = F) i.sl = query(ae, query = &quot;[Phonetic == i:]&quot;) i.dft = get_trackdata(ae, seglist = i.sl, ssffTrackName = &quot;fm&quot;, resultType = &quot;tibble&quot;) i.dft = i.dft %&gt;% group_by(sl_rowIdx) %&gt;% mutate(F2_smoothed = emuR::dct(T2, m = 2, fit = T)) ggplot(i.dft) + aes(x = times_norm, y = T2, group = sl_rowIdx)+ geom_line() + geom_line(aes(y = F2_smoothed, col = &quot;smoothed&quot;))+ ggtitle(&quot;Orig. (black) vs. smoothed (red) F2-tracks in /i:/&quot;) 21.4 Spectral moments The types of parameters discussed in the preceding section can often effectively distinguish between spectra of different phonetic categories. Another useful way of quantifying spectral differences is to reduce the spectrum to a small number of parameters that encode basic properties of its shape. This can be done by calculating what are often called spectral moments (Forrest et al., 1988). The function for calculating moments is borrowed from statistics in which the first four moments describe the mean, variance, skew, and kurtosis of a probability distribution. Before looking at spectral moments, it will be helpful to consider (statistical) moments in general. The matrix bridge includes some hypotheticaldata of counts that were made on three separate days of the number of cars crossing a bridge at hourly intervals. It looks like this: bridge ## Mon Tues Wed ## 0 9 1 0 ## 1 35 1 1 ## 2 68 5 7 ## 3 94 4 27 ## 4 90 27 68 ## 5 76 28 87 ## 6 62 62 108 ## 7 28 76 111 ## 8 27 90 57 ## 9 4 94 28 ## 10 5 68 6 ## 11 1 35 0 ## 12 1 9 0 The first row shows that between midday and 1 p.m., 9 cars were counted on Monday, one on Tuesday, and none on Wednesday. The second row has the same meaning but is the count of cars between 1 p.m. and 2 p.m. The figure below shows the distribution of the counts on these three separate days: par(mfrow = c(1, 3)) barplot(bridge[,1], ylab = &quot;Observed number of cars&quot;, main = &quot;Monday&quot;) barplot(bridge[,2], xlab = &quot;Hours&quot;, main = &quot;Tuesday&quot;) barplot(bridge[,3], main = &quot;Wednesday&quot;) Figure 21.1: Hypothetical data of the count of the number of cars crossing a bridge in a 12 hour period. There are obviously overall differences in the shape of these distributions. The plot for Monday is skewed to the left, the one for Tuesday is a mirror-image of the Monday data and is skewed to the right. The data for Wednesday is not as dispersed as for the other days: that is, it has more of its values concentrated around the mean. Leaving aside kurtosis for the present, the following predictions can be made: Monday’s mean (1st moment) is somewhere around 4-5 p.m. while the mean for Tuesday is a good deal higher (later), nearer 8 or 9 p.m. The mean for Wednesday seems to be between these two, around 6-7 p.m. The values for Wednesday are not as spread out as for Monday or Tuesday: it is likely therefore that its variance (2nd moment) will be lower than for those of the other two days. As already observed, Monday, Tuesday, and Wednesday are all likely to have different values for skew (3rd moment). The core calculation of moments involves the formula: \\(\\frac{\\sum{f(x-k)^m}}{\\sum{f}}\\) in which f is the observed frequency (observed number of cars in this example) x is the class (hours from 0 to 12 in our example), m is the moment (m =1, 2, 3, 4) and k is a constant (see also Harrington, 2009). The above formula can be calculated with a function in the Emu-R library, moments(count, x). In this function, count is the observed frequency of occurence and x the class. So the first four moments for the Monday data are given by hours = 0:12 moments(bridge[,1], hours) ## [1] 4.17200000 4.42241600 0.47063226 0.08290827 while all four moments for Monday, Tuesday, Wednesday are given by: apply(bridge, 2, moments, hours) ## Mon Tues Wed ## [1,] 4.17200000 7.82800000 5.99200000 ## [2,] 4.42241600 4.42241600 2.85193600 ## [3,] 0.47063226 -0.47063226 -0.07963716 ## [4,] 0.08290827 0.08290827 -0.39367681 As expected, the first moment (row 1) is at about 4-5 p.m. for Monday, close to 6 p.m. for Wednesday and higher (later) than this for Tuesday. Also, as expected, the variance (second moment, row 2), whose unit in this example is \\(hours^2\\), is least for Wednesday. The skew is a dimensionless number that varies between -1 and 1. When the skew is zero, then the values are distributed evenly about the mean, as they are for a Gaussian normal distribution. When the values are skewed to the left so that there is a longer tail to the right, then kurtosis is positive (as it is for the Monday data); the skew is negative when the values are skewed to the right (as for the Tuesday data). Finally, the kurtosis is also a dimensionless number that is zero for a normal Gaussian distribution. Kurtosis is often described as a measure of how ‘peaked’ a distribution is. In very general terms, if the distribution is flat – that is, its shape looks rectangular – then kurtosis is negative, whereas if the distribution is peaked, then kurtosis is typically positive. However, this general assumption only applies if the distributions are not skewed (skewed distributions tend to have positive kurtosis) and kurtosis depends not just on the peak but also on whether there are high values at the extremes of the distribution (see Wuensch, 2006 for some good examples of this). For all these reasons - and in particular in view of the fact that spectra are not usually symmetrical about the frequency axis - it is quite difficult to use kurtosis to make predictions about the spectral differences between phonetic categories. When spectral moments are calculated, then x and f in both (1) and corresponding R function are the frequency in Hz and the corresponding dB values (and not the other way round!). This can be understood most easily by having another look at the plots of spectral slices above and the observation that a spectral slice has a horizontal axis of frequency in Hz and a vertical axis of dB. On this assumption, the calculation of the 1st spectral moment results in a value in \\(Hz\\) (analogous to a value in hours for the worked example above), and the second spectral moment a value in \\(Hz^2\\), while the 3rd and 4th spectral moments are dimensionless, as before. In order to apply the moments function to our spectral data in the long format tibble, we use the same procedure that we had used in order to calculate dct-coefficients: sS.moments = sS.dftlong %&gt;% group_by(labels,sl_rowIdx) %&gt;% do(data_frame(Moments = emuR::moments(.$track_value,.$freq))) %&gt;% mutate(Momentnumbers = paste0(&quot;Moment&quot;,1:(table(sl_rowIdx)))) %&gt;% tidyr::spread(Momentnumbers, Moments) sS.moments ## # A tibble: 21 x 6 ## # Groups: labels, sl_rowIdx [21] ## labels sl_rowIdx Moment1 Moment2 Moment3 Moment4 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s 1 5238. 7315137. -0.121 -0.978 ## 2 s 3 5660. 6964718. -0.323 -0.776 ## 3 s 4 5544. 7485010. -0.293 -0.923 ## 4 s 5 5569. 7278247. -0.308 -0.903 ## 5 s 8 5557. 7312230. -0.292 -0.934 ## 6 s 9 5790. 6988020. -0.329 -0.837 ## 7 s 10 5770. 7241899. -0.362 -0.840 ## 8 s 11 5653. 7033307. -0.301 -0.845 ## 9 s 13 5606. 6847687. -0.258 -0.867 ## 10 s 15 5653. 6923524. -0.281 -0.868 ## # … with 11 more rows However, the above command may sometimes fail. This is because some of the dB values can be negative and yet the calculation of moments assumes that the values for the observations are positive (it would never be possible, for example, to have a negative value in counting how many cars crossed the bridge in an hourly time interval!). To overcome this problem, the dB values are typically rescaled in calculating moments so that the minimum dB value is set to zero (as a result of which all dB values are positive and the smallest value is 0 dB). The moments() function does this whenever the argument minval = T is included. Thus: sS.moments = sS.dftlong %&gt;% group_by(labels, sl_rowIdx) %&gt;% do(data_frame(Moments = emuR::moments(.$track_value,.$freq, minval = TRUE))) %&gt;% mutate(Momentnumbers = paste0(&quot;Moment&quot;, 1:(table(sl_rowIdx)))) %&gt;% tidyr::spread(Momentnumbers, Moments) sS.moments ## # A tibble: 21 x 6 ## # Groups: labels, sl_rowIdx [21] ## labels sl_rowIdx Moment1 Moment2 Moment3 Moment4 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s 1 5284. 7092414. -0.142 -0.925 ## 2 s 3 5794. 6568993. -0.377 -0.631 ## 3 s 4 5619. 7312918. -0.332 -0.857 ## 4 s 5 5523. 7392654. -0.284 -0.941 ## 5 s 8 5573. 7270354. -0.301 -0.921 ## 6 s 9 5746. 7099514. -0.314 -0.868 ## 7 s 10 5745. 7297563. -0.351 -0.860 ## 8 s 11 5688. 6938212. -0.314 -0.816 ## 9 s 13 5484. 7217595. -0.215 -0.955 ## 10 s 15 5551. 7211331. -0.244 -0.941 ## # … with 11 more rows We can now check, which of the moments could be good separators between [s] and [ʃ]: As [ʃ] has a much lower spectral “center of gravity”, it is reasonable to assume that the mean of the distribution of dB-values is lower along the frequency axis: # plot moment no. 1 (mean of the distribution): ggplot(sS.moments) + aes(x = labels, y = Moment1) + geom_boxplot() + ggtitle(&quot;moment no. 1 (means of the spectral distributions)\\n(= spectral center of gravity)&quot;) However, there is no reason to believe that variance - expressed here by the spectral moment no. 2 - should be extremely different between the two sibilants: # plot moment no. 2 (variance of the distribution): ggplot(sS.moments) + aes(x = labels, y = Moment2) + geom_boxplot() + ggtitle(&quot;moment no. 2 (variances of the distributions)&quot;) Another parameter, the skew of the distribution of dB-values along the frequency axis, should be different, with a greater skew towards the right for [s] as compared to the post-alveolar, that is distributed more around the center of the frequency range 0-10000 Hz: # plot moment no. 3 (skew of the distribution): ggplot(sS.moments) + aes(x = labels, y = Moment3) + geom_boxplot() + ggtitle(&quot;moment no. 3 (skew values of the distributions)&quot;) It has been mentioned earlier, that it is usually quite difficult to use kurtosis to make predictions about the spectral differences between phonetic categories, for various reasons. In this specific case, however, kurtosis would nicely divide the two fricative types: # plot moment no. 4 (kurtosis of the distribution): ggplot(sS.moments) + aes(x = labels, y = Moment4) + geom_boxplot() + ggtitle(&quot;moment no. 4 (kurtosis values of the distributions)&quot;) So, in our case, the two best variables that would differ most when we were trying to distinguish between alveolar and post-alveolar fricatives, would be the skew of the dB-distribution along the frequency axis (at least in out case, where frequency varies between 0 and 10000 Hz), and, as a measure for spectral center of gravity, the first spectral moment, i.e. the mean of the distribution along the frequency axis (which is quite a different thing than the first dct coefficient, which represents the mean along the dB-axis). "],
["plotting-snippets.html", "22 Plotting snippets 22.1 Formant trajectory plots (equivalent to the legacy dplot(..., normalise = T/F, average = T/F,...)) 22.2 F1/F2 plots (equivalent to the legacy eplot(..., dopoints = T/F, doellipse = T/F, centroid = T/F, ...)) 22.3 F1/F2 plot separated by speaker", " 22 Plotting snippets This recipe contains various plotting snippets for various visualization challenges. All snippets will use the ae demo emuDB. Let us create and load that emuDB: library(emuR) library(tidyverse) # containing - amongst others - dplyr, purrr, tibble, and ggplot2 # create demo data in directory provided by the tempdir() function # (of course other directory paths may be chosen) create_emuRdemoData(dir = tempdir()) # create path to demo data directory, which is # called &quot;emuR_demoData&quot; demo_data_dir = file.path(tempdir(), &quot;emuR_demoData&quot;) # create path to ae_emuDB which is part of the demo data path2ae = file.path(demo_data_dir, &quot;ae_emuDB&quot;) # load database ae = load_emuDB(path2ae, verbose = F) 22.1 Formant trajectory plots (equivalent to the legacy dplot(..., normalise = T/F, average = T/F,...)) # query A and V (front and back open vowels), # i:and u: (front and back closed vowels), and # E and o: (front and back mid vowels) ae_vowels = query(emuDBhandle = ae, query = &quot;[Phonetic == V | A | i: | u: | o: | E]&quot;) #get the formants: ae_formants = get_trackdata(ae, seglist = ae_vowels, ssffTrackName = &quot;fm&quot;, resultType = &quot;tibble&quot;) # plot all F2 trajectories # (note that T1 == F1, T2 == F2, ...) ggplot(ae_formants) + aes(x = times_rel, y = T2, col = labels, group = sl_rowIdx) + geom_line() + labs(x = &quot;time (ms)&quot;, y = &quot;F2 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) # time normalize the formant values ae_formants_norm = normalize_length(ae_formants) # plot all normalized F2 trajectories ggplot(ae_formants_norm) + aes(x = times_norm, y = T2, col = labels, group = sl_rowIdx) + geom_line() + labs(x = &quot;normalized time&quot;, y = &quot;F2 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) # calculate and plot averages (== dplot(..., average = T, ...)) ae_formants_norm_average = ae_formants_norm %&gt;% group_by(labels, times_norm) %&gt;% summarise(F2 = mean(T2)) ggplot(ae_formants_norm_average) + aes(x = times_norm, y = F2, col = labels) + geom_line() + labs(x = &quot;normalized time&quot;, y = &quot;F2 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) 22.2 F1/F2 plots (equivalent to the legacy eplot(..., dopoints = T/F, doellipse = T/F, centroid = T/F, ...)) # query A and V (front and back open vowels), # i:and u: (front and back closed vowels), and # E and o: (front and back mid vowels) ae_vowels = query(emuDBhandle = ae, query = &quot;[Phonetic == V | A | i: | u: | o: | E]&quot;) #get the formants: ae_formants = get_trackdata(ae, seglist = ae_vowels, ssffTrackName = &quot;fm&quot;, resultType = &quot;tibble&quot;) # time normalize the formant values ae_formants_norm = normalize_length(ae_formants) # extract the temporal mid-points ae_midpoints = ae_formants_norm %&gt;% filter(times_norm == 0.5) # plot F1 &amp; F2 values (== eplot(..., dopoints = T, doellipse = F, centroid = F, ...)) ggplot(ae_midpoints) + aes(x = T2, y = T1, label = labels, col = labels) + geom_text() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) # plot F1 &amp; F2 values (== eplot(..., dopoints = T, doellipse = T, centroid = F, ...)) ggplot(ae_midpoints) + aes(x = T2, y = T1, label = labels, col = labels) + geom_text() + stat_ellipse() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) ## Warning: Removed 3 rows containing missing values (geom_path). # filter out vowels with enough data points # to calc. ellipse ae_midpoints_Eiu = ae_midpoints %&gt;% filter(labels%in%c(&quot;E&quot;,&quot;i:&quot;,&quot;u:&quot;)) ae_centroid = ae_midpoints_Eiu %&gt;% group_by(labels) %&gt;% summarise(T1 = mean(T1), T2 = mean(T2)) # plot F1 &amp; F2 values (== eplot(..., dopoints = T, doellipse = T, centroid = T, ...)) ggplot(ae_midpoints_Eiu) + aes(x = T2, y = T1, label = labels, col = labels) + stat_ellipse() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) + geom_text(data = ae_centroid) Regarding stat_ellipse() this is worth pointing out: https://github.com/tidyverse/ggplot2/issues/2776 22.3 F1/F2 plot separated by speaker # query A and V (front and back open vowels), # i:and u: (front and back closed vowels), and # E and o: (front and back mid vowels) ae_vowels = query(emuDBhandle = ae, query = &quot;[Phonetic == V | A | i: | u: | o: | E]&quot;) #get the formants: ae_formants = get_trackdata(ae, seglist = ae_vowels, ssffTrackName = &quot;fm&quot;, resultType = &quot;tibble&quot;) # extract the temporal mid-points ae_midpoints = ae_formants %&gt;% filter(times_norm == 0.5) # plot using facet_wrap() # to plot vowels separately for every bundle # (this assumes that every bundle contains a different # speaker which is actually not the case in the ae emuDB) ggplot(ae_midpoints) + aes(x = T2, y = T1, label = labels, col = labels) + geom_text() + scale_y_reverse() + scale_x_reverse() + labs(x = &quot;F2 (Hz)&quot;, y = &quot;F1 (Hz)&quot;) + theme(legend.position = &quot;none&quot;) + facet_wrap(~bundle) "],
["version-control-of-emudbs-with-git-git-lfs-and-gitlab.html", "23 Version control of emuDBs with Git, Git LFS and GitLab 23.1 Setup 23.2 Collaborating with others", " 23 Version control of emuDBs with Git, Git LFS and GitLab This document describes how to do Git versioning of an emuDB for collaboratively working on a emuDB. In this short introduction we will focus on using a GitLab instance such as the one provided by the LRZ (https://gitlab.lrz.de/). However, it should generalize to most comparable services (Github, BitBucket, etc.) with a few slight adjustments. As we will be using Git as well as a Git extension called Git Large File Storage (LFS), please be sure you have these installed on your system. For information on how to install these on your system see: https://git-scm.com/ https://git-lfs.github.com/ 23.1 Setup The first thing we need to do is create and load an example emuDB: library(emuR) create_emuRdemoData() db = load_emuDB(file.path(tempdir(), &quot;emuR_demoData&quot;, &quot;ae_emuDB&quot;), verbose = F) As the ae emuDB is not yet under Git version control we will proceed by initializing a new Git repository in the emuDB directory. In a terminal (e.g. the Terminal tab in RStudio) type: # cd /path/to/emuDB git init Next, we will set up Git LFS to track the large files of our emuDB (*.wav, *.fms, *.dft and *.sqlite). In a terminal (e.g. the Terminal tab in RStudio) type: # in the emuDB dir git lfs install # install lfs git lfs track &quot;*.wav&quot; git lfs track &quot;*.fms&quot; git lfs track &quot;*.dft&quot; git lfs track &quot;*.sqlite&quot; git add .gitattributes # make sure .gitattributes is tracked Now, we will add everything to the Git repository and create our initial commit. In a terminal type: # in the emuDB dir git add * git commit -am &quot;initial commit&quot; This is it for the local setup of Git + Git LFS. If you just wish to work locally simply repeat the above two commands every time you wish to commit the current state of the emuDB to the repository (don’t forget to use a concise commit message). 23.1.1 Using GitLab to host the emuDB The above examples only work on a local Git repository that is located inside of the emuDB directory (contained in a hidden directly called .git). Although this is already beneficial, as we have versioning enabled for our emuDB and can also go back to previous versions, it doesn’t utilize one of Git’s most powerful features. Git is able to sync repository states between multiple machines. Here, we will use GitLab to host the emuDB. Initially you need to create a new project in GitLab under Projects -&gt; New project: that has the same name as the emuDB: Make sure to change the Project slug to match the casing (_emuDB vs. emudb) of the database suffix. The URL of the repository should now be something like: https://gitlab.lrz.de/raphywink/ae_emuDB.git. Next, we will add the newly created remote repo to the configuration of the local repo and push the local changes to the remote. In a terminal type: # in the emuDB dir git remote add origin git@gitlab.lrz.de:raphywink/ae_emuDB.git git push -u origin master Enter your login credentials and that is it! You now have a local and remote repository that can easily be synced. To push a new local commit to the remote repository simply type: # in the emuDB dir git push If you want to pull any changes from the remote repository simply type: # in the emuDB dir git pull If you get sick of entering your login credentials (I certainly do) see here: https://docs.gitlab.com/ee/ssh/ 23.2 Collaborating with others If you wish others to access and/or collaborate with you on the database you simply have to add them as “Project members” in GitLab. Under Project -&gt; Settings -&gt; Members select your colaborator and choose “Maintainer” (read and write access) as their role permission: Once this is set, the collaborator is able to clone the repository using their own credentials: git clone git@gitlab.lrz.de:raphywink/ae_emuDB.git 23.2.1 Default work-flow When collaborating with multiple people it is usually a good idea to do the following: 1.) every time before you start working on an emuDB get the newest version: # in the emuDB dir git pull 2.) once you have made changes that you wish to share, create a new commit and push it to the remote repository so the others can access your changes: # in the emuDB dir git commit -am &quot;added autobuild links from ORT to KAN&quot; # change message accordingly git push "],
["references.html", "References", " References "]
]
